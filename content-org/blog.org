#+title: Boyang Yan's Blog
#+hugo_base_dir: ~/blog/
#+hugo_section: posts
#+hugo_front_matter_format: yaml

* Editor :@Editor:
** DONE VIM Keyboard Shortcuts Cheatsheet :vim:
CLOSED: [2021-12-21 Tue 16:16]
:PROPERTIES:
:EXPORT_FILE_NAME: vim
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction
This post document is brief introduction about how to use VIM/VI text editor and help you remember the shortcuts. The Vim editor is a command-line based tool that’s an enhanced version of the venerable vi editor. Despite the abundance of graphical rich text editors, familiarity with Vim will help every Linux user – from an experienced system administrator to a newbie Raspberry Pi user.

One important thing to note when using Vim, is that the function of a key depends on the "mode" the editor is in. For example, pressing the alphabet "j" will move the cursor down one line in the "command mode". You’ll have to switch to the "insert mode" to make the keys input the character they represent.

*TIPS:*
1. [[http://gen.lib.rus.ec/book/index.php?md5=BC6FB75F968BCC39E4446C29BF04D2D1][Book reference - Learning the Vi and Vim Editors]]

2. I recommend you using [Spacemacs](https://github.com/syl20bnr/spacemacs), which is a new way to experience Emacs, you can use VI's editing styles with emacs' extendibility.

*** User Scenario
1. Most of Embedded system have not GUI avabliable. Nowadays, edge computing become poplar, using command-line text editor can be inportance for configure Edge Devices.
2. Cloud Virual Machine(VM) - marjor of VM only support using SSH login, when you login the cloud VM, CLI will be the only way.

*** Cheatsheet
Here’s a cheatsheet to help you get the most out of Vim.

| *Shortcut Keys*                                | *Function*                                                                                                              |
|----------------------------------------------+-----------------------------------------------------------------------------------------------------------------------|
| *Main (Change Mode)*                           |                                                                                                                       |
| Escape key                                   | Gets out of the current mode into the “command mode”. All keys are bound of commands.                                 |
| i                                            | "Insert mode" for inserting text. Keys behave as expected.                                                            |
| :                                            | "Last-line mode" where Vim expects you to enter a command such as to save the document.                               |
| v                                            | Enter "visual mode"                                                                                                   |
| *Navigation keys (used in command mode)*       |                                                                                                                       |
| h                                            | moves the cursor one character to the left                                                                            |
| j or Ctrl + J                                | moves the cursor down one line                                                                                        |
| k or Ctrl + P                                | moves the cursor up one line                                                                                          |
| l                                            | moves the cursor one character to the right                                                                           |
| 0                                            | moves the cursor to the beginning of the line                                                                         |
| $                                            | moves the cursor to the end of the line                                                                               |
| ^                                            | moves the cursor to the first non-empty character of the line                                                         |
| w                                            | move forward one word (next alphanumeric word)                                                                        |
| W                                            | move forward one word (delimited by a white space)                                                                    |
| 5w                                           | move forward five words                                                                                               |
| b                                            | move backward one word (previous alphanumeric word)                                                                   |
| B                                            | move backward one word (delimited by a white space)                                                                   |
| 5b                                           | move backward five words                                                                                              |
| G                                            | move to the end of the file                                                                                           |
| gg                                           | move to the beginning of the file                                                                                     |
| *Navigate around the document*                 |                                                                                                                       |
| (                                            | jumps to the previous sentence                                                                                        |
| )                                            | jumps to the next sentence                                                                                            |
| {                                            | jumps to the previous paragraph                                                                                       |
| }                                            | jumps to the next paragraph                                                                                           |
| [[                                           | jumps to the previous section                                                                                         |
| ]]                                           | jumps to the next section                                                                                             |
| []                                           | jump to the end of the previous section                                                                               |
| ][                                           | jump to the end of the next section                                                                                   |
| *Insert text*                                  |                                                                                                                       |
| a                                            | Insert text after the cursor                                                                                          |
| A                                            | Insert text at the end of the line                                                                                    |
| i                                            | Insert text before the cursor                                                                                         |
| o                                            | Begin a new line below the cursor                                                                                     |
| O                                            | Begin a new line above the cursor                                                                                     |
| *Special inserts*                              |                                                                                                                       |
| :r [filename]                                | Insert the file [filename] below the cursor                                                                           |
| :r ![command]                                | Execute [command] and insert its output below the cursor                                                              |
| *Delete text*                                  |                                                                                                                       |
| x                                            | delete character at cursor                                                                                            |
| dw                                           | delete a word                                                                                                         |
| d0                                           | delete to the beginning of a line                                                                                     |
| d$                                           | delete to the end of a line                                                                                           |
| d)                                           | delete to the end of sentence                                                                                         |
| dgg                                          | delete to the beginning of the file                                                                                   |
| dG                                           | delete to the end of the file                                                                                         |
| dd                                           | delete line                                                                                                           |
| 3dd                                          | delete three lines                                                                                                    |
| *Simple replace text*                          |                                                                                                                       |
| r{text}                                      | Replace the character under the cursor with {text}                                                                    |
| R                                            | Replace characters instead of inserting them                                                                          |
| *Copy/Paste text*                              |                                                                                                                       |
| yy                                           | copy current line into storage buffer                                                                                 |
| ["x]yy                                       | Copy the current lines into register x                                                                                |
| p                                            | paste storage buffer after current line                                                                               |
| P                                            | paste storage buffer before current line                                                                              |
| ["x]p                                        | paste from register x after current line                                                                              |
| ["x]P                                        | paste from register x before current line                                                                             |
| *Undo/Redo operation*                          |                                                                                                                       |
| u                                            | undo the last operation                                                                                               |
| Ctrl+r                                       | redo the last undo                                                                                                    |
| *Search and Replace keys*                      |                                                                                                                       |
| /search_text                                 | search document for search_text going forward                                                                         |
| ?search_text                                 | search document for search_text going backward                                                                        |
| n                                            | move to the next instance of the result from the search                                                               |
| N                                            | move to the previous instance of the result                                                                           |
| :%s/original/replacement                     | Search for the first occurrence of the string "original" and replace it with "replacement"                            |
| :%s/original/replacement/g                   | Search and replace all occurrences of the string "original" with "replacement"                                        |
| :%s/original/replacement/gc                  | Search for all occurrences of the string “original” but ask for confirmation before replacing them with "replacement" |
| *Bookmarks*                                    |                                                                                                                       |
| m {a-z A-Z}                                  | Set bookmark {a-z A-Z} at the current cursor position                                                                 |
| :marks                                       | List all bookmarks                                                                                                    |
| `{a-z A-Z}                                   | Jumps to the bookmark {a-z A-Z}                                                                                       |
| *Select text*                                  |                                                                                                                       |
| v                                            | Enter visual mode per character                                                                                       |
| V                                            | Enter visual mode per line                                                                                            |
| Esc                                          | Exit visual mode                                                                                                      |
| *Modify selected text (used in visual mode)*   |                                                                                                                       |
| ~                                            | Switch case                                                                                                           |
| d                                            | delete a word                                                                                                         |
| c                                            | change                                                                                                                |
| y                                            | yank                                                                                                                  |
| >                                            | shift right                                                                                                           |
| <                                            | shift left                                                                                                            |
| !                                            | filter through an external command                                                                                    |
| *Save and quit*                                |                                                                                                                       |
| :q                                           | Quits Vim but fails when file has been changed                                                                        |
| :w                                           | Save the file                                                                                                         |
| :w new_name                                  | Save the file with the new_name filename                                                                              |
| :wq                                          | Save the file and quit Vim                                                                                            |
| :q!                                          | Quit Vim without saving the changes to the file                                                                       |
| ZZ                                           | Write file, if modified, and quit Vim                                                                                 |
| ZQ                                           | Same as :q! Quits Vim without writing changes                                                                         |


* Emacs :@Emacs:
** DONE Getting Started with Doom Emacs :Doom:Emacs:
CLOSED: [2021-10-05 Tue 03:44]
:PROPERTIES:
:EXPORT_FILE_NAME: doom
:EXPORT_OPTIONS: author:nil
:ID:       48390f93-f8f0-435c-8938-acf20f581e46
:END:
*** Prerequisites
**** Instation
***** Arch Linux
#+begin_src bash
# required dependencies
pacman -S git emacs ripgrep
# optional dependencies
pacman -S fd
yay -S emacs-pdf-tools-git
```
#+end_src

With Emacs and Doom’s dependencies installed, next is to install Doom Emacs itself:
#+begin_src bash
git clone https://github.com/hlissner/doom-emacs ~/.emacs.d
~/.emacs.d/bin/doom install
#+end_src

*** Sart-up
https://zzamboni.org/post/my-doom-emacs-configuration-with-commentary/

Install the icons to avoid having weird symbols.
*M-x all-the-icons-install-fonts*

*Note*: M-x is SPC :

Open a file

**SPC f f** OR *SPC .*

*** The bin/doom utility
This utility is your new best friend. It won’t spot you a beer, but it’ll shoulder much of the work associated with managing and maintaining your Doom Emacs configuration, and then some. Not least of which is installation of and updating Doom and your installed packages.

It exposes a variety of commands. *bin/doom* help will list them all, but here is a summary of the most important ones:

+ *doom sync*: This synchronizes your config with Doom Emacs. It ensures that needed packages are installed, orphaned packages are removed and necessary metadata correctly generated. Run this whenever you modify your doom! block or packages.el file. You’ll need doom sync -u if you override the recipe of package installed by another module.
+ *doom upgrade*: Updates Doom Emacs (if available) and all its packages.
doom env: (Re)generates an “envvar file”, which is a snapshot of your shell environment that Doom loads at startup. If your app launcher or OS launches Emacs in the wrong environment you will need this. **This is required for GUI Emacs users on MacOS.**
+ *doom doctor*: If Doom misbehaves, the doc will diagnose common issues with your installation, system and environment.
+ *doom purge*: Over time, the repositories for Doom’s plugins will accumulate. Run this command from time to time to delete old, orphaned packages, and with the -g switch to compact existing package repos.
Use doom help to see an overview of the available commands that doom provides, and doom help COMMAND to display documentation for a particular COMMAND.

I recommend you add a couple of *alias* to your ZSH configuration.
#+begin_example
# Doom Emacs
alias doomsync="$HOME/.emacs.d/bin/doom sync"
alias doomupgrade="$HOME/.emacs.d/bin/doom upgrade"
alias doomdoctor="$HOME/.emacs.d/bin/doom doctor"
alias doompurge="$HOME/.emacs.d/bin/doom purge"
alias doomclean="$HOME/.emacs.d/bin/doom clean"
alias doombuild="$HOME/.emacs.d/bin/doom build"
#+end_example

*** Doom config file overview
We already know how to open a file and how to use doom utility, so let's we take a overview for Doom configuration.

Doom Emacs at the least uses three config files.

+ *init.el* defines which of the existing Doom modules are loaded. A Doom module is a bundle of packages, configuration and commands, organized into a unit that can be toggled easily from this file. You also can design your own *Module*.
+ *packages.el* defines which packages should be installed, beyond those that are installed and loaded as part of the enabled modules.
+ *config.el* contains all custom configuration and code. when you have get so many configuration contains, you may need seperate for each category. There is a example. Click [[https://github.com/yanboyang713/doom.git][Here]]

There are other files that can be loaded, but theses are the main ones. The load order of different files is defined depending on the type of session being started.

**** Config file headers
We start by simply defining the standard headers used by the three files. These headers come from the initial files generated by doom install, and contain either some Emacs-LISP relevant indicators like lexical-binding, or instructions about the contents of the file.

+ init.el
+ packages.el
+ config.el

#+begin_example
;;; ../../dotfiles/doom/+research.el -*- lexical-binding: t; -*-
#+end_example

**** Customized variables
Doom [[https://github.com/hlissner/doom-emacs/blob/develop/docs/getting_started.org#configure][does not recommend the Emacs customize mechanism]] :

*Note*: do not use M-x customize or the customize API in general. Doom is designed to be configured programmatically from your config.el, which can conflict with Customize’s way of modifying variables.

All necessary settings are therefore set by hand as part of this configuration file. The only exceptions are “safe variable” and “safe theme” settings, which are automatically saved by Emacs in custom.el, but this is OK as they don’t conflict with anything else from the config.

*** General configuration
My user information.
#+begin_src emacs-lisp
(setq user-full-name "Boyang Yan"
      user-mail-address "yanboyang713@gamil.com")
#+end_src

*** Projects with Projectile, File Explorer with Treemacs & EShell
**** Projectile
Doom Emacs have used package [[https://github.com/bbatsov/projectile][Projectile]] to management our project.

*SPC p p* - Switch to project.
*SPC SPC* - Find a File in a project

#+begin_src emacs-lisp
projectile-project-search-path '("~/Project/" "~/dotfiles/" "~/blog/content-org/")
#+end_src

**** Treemacs
SPC o p - Open
When Treemacs is opened, you can type *q* to close.


** DONE Getting Started Org-mode on Emacs :Emacs:org:
CLOSED: [2021-12-15 Wed 11:29]
:PROPERTIES:
:EXPORT_FILE_NAME: org-mode
:EXPORT_OPTIONS: author:nil
:ID:       93eddd5b-1e6c-4107-9c04-2d68968a01cc
:END:

*** Introduction
Org mode is an artifact under Emacs. Compared with the lightweight format of markdown,
It has more functions and expandability, but it is still difficult to get started.
Here is only a brief introduction to some of the basic functions.

*** Pre-reading
1. [[id:48390f93-f8f0-435c-8938-acf20f581e46][Emacs/Getting Started with Doom Emacs :Emacs:Doom-Emacs:]]

*** Fundamental Feature
**** Headings
org-mode uses * to indicate headings, and the number of * to indicate heading level:

#+begin_example
,* First level heading
,** Secondary heading
,*** Level 3 heading
#+end_example

1. Use the *Control-[RET]* key to insert the title of the same level, and you can use the *Alt-left/right* title to increase and decrease the level
2. Using *Alt-Up* and *Alt-Down* swap the *Headings* content Up and Down (including sub-chapters).
3. You can use the *Tab* key to expand and collapse the headings, and the *Shift+Tab* key to expand and collapse all chapters.

*** List
The *List* includes two types:
1. Unordered list
   #+begin_example
+ list
+ list
    + sub-list
   #+end_example

2. Ordered list
   #+begin_example
1. list
2. list
    1. sub-list
   #+end_example

*** Link
Here is an example of Link.

#+begin_example
[[URL][description]]
[[https://yanboyang.com][Boyang Yan's blog]]
#+end_example

*NOTE*: After inputting, the GUI display will change, edit through *C-c C-l*

For display image is similar:
#+begin_example
[[file:/home/path_of_pics]]
[[./pics/file.jpg]]
#+end_example

*** font settings
#+begin_example
*bold*
/italic/
_underlined_
=code=
~verbatim~
+striken-through+
#+end_example

*** Footnote
Here is an example of Footnotes.

#+begin_example
[fn:name] The link: https:yanboyang.com
Boyang Yan's Blog[fn:name]
#+end_example

Can be inserted through *C-c C-x f*, and footnote jump to the definition *C-c C-c*

The specific content is [fn:NAME], [fn:: This is the inline definition of this footnote] and [fn:NAME: a definition].

*** Table
The Org-mode table is implemented in the following format:
#+begin_example
| name     |   phone | job     | score1 | score2 | total |
|----------+---------+---------+--------+--------+-------|
| Zhangsan | 1234455 | student |     19 |     30 |    49 |
| Lisi     |  423423 | layer   |     20 |     10 |    30 |
#+TBLFM: $6=$4+$5
#+end_example

You need to set the number of columns in the first row, and the *Tab* key can complete to the next grid (next row)

*|-* can be completed to get *|----------+---------+---------|*

The total column can be automatically summed in the form of = /$4+/$5 and executed as *C-c C-c*

*** Text format output
The text format output needs to be run in different modes, generally obtained by the form of *<+char+Tab* completion, such as *<s+Tab*

Different characters in different text forms are defined as follows:
#+begin_example
s       #+BEGIN_SRC ... #+END_SRC

e       #+BEGIN_EXAMPLE ... #+END_EXAMPLE

q       #+BEGIN_QUOTE ... #+END_QUOTE

v       #+BEGIN_VERSE ... #+END_VERSE

c       #+BEGIN_CENTER ... #+END_CENTER

C       #+BEGIN_COMMENT ... #+END_COMMENT

l       #+BEGIN_EXPORT latex ... #+END_EXPORT

L       #+LATEX:

h       #+BEGIN_EXPORT html ... #+END_EXPORT

H       #+HTML:

a       #+BEGIN_EXPORT ascii ... #+END_EXPORT

A       #+ASCII:

i       #+INDEX: line

I       #+INCLUDE: line
#+end_example

Here are examples:
#+begin_example
Set title and the table of content:
# +TITLE: This is the title of the document
# +OPTIONS: toc:2 (only to two levels in TOC)
# +OPTIONS: toc:nil (no TOC at all)

Add quote：
# +BEGIN_QUOTE
Everything should be made as simple as possible,
but not any simpler -- Albert Einstein
# +END_QUOTE

Set to center：
# +BEGIN_CENTER
    Everything should be made as simple as possible,but not any simpler
# +END_CENTER

Set Example (The content inside will be output directly):
# +BEGIN_EXAMPLE
The text will be output directly, NOT tranfer the others form.
# +END_EXAMPLE

Write Comment (The content inside will NOT output)
# +BEGIN_COMMENT
The content inside will NOT export.
# +END_COMMENT
#+end_example

*** Post-reading
1. [[id:9b54ba0f-8677-428b-bf01-0fc8aa44e1e3][Emacs/Best practices for using Emacs org-mode as a day-planner/scheduler/calendar]]
2. [[id:44cd9edd-924b-49e1-8a91-5f85961165c4][Emacs/Using Emacs Org-Babel Mode to Write Literate Programming Documents]]

*** Reference List
1. https://blog.csdn.net/sunny0660/article/details/104078734


** DONE Using Emacs Org-Babel Mode to Write Literate Programming Documents :babel:org:emacs:literate:programming:
CLOSED: [2021-12-17 Fri 10:29]
:PROPERTIES:
:EXPORT_FILE_NAME: orgBabel
:EXPORT_OPTIONS: author:nil
:ID:       44cd9edd-924b-49e1-8a91-5f85961165c4
:END:

*** Introduction
We introduce the use of emacs org-babel model in this document. Emacs Org-Babel mode is a literate programming tool (aka. active document), which can embed multiple programming languages, inlcuding R, Python, C/C++ in one document. Babel has ability to execute source code within Org-mode documents.

*** Pre-reading
1. [[id:93eddd5b-1e6c-4107-9c04-2d68968a01cc][Emacs/Getting Started Org-mode on Emacs]]

*** Source Code Insert & Running

Here is an format example:
#+begin_example
#+BEGIN_SRC language
#+END_SRC
#+end_example


*** Reference List
1. https://orgmode.org/worg/org-contrib/babel/languages/ob-doc-C.html
2. https://orgmode.org/worg/org-contrib/babel/languages/ob-doc-R.html
3. https://orgmode.org/worg/org-contrib/babel/how-to-use-Org-Babel-for-R.html


** DONE Best practices for using Emacs org-mode as a day-planner/scheduler/calendar :Emacs:org:planning:
CLOSED: [2021-12-16 Thu 18:28]
:PROPERTIES:
:EXPORT_FILE_NAME: orgPlanning
:EXPORT_OPTIONS: author:nil
:ID:       9b54ba0f-8677-428b-bf01-0fc8aa44e1e3
:END:

*** Pre-reading
1. [[id:93eddd5b-1e6c-4107-9c04-2d68968a01cc][Emacs/Getting Started Org-mode on Emacs]]

*** Checkbox
It often great to split a task into a number of simple steps. Or you can use them in a shopping list.

The form is [], put in front of the task can mark the completion status of the task, Put it after the task to mark the completion degree of the task, which needs to add % or /

Here is an example of a checkbox list.
#+begin_example
call people [1/3]
    - [ ] Peter
    - [X] Sarah
    - [-] Sam [50%]
      + [X] topic 1
      + [ ] topic 2
#+end_example

*NOTE*:
1. *M-S-RET* (org-insert-todo-heading) *M* is ALT on my system.

   Insert a new item with a checkbox. This works only if point is already in a plain list item
2. *C-c C-c* (org-toggle-checkbox)
   Toggle checkbox status or—with prefix argument—checkbox presence at point. With a single prefix argument, add an empty checkbox or remove the current one50. With a double prefix argument, set it to ‘[-]’, which is considered to be an intermediate state.


** DONE Getting Started with send and receive Email with Doom Emacs
CLOSED: [2021-12-06 Mon 13:44]
:PROPERTIES:
:EXPORT_FILE_NAME: sendAndReceiveEmailWithdoom
:EXPORT_OPTIONS: author:nil
:END:
*** Receive Email
#+begin_src console
yay -S mu mbsync-git
#+end_src

https://devanswers.co/create-application-specific-password-gmail/

#+begin_src console
gpg2 -c xxxxxxx
#+end_src

#+begin_src console
time mu init --maildir=~/MailDir --my-address='yanboyang713@gmail.com'
mu index

time mbsync -c ~/.config/mu4e/mbsyncrc -a

#+end_src

[yanboyang713@Boyang-PC] ➜ ~ time mu init --maildir=~/MailDir --my-address='yanboyang713@gmail.com'
error: failed to open store @ /home/yanboyang713/.cache/mu/xapian: Unable to get write lock on /home/yanboyang713/.cache/mu/xapian: already locked
mu init --maildir=~/MailDir --my-address='yanboyang713@gmail.com'  0.00s user 0.00s system 66% cpu 0.006 total
[yanboyang713@Boyang-PC] ➜ ~ mu index
error: failed to open store @ /home/yanboyang713/.cache/mu/xapian: Unable to get write lock on /home/yanboyang713/.cache/mu/xapian: already locked


pkill -2 -u $UID mu
sleep 1
mu index



Using GPG for mbsync passwords
The basic idea is that every time a password is needed, an particular file is decrypted and loaded. The key for the decryption can be prompted for and be stashed by gpg-agent. The first step is to create a GPG key, which is covered very well elsewhere. The standard authentication mechanism for gnus and smtpmail can be reused to store login information for mbsync. For any one account, the password for IMAP access and the password for sending email (usually the same) can be added to ~/.authinfo.gpg:

machine imap.gmail.com login MyAccountName@gmail.com password MYPASSWORD machine smtp.gmail.com login MyAccountName@gmail.com password MYPASSWORD The first line is used by mbsync and the second by smtpmail. The line: PassCmd “gpg2 -q –for-your-eyes-only –no-tty -d ~/.authinfo.gpg | awk ’machine imap.gmail.com login MyAccountName@gmail.com {print $NF}’”

*** Send Email

#+begin_src console
yay -S msmtp msmtp-mta s-nail
#+end_src
echo "hello there username." | msmtp -a default username@domain.com

~/.mailrc
set mta=/usr/bin/msmtp


*** Doom Emacs Set-up
(package! mu4e)

*** Usage
https://cheatography.com/ddoherty03/cheat-sheets/mu4e-with-gmail-hints/


*** Reference List
1. https://macowners.club/posts/email-emacs-mu4e-macos/


* Computer Vision :@ComputerVision:

** DONE Getting started with FFmpeg
CLOSED: [2021-11-01 Mon 15:03]
:PROPERTIES:
:EXPORT_FILE_NAME: ffmpeg
:EXPORT_OPTIONS: author:nil
:END:
*** Overview
FFmpeg is a free software project and is the leading software for everything related to multimedia like video encoding, streaming and muxing.

FFmpeg - "FF" mean "Fast Forward", "mpeg" mean "Moving Picture Expers Group"

*** Installation
https://github.com/jrottenberg/ffmpeg

https://www.whoishostingthis.com/compare/ffmpeg/resources/

alias ffmpeg='docker run -v=`pwd`:/tmp/ffmpeg opencoconut/ffmpeg'

#+begin_src bash
yay -S ffmpeg
#+end_src

*** Using Linux Terminal to Install VLC in Ubuntu
sudo snap install vlc

*** Invert the video stream to a virtual video camera
If your video stream is inverted, you can make a new virtual video camera which inverts the inverted video. You need to install v4l-utils and also v4l2loopback-dkms.

#+begin_src bash
yay -S v4l-utils v4l2loopback-dkms
#+end_src

*** Create the virtual video camera:
#+begin_src bash
modprobe v4l2loopback
#+end_src
https://askubuntu.com/questions/881305/is-there-any-way-ffmpeg-send-video-to-dev-video0-on-ubuntu

Check the name of the newly created camera:

#+begin_src console
[yanboyang713@boyang ~]$ v4l2-ctl --list-devices
Dummy video device (0x0000) (platform:v4l2loopback-000):
	/dev/video0
#+end_src

*** Image to virtual camera
ffmpeg -re -loop 1 -i input.jpg -vf format=yuv420p -f v4l2 /dev/video0


Then you can run ffmpeg to read from your actual webcam (here /dev/video0) and invert it and feed it to the virtual camera:

$ ffmpeg -f v4l2 -i /dev/video0 -vf "vflip" -f v4l2 /dev/video1
You can use the "Dummy" camera in your applications instead of the "Integrated" camera.

Bad image quality
If you experience images being too bright, too dark, too exposed or any other, you can install v4l2ucpAUR to tweak your image output.


** TODO Image Compression Based on Principal Component Analysis (PCA)
:PROPERTIES:
:EXPORT_FILE_NAME: PCAforImage
:EXPORT_OPTIONS: author:nil
:END:
*** Introduction
Principal Component Analysis (PCA) is a linear dimensionality reduction technique (algorithm) that transform a set of correlated variables (p) into a smaller k (k<p) number of uncorrelated variables called principal components while keeping as much of the variability in the original data as possible.

One of the use cases of PCA is that it can be used for image compression — a technique that minimizes the size in bytes of an image while keeping as much of the quality of the image as possible.



** DONE Beginning Explore artificial intelligence and computer vision
CLOSED: [2021-10-07 Thu 19:17]
:PROPERTIES:
:EXPORT_FILE_NAME: firstExploreAIandComputerVision
:EXPORT_OPTIONS: author:nil
:END:

*** What is artificial intelligence?
**** Explore into artificial intelligence
For the definition of artificial intelligence, academic research area always have different understandings. The widely accepted definition is:

+ *Artificial intelligence is the use of machines to simulate human cognitive abilities technology*.

Artificial intelligence involves a wide range of insights, learning, reasoning and decision-making.

From the perspective of industry application, the core ability of artificial intelligence ability is to make judgments or predictions based on given input.

The rise of deep learning and the three booms of AI.
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633594626/cv/MLThreeBooms_xnexn3.png]]

The Turing test, the cornerstone of artificial intelligence
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633607665/cv/turingTest_000_wuxoka.png]]


**** Three core elements of artificial intelligence
Three core elements of AI: data, algorithm and compute resource.
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633609084/cv/threeCoreElements_pj0xlg.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633609181/cv/threeCoreElementsOne_vn5zm9.png]]

***** Data
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633607997/cv/data_yoauah.png]]

***** Algorith
When you give a computer a task, you tell it not only what to do, but how to do it and a set of instructions about how to do it is called an algorithm.

+ Traditional algorithms -- traversal
+ Smarter algorithms -- gradient descent
+ More complex algorithms -- machine learning

***** Compute Resource/Power
 Breakthrough in computing power -- traditional CPU and new computing acceleration technology.

 [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633608323/cv/cpu_tkdhfn.png]]

 [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633608393/cv/fpga_bsknu0.png]]

 [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633608486/cv/compare_ou1gus.png]]

 smart chip
 [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633608633/cv/smartChip_ev498y.png]]


**** Artificial intelligence technonly relationship
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633609345/cv/relationship_nugk48.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673844/cv/AIrelationship_myqcj6.png]]

+ *Machine learning*: a way to achieve artificial intelligence

It is a multi-field interdisciplinary subject, involving probability theory, statistics, approximation theory, convex analysis, algorithm complexity theory and other subjects. Machine learning is the core of artificial intelligence, the fundamental way to make computers intelligent, and its applications are widespread
In all fields of artificial intelligence, it mainly uses induction and synthesis rather than deduction.

+ *Deep learning*: a technology that implements machine learning.

It uses a deep neural network to process the model more complex, so that the model has a deeper understanding of the data. It is a method of machine learning based on data representation learning. The motivation is to establish and simulate the human brain to analyzing the learning neural network, it imitates the mechanism of the human brain to interpret data, such as images, sounds and texts. The essence of deep learning is to learn more by building a machine learning model with many hidden layers and massive training data. Use the features to ultimately improve the accuracy of classification or prediction.

+ *Artificial neural network*: a machine learning algorithm

Neural networks generally have input layer -> hidden layer -> output layer. Generally speaking, a neural network with more than two hidden layers is called a deep neural network. Deep learning is a machine that uses a deep architecture like a deep neural network. Learn method.

***** What is machine Learning
*Artificial intelligence is a technology that uses machines to simulate human cognitive abilities*.

+ Traditional artificial intelligence methods: logical reasoning, expert systems (answering questions based on manually defined rules), etc.;

+ Contemporary artificial intelligence generally acquires the ability to make predictions and judgments through learning-machine learning

#+begin_example
Normal cat: round head, short face, five fingers on the forelimbs, four toes on the hind limbs, with sharp and curved claws at the ends of the toes,
The claws can stretch. Nocturnal. ---Baidu Encyclopedia
#+end_example

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633610156/cv/ml1_zrobow.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633610156/cv/ml2_lb82ew.png]]

***** Typical machine learning process
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633610289/cv/ml3_tzhdmh.png]]

***** What is Neural Network
****** How do people think? --Biological Neural Network

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633610519/cv/neuron_lhvbvb.png]]

sensor:
1. External stimulation passes through nerve endings and turns converted into electrical signals, transduced to nerve cells (Also called neuron)
2. Numerous neurons form the nerve center
3. The nerve center integrates various signals to do judgement.
4. According to the instructions of the nerve center, the human body respond to external stimuli.

****** How does the machine think? --Artificial neural networks

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633672744/cv/arNeuron_awz0bq.png]]

Artificial neuron

Input: x1,x2,x3
Output: output
Simplified model: It is agreed that each input has only two possible 1 or 0

All inputs are 1, which means that various conditions are met, and the output is 1;

All inputs are 0, which means that the condition is not true, and the output is 0

#+begin_example
Is watermelon good or bad?
Color: green; root: curled up; knock: voiced thoughts. ---Good melon
#+end_example

#+begin_example
Family Spring Outing?
Price: high and low; weather: good or bad; family: can you travel
#+end_example

****** The logical architecture of the neural network

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673214/cv/architectureNeuralNetworkOne_sijbou.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673216/cv/architectureNeuralNetworkTwo_l4dsh1.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673225/cv/architectureNeuralNetworkThree_cuepcr.png]]



***** What is Deep Learning
Deep neural network & deep learning

+ The traditional neural network has developed to a situation with multiple hidden layers,

+ Neural networks with multiple hidden layers are called deep neural networks, and machine learning research based on deep neural networks is called deep learning.

  [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673700/cv/DeepLearning_tq9n1e.png]]

**** The foreseeable future of artificial intelligence
***** Computer vision

+ Typical technology:
Face detection, tracking, recognition and attribute analysis, pedestrian and vehicle detection, tracking, recognition and attribute analysis, text detection and recognition, object detection and recognition

+ Typical application:
Face authentication, intelligent transportation, robot vision (such as drones), image search engine, image and video understanding, image and video beautification

***** Speech Recognition

+ Typical technology:
Voice recognition, voiceprint recognition, multi-microphone array system

+ Typical application:
Voice input, voice control, intelligent assistant, machine translation, robot hearing

***** natural language

+ Typical technology:
Words and sentences embedded, semantic modeling

+ Typical application:
Chatbot, smart assistant, smart customer service, video Frequency understanding, machine translation

*** Computer vision (CV)
**** What is CV
Several more rigorous definitions:

+ "Construct a clear and meaningful description of the objective objects in the image" (Ballard & Brown, 1982)

+ "Calculate the characteristics of the three-dimensional world from one or more digital images" (Trucco & Verri, 1998)

+ "Based on perceptual images to make useful decisions for objective objects and scenes" (Sockman & Shapiro, 2001)

Overview in one sentence:

It means that the computer has the ability to see, know, and think. It can be said that the computer has vision, that is, computer vision.

**** Deep learning and CV
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633674756/cv/computerVisionOne_kjgmyf.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633674755/cv/computerVisionTwo_l2tvqa.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633674756/cv/computerVisionThree_rdqpta.png]]

**** Application of CV
***** Image Classification

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678185/cv/classificationOne_lqnjcd.png]]

Image Classification - Neural Neural Network (CNN)

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678184/cv/classificationTwo_pu6kuc.png]]

Linear rectifier layer--RELU

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678183/cv/classificationThree_zhnzgk.png]]

Pooling layer-pool

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678183/cv/classificationFour_k9lyy9.png]]

***** Target Detection

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678492/cv/detactionOne_rsz53f.png]]

R-CNN

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678492/cv/detactionTwo_lnellb.png]]

***** Target Tracking
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678587/cv/tracking_rjkb2z.png]]

***** Semantic Image Segmentation
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678681/cv/SegmentationOne_zfgix9.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v163367868e/cv/SegmentationTwo_orkdal.png]]

***** Instance Segmentation

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678767/cv/instanceOne_duswl8.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678767/cv/instanceTwo_osehjy.png]]

**** CV skills tree construction

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633680105/cv/treeOne_kbmiwg.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633680106/cv/treeTwo_wchdqb.png]]


** DONE Gettting Started SRS(Simple Realtime Server) :SRS:video-server:
CLOSED: [2021-11-13 Sat 11:43   ]
:PROPERTIES:
:EXPORT_FILE_NAME: srs
:EXPORT_OPTIONS: author:nil
:END:

*** Overview
SRS is a simple, high efficiency and realtime video server, supports RTMP/WebRTC/HLS/HTTP-FLV/SRT.


* Networking :@Networking:
** DONE Getting Started IpTV with RouterOS and OpenVswitch :IPTV:
CLOSED: [2021-12-13 Mon 19:52]
:PROPERTIES:
:EXPORT_FILE_NAME: iptv
:EXPORT_OPTIONS: author:nil
:END:

After replacing the optical modem with the optical module, it is found that there are vlan 3964 and 4000, and there are igmp data on them, you can watch iptv through igmp proxy.

The specific operation method is as follows:

1. Interface–>Add a VLAN virtual interface to the sfp optical port, with any Name and VLAN ID=4000. Not necessarily 4000 in other regions

[[https://danteng.org/wp-content/uploads/2020/03/1584948032751.png]]

2. Randomly configure an IP address for the VLAN4000 virtual interface. This address is not needed, but it must be present.
   [[https://danteng.org/wp-content/uploads/2020/03/1584948032760.png]]
   [[https://danteng.org/wp-content/uploads/2020/03/1584948032766.png]]
3. An upstream interface is added to Routing–>IGMP Proxy, and VLAN 4000 virtual interface is selected here.
   [[https://danteng.org/wp-content/uploads/2020/03/1584948032775.png]]
   [[https://danteng.org/wp-content/uploads/2020/03/1584948032780.png]]
4. Configure the internal network interface, be careful not to check upstream
   [[https://danteng.org/wp-content/uploads/2020/03/1584948032784.png]]
   After adding, you can see the multicast information in the MFC tag
   [[https://danteng.org/wp-content/uploads/2020/03/1584948032788.png]]
5. Check IGMP Snooping in the Intranet bridge, General tab. The function is to allow IGMP frames to pass. If you do not check it, you will not be able to watch them.
   [[https://danteng.org/wp-content/uploads/2020/03/1584948032792.png]]
6. Use a network cable to connect to the computer, it must be a network cable
7. It can be watched by pot player or VLC under windows, and it can be watched by importing the Beijing Unicom multicast address table

   When watching iptv, you can see that traffic is generated on the VLAN 4000 virtual interface. According to observations, the standard-definition signal traffic is around 2.6M, and the high-definition signal is around 4M. My computer's network card is a 100M network card, and can only watch 4 channels of high-definition signals at the same time.

   [[https://danteng.org/wp-content/uploads/2020/03/1584948032796.png]]

   If there is no IGMP Proxy option in the routing menu, please go to System->Packages to check whether the multicast package is installed.

   [[https://danteng.org/wp-content/uploads/2020/03/1584948032800.png]]


** DONE Configuring Network Bonding :bonding:
CLOSED: [2021-12-12 Sun 17:57]
:PROPERTIES:
:EXPORT_FILE_NAME: bonding
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction
Network bonding refers to the combination of network interfaces on one host for redundancy and/or increased throughput.

Redundancy is the key factor:
we want to protect our virtualized environment from loss of service due to failure of a single physical link. This network bonding is the same as the Linux network bonding.

Using network bonding in *OpenVswitch OVS* require some switch configuration.

In this article, I will demonstrate How to use *Networking Bonding* between OVSBridge and Ubiquiti.

There are three modes of network bonding:
+ *Active-Passive*: there is one NIC active while another NIC is asleep. If the active NIC goes down, another NIC becomes active.
+ *Link Aggregation*: aggregated NICs act as one NIC which results in a higher throughput.
+ *Load Balanced*: the network traffic is equally balanced over the NICs of the machine.

*** Ubiquiti bonding Set-up
Steps for configure *Link Aggregation Groups*

1. Navigate to the *Devices* section in the UniFi Network application and click on the switch to open the Properties Panel.
2. In the Properties Panel, go to the *Ports* section and select a port that will participate in the link aggregation group by selecting Edit (pencil icon) when hovering over it.
3. Click *Profile Overrides* to expand section.
4. Under Operations, select *Aggregate*. This will expose some Aggregate options.
5. Under the Aggregate Ports input which ports to include in the LAG.

https://help.ui.com/hc/en-us/articles/360007279753-UniFi-USW-Configuring-Link-Aggregation-Groups-LAG-

mode=4 (802.3ad)

This mode is known as a Dynamic Link Aggregation mode that has it created aggregation groups having same speed. It requires a switch that supports IEEE 802.3ad dynamic link. The slave selection for outgoing traffic is done based on a transmit hashing method. This may be changed from the XOR method via the xmit_hash_policy option.

*** OpenVswitch Bonding
https://docs.openvswitch.org/en/latest/topics/bonding/


** DONE Getting Started OpenWrt :openwrt:
CLOSED: [2021-12-12 Sun 11:42]
:PROPERTIES:
:EXPORT_FILE_NAME: openwrt
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction
The OpenWrt Project is a Linux operating system targeting embedded devices. Instead of trying to create a single, static firmware, OpenWrt provides a fully writable filesystem with package management. This frees you from the application selection and configuration provided by the vendor and allows you to customize the device through the use of packages to suit any application.

OpenWrt official Website [[https://openwrt.org/][Here]].

In this article, I will talk about:
1. Compile Openwrt from Source Code.
2. Install Openwrt on ProxMox.
3. Basic Set-Up for Openwrt.

*** Compile OpenWrt
 1. Make sure your have a avaiable Linux/MacOS system, offers recommand Ubuntu 18 LTS x64.
 2. Install required packages.
    #+begin_src bash
sudo apt-get update

sudo apt-get -y install build-essential asciidoc binutils bzip2 curl gawk gettext git libncurses5-dev libz-dev patch python3.5 python2.7 unzip zlib1g-dev lib32gcc1 libc6-dev-i386 subversion flex uglifyjs git-core gcc-multilib p7zip p7zip-full msmtp libssl-dev texinfo libglib2.0-dev xmlto qemu-utils upx libelf-dev autoconf automake libtool autopoint device-tree-compiler g++-multilib antlr3 gperf
    #+end_src
 3. Getting Source Code and enter direction.
    #+begin_src bash
git clone -b main --single-branch https://github.com/Lienol/openwrt openwrt
cd openwrt
    #+end_src
 4. Add additional package/plugin to Source Code, such as *Passwall*.
    #+begin_src bash
vim feeds.conf.default
    #+end_src

    Adding src-git at the end of file.
    #+begin_src file
src-git passwall https://github.com/xiaorouji/openwrt-passwall
    #+end_src
 5. Update the feeds
    #+begin_src bash
./scripts/feeds clean
./scripts/feeds update -a
./scripts/feeds install -a
    #+end_src
 6. Configure the firmware image
    #+begin_src bash
make menuconfig
    #+end_src
    *NOTE*:
    1. First three menu is very inportance, Please, carefully choose with your correct CPU architecture. For example, x86.

    2. Settings your package/plugins luci-app, such as, luci-app-passwall

 7. Downloading DL library.
    #+begin_src bash
make -j8 download v=s
    #+end_src
 8. Start Compile
    #+begin_src bash
make -j1 V=s
    #+end_src
    *NOTE*: -j1 is followed by the number of threads. Single thread is recommended for the first compilation.
 9. Output path after compilation.
    *openwrt/bin/targets*

*** Installation

**** Proxmox :proxmox:

https://www.77bx.com/34.html

***** Firstly, Upload your compiled IMG File to ProxMox
sftp into ProxMox.
#+begin_src console
[yanboyang713@manjaro] ➜ 64 (U main) sftp root@192.168.1.2
The authenticity of host '192.168.1.2 (192.168.1.2)' can't be established.
ED25519 key fingerprint is SHA256:VPD220yr70tQsDuIn/z41hTWzte0bZ1k6wF8JjBzjiw.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.1.2' (ED25519) to the list of known hosts.
root@192.168.1.2's password:
Connected to 192.168.1.2.
#+end_src

Uploading
#+begin_src bash
sftp> put openwrt-x86-64-generic-squashfs-combined.img.gz
Uploading openwrt-x86-64-generic-squashfs-combined.img.gz to /root/openwrt-x86-64-generic-squashfs-combined.img.gz
openwrt-x86-64-generic-squashfs-combined.img.gz                                                          100%   53MB 111.4MB/s   00:00
sftp>
#+end_src

*NOTE*:
1. List local directory.
   #+begin_src console
sftp> lls
openwrt-x86-64-generic-squashfs-combined.img.gz
   #+end_src
2. List remote directory.
    #+begin_src console
sftp> ls
ROSinstall.sh  interfaces     temp
    #+end_src

***** Secondly, ssh into ProxMox and using the below Script create a new VM.
#+begin_src bash
#!/bin/bash

#vars
vmID="nil"

echo "############## Start of Script ##############

#List already existing VM's and ask for vmID

echo "== Printing list of VM's on this hypervisor!"
qm list
echo ""
read -p "Please Enter free vm ID to use:" vmID
echo ""

# Create storage dir for VM if needed.
if [ -d /var/lib/vz/images/$vmID ]
then
    echo "-- VM Directory exists! Ideally try another vm ID!"
    read -p "Please Enter free vm ID to use:" vmID
else
    echo "-- Creating VM image dir!"
    mkdir /var/lib/vz/images/$vmID
fi


# Creating VM
echo "-- Creating new CHR VM"
qm create $vmID \
  --name chr-$version \
  --net0 virtio,bridge=vmbr0 \
  --bootdisk virtio0 \
  --ostype l26 \
  --memory 2048 \
  --onboot no \
  --sockets 1 \
  --cores 1 \
  --virtio0 local-lvm:vm-$vmID-disk-0

# Decompression image
gzip -d /root/openwrt-x86-64-generic-squashfs-combined.img.gz

# Resize image
qemu-img resize /root/openwrt-x86-64-generic-squashfs-combined.img +10G

echo "-- Import RAW image to local-lvm"
qm importdisk $vmID /root/openwrt-x86-64-generic-squashfs-combined.img local-lvm

echo "############## End of Script ##############"

#+end_src


**** Raspberry Pi :raspberry:pi:
https://openwrt.org/toh/raspberry_pi_foundation/raspberry_pi

*** Set-up
1. Change Password
   #+begin_src bash
passwd
   #+end_src
2. Set *Lan* IP Address, Gateway and DNS

network -> interface -> edit

Content need to set-up:
+ General Settings:
IPv4 IP Address: 192.168.1.252
IPv4 gateway: 192.168.1.253
IPv4 bradcast: 192.168.1.0
+ Advantages Settings:
DNS set as public DNS Server: 114.114.114.114, 114,114,115,115

1. If this Openwrt as your bypass router, please follow at the below settings.
   *LAN Settings*: Let lede only be used as a pure bypass route, DHCP and IPv6 are both allocated by the main route.

   + DHCP Server -> General Settings -> choose Ignore this interface.
   + IPv6 Settings -> RA Service - Disable
   + IPv6 Settings -> DHCPv6 Service - Disable

2. Let lede obtain IPv6 information normally.
add new interface -> name: IPv6; Protocol: DHCPv6 Client Device: @lan -> create interface -> Firewall settings: lan -> create interface.


* Cluster :@Cluster:
** DONE Getting Started Set-up OVS for Proxmox :OVS:
CLOSED: [2021-12-06 Mon 12:58]
:PROPERTIES:
:EXPORT_FILE_NAME: ProxmoxOVS
:EXPORT_OPTIONS: author:nil
:ID:       ffb1b001-6dba-40f3-a222-4260015a6863
:END:

*** Introduction

*** Install Open vSwitch
Update the package index and then install the Open vSwitch packages by executing:

#+begin_src console
 apt update
 apt install ifupdown2
 apt install openvswitch-switch
#+end_src

root@pve-home:~# cat /etc/network/interfaces
https://karneliuk.com/2021/08/infrastructure-1-building-virtualized-environment-with-debian-linux-and-proxmox-on-hp-and-supermicro/

ifreload -a
ifup vmbr0

#+begin_src file
auto lo
iface lo inet loopback

auto enp6s0
iface enp6s0 inet manual

auto enp1s0
iface enp1s0 inet manual

auto enp2s0
iface enp2s0 inet manual

auto enp3s0
iface enp3s0 inet manual

auto enp5s0
iface enp5s0 inet manual

auto ens9
iface ens9 inet manual

auto vlan1
iface vlan1 inet static
        address 192.168.1.2/24
        gateway 192.168.1.1
        ovs_type OVSIntPort
        ovs_bridge vmbr0
        ovs_options vlan_mode=access
        ovs_extra set interface ${IFACE} external-ids:iface-id=$(hostname -s)-${IFACE}-vif
        dns-nameservers 192.168.1.1 8.8.8.8 8.8.4.4

auto bond0
iface bond0 inet manual
        ovs_bonds enp1s0 enp2s0 enp3s0 ens9 enp5s0
        ovs_type OVSBond
        ovs_bridge vmbr0
        ovs_options vlan_mode=native-untagged bond_mode=balance-slb

auto vmbr0
iface vmbr0 inet manual
        ovs_type OVSBridge
        ovs_ports bond0 vlan1
#+end_src


** DONE Proxmox PCI Passthrough :passthrough:proxmox:
CLOSED: [2021-12-06 Mon 16:04]
:PROPERTIES:
:EXPORT_FILE_NAME: ProxmoxPassthrough
:EXPORT_OPTIONS: author:nil
:ID:       0a2b0901-86a5-4a83-b6a7-e9f18516053a
:END:

*** Introduction
PCI passthrough allows you to use a physical PCI device (graphics card, network card) inside a VM (KVM virtualization only).

If you "*PCI passthrough*" a device, the device is not available to the host anymore.

*Note*:
PCI passthrough is an experimental feature in Proxmox VE! VMs with passthroughed devices cannot be *migrated*.

*** Enable the IOMMU
First open your bootloader kernel command line config file.

#+begin_src bash
vim /etc/default/grub
#+end_src

Find line *GRUB_CMDLINE_LINUX_DEFAULT="quiet"*

Change to:

*GRUB_CMDLINE_LINUX_DEFAULT="quiet intel_iommu=on"*

Then save the changes and update grub:

#+begin_src bash
update-grub
#+end_src

and than, reboot your PVE
#+begin_src bash
reboot
#+end_src

Verify IOMMU is enabled

after reboot, then run:
#+begin_src bash
dmesg | grep -e DMAR -e IOMMU
#+end_src

There should be a line that looks like "DMAR: IOMMU enabled". If there is no output, something is wrong.

Add *PT* Mode,
Both Intel and AMD chips can use the additional parameter "iommu=pt", added in the same way as above to the kernel cmdline.
#+begin_src file
GRUB_CMDLINE_LINUX_DEFAULT="quiet intel_iommu=on iommu=pt"
#+end_src

This enables the IOMMU translation only when necessary, the adapter does not need to use DMA translation to the memory, and can thus improve performance for hypervisor PCIe devices (which are not passthroughed to a VM)

than, update grub and root
#+begin_src bash
update-grub
reboot
#+end_src

*** Add required Modules
add to /etc/modules (default is empty)

#+begin_src file
vfio
vfio_iommu_type1
vfio_pci
vfio_virqfd
#+end_src

Then, reboot. Well Done


** DONE Getting Started MikroTik Cloud Hosted Router (CHR) on Proxmox :MikroTik:CHR:Proxmox:
CLOSED: [2021-12-06 Mon 12:28]
:PROPERTIES:
:EXPORT_FILE_NAME: CHRonProxmox
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction
Cloud Hosted Router (CHR) is a RouterOS version intended for running as a virtual machine. It supports the x86 64-bit architecture and can be used on most of the popular hypervisors such as VMWare, Hyper-V, VirtualBox, KVM and others. CHR has full RouterOS features enabled by default but has a different licensing model than other RouterOS versions.

*** Prerequires
1. read [[id:ffb1b001-6dba-40f3-a222-4260015a6863][Cluster/Getting Started Set-up OVS for Proxmox]]
2. read [[id:0a2b0901-86a5-4a83-b6a7-e9f18516053a][Cluster/Proxmox PCI Passthrough]]

*** System Minimal Requirements
+ Package version: RouterOS v6.34 or newer
+ Host CPU: 64-bit with virtualization support
+ RAM: 128MB or more
+ Disk: 128MB disk space for the CHR virtual hard drive (Max: 16GB)

*NOTE*: The minimum required RAM depends on interface count and CPU count. You can get an approximate number by using the following formula: RAM = 128 + [ 8 × (CPU_COUNT) × (INTERFACE_COUNT - 1) ]

*** The CHR has 4 license levels:

+ free
+ *p1* perpetual-1 ($45)
+ *p10* perpetual-10 ($95)
+ *p-unlimited* perpetual-unlimited ($250)

Perpetual is a lifetime license (buy once, use forever). It is possible to transfer a perpetual license to another CHR instance. A running CHR instance will indicate the time when it has to access the account server to renew it's license. If the CHR instance will not be able to renew the license it will behave as if the trial period has ran out and will not allow an upgrade of RouterOS to a newer version.

After licensing a running trial system, you must manually run the */system license renew* function from the CHR to make it active. Otherwise the system will not know you have licensed it in your account. If you do not do this before the system deadline time, the trial will end and you will have to do a complete fresh CHR installation, request a new trial and then license it with the license you had obtained.

**** Paid licenses
***** p1
p1 (perpetual-1) license level allows CHR to run indefinitely. It is limited to 1Gbps upload per interface. All the rest of the features provided by CHR are available without restrictions. It is possible to upgrade p1 to p10 or p-unlimited After the upgrade is purchased the former license will become available for later use on your account.

***** p10
p10 (perpetual-10) license level allows CHR to run indefinitely. It is limited to 10Gbps upload per interface. All the rest of the features provided by CHR are available without restrictions. It is possible to upgrade p10 to p-unlimited After the upgrade is purchased the former license will become available for later use on your account.

***** p-unlimited
The p-unlimited (perpetual-unlimited) license level allows CHR to run indefinitely. It is the highest tier license and it has no enforced limitations.

***** Free licenses
The free license level allows CHR to run indefinitely. It is limited to 1Mbps upload per interface. All the rest of the features provided by CHR are available without restrictions. To use this, all you have to do is download disk image file from our download page and create a virtual guest.

*** CHR ProxMox installation
**** Step 1: Registration a new mikrotik account, if you have NOT it.
https://mikrotik.com/client

**** Step 2: Installation
I recommand using the below Bash script to install. You need to *ssh* into your ProxMox and run below script.

Before run this script, Please do some research, which version of ROS you want to install. Please, check this [[https://mikrotik.com/download][link]].

What this script does:
+ Stores tmp files in: /root/temp dir.
+ Downloads raw image archive from MikroTik download page.
+ Converts image file to qcow format.
+ Creates basic VM that is attached to MGMT bridge.

*Important Note*:
1. Make sure you have a MGMT bridge, which named *vmbr0*. If you have NOT  avaiable bridge, please have a look [[id:ffb1b001-6dba-40f3-a222-4260015a6863][Cluster/Getting Started Set-up OVS for Proxmox]]
2. If your network card is Intel i211, Please install RouterOS 7, not RouterOS 6. RouterOS 6 does NOT support i211 network card.

#+begin_src bash
#!/bin/bash

#vars
version="nil"
vmID="nil"

echo "############## Start of Script ##############

## Checking if temp dir is available..."
if [ -d /root/temp ]
then
    echo "-- Directory exists!"
else
    echo "-- Creating temp dir!"
    mkdir /root/temp
fi

# apt install unzip
echo "Install unzip"
apt update
apt install unzip -y

# Ask user for version
echo "## Preparing for image download and VM creation!"
read -p "Please input CHR version to deploy ( 6.49.1 (Stable) 6.49rc2 (Testing) 7.1 (Testing)):" version
# Check if image is available and download if needed
if [ -f /root/temp/chr-$version.img ]
then
    echo "-- CHR image is available."
else
    echo "-- Downloading CHR $version image file."
    cd  /root/temp
    echo "---------------------------------------------------------------------------"
    wget https://download.mikrotik.com/routeros/$version/chr-$version.img.zip
    unzip chr-$version.img.zip
    echo "---------------------------------------------------------------------------"
fi
# List already existing VM's and ask for vmID
echo "== Printing list of VM's on this hypervisor!"
qm list
echo ""
read -p "Please Enter free vm ID to use:" vmID
echo ""
# Create storage dir for VM if needed.
if [ -d /var/lib/vz/images/$vmID ]
then
    echo "-- VM Directory exists! Ideally try another vm ID!"
    read -p "Please Enter free vm ID to use:" vmID
else
    echo "-- Creating VM image dir!"
    mkdir /var/lib/vz/images/$vmID
fi

# Resize image
qemu-img resize /root/temp/chr-$version.img +10G

# Creating VM
echo "-- Creating new CHR VM"
qm create $vmID \
  --name chr-$version \
  --net0 virtio,bridge=vmbr0 \
  --bootdisk virtio0 \
  --ostype l26 \
  --memory 256 \
  --onboot no \
  --sockets 1 \
  --cores 1 \
  --virtio0 local-lvm:vm-$vmID-disk-0

# import image
echo "-- Import RAW image to local-lvm"
qm importdisk $vmID /root/temp/chr-$version.img local-lvm

# remove downloaded raw image and zip
rm /root/temp/chr-$version.img.zip
rm /root/temp/chr-$version.img

echo "############## End of Script ##############"
#+end_src

*NOTE*: ERROR: storage 'local' does not support content-type 'images'
*NOTE*: Useful snippet to clean up the BASH script from Windows formatting that may interfere with script if it's edited on a Windows workstation:

#+begin_src console
sed -i -e 's/\r$//' *.sh
#+end_src

**** Step 3: Add WAN port to ROS
I am add a passthrough NIC as WAM, so before you read this section. Please, read [[id:0a2b0901-86a5-4a83-b6a7-e9f18516053a][Cluster/Proxmox PCI Passthrough]] first.

When you *DONE* set-up passthrough, now lets we list network interfaces name with PCI ID and add WAN.

***** List network interface name with PCI ID

#+begin_src bash
apt install lshw
lshw -class network
#+end_src

For example, you can found interface name with bus id at below.
#+begin_src file
*-network
       description: Ethernet interface
       product: I211 Gigabit Network Connection
       vendor: Intel Corporation
       physical id: 0
       bus info: pci@0000:06:00.0
       logical name: enp6s0
       version: 03
       serial: 00:90:27:e5:8d:09
       capacity: 1Gbit/s
       width: 32 bits
       clock: 33MHz
       capabilities: pm msi msix pciexpress bus_master cap_list ethernet physical tp 10bt 10bt-fd 100bt 100bt-fd 1000bt-fd autonegotiation
       configuration: autonegotiation=on broadcast=yes driver=igb driverversion=5.13.19-2-pve firmware=0. 6-1 latency=0 link=no multicast=yes port=twisted pair
       resources: irq:17 memory:df000000-df01ffff ioport:9000(size=32) memory:df020000-df023fff
#+end_src

***** Add WAN
now lets we add WAN to ROS.

Go to *Hardware* Section -> *Add* -> *PCI Device*

Choose your WAN need to add in.

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1638780489/PVE/hardwareAdd_aqh8vq.png]]


**** Step 4: Start your ROS
check interface name:
#+begin_src bash
interface print
#+end_src

check IP address:
#+begin_src bash
ip export
#+end_src

remove IP address
#+begin_src bash
ip address remove IDnum
#+end_src

Assign IP address:
#+begin_src bash
ip address add address=192.168.1.253/24 interface=ether2
#+end_src
**** Step 5: Disable API, API-SSL, Telnet, FTP, WWW and WWW-SSL
#+begin_src bash
ip service disable api,api-ssl,ftp,ssh,telnet,www,www-ssl
#+end_src

**** Step 6: interface rename
#+begin_src bash
interface set ether2 name="LAN"
interface set ether1 name="WAN"
#+end_src


**** Step 7: add dhcp client
#+begin_src bash
ip dhcp-client print detail

ip dhcp-client set interface=WAN disable=no use-peer-dns=no

ip dhcp-client print detail
#+end_src

**** Step 8: add DNS
https://wiki.mikrotik.com/wiki/Manual:IP/DNS

#+begin_src bash
ip dns set servers=192.168.1.252 max-udp-packet-size=8192
#+end_src

#+begin_src bash
ip dns static add name=ros address=192.168.1.253
#+end_src
**** Step 9: firwall NAT
#+begin_src bash
ip firewall nat add chain=srcnat action=masquerade
#+end_src

**** Step 10: IP Pool
https://wiki.mikrotik.com/wiki/Manual:IP/Pools
#+begin_src bash
ip pool add name=ip-pool ranges=192.168.1.100-192.168.1.200
#+end_src

**** Step 11: DHCP Server
#+begin_src bash
ip dhcp-server add name=LANDHCP interface=LAN address-pool=ip-p
ool
#+end_src

#+begin_src bash
ip dhcp-server network add address=192.168.1.0/24 gateway=192.1
68.1.252
#+end_src


** DONE Getting Started ROS and Openwrt with Proxmox :ROS:Openwrt:proxmox:
CLOSED: [2021-11-05 Fri 19:52]
:PROPERTIES:
:EXPORT_FILE_NAME: ROSandOpenwrtProxmox
:EXPORT_OPTIONS: author:nil
:END:

*** mikrotik
https://mikrotik.com/software


*** OpenWrt
https://openwrt.org/downloads
https://downloads.openwrt.org/releases/21.02.1/targets/x86/
https://downloads.openwrt.org/releases/21.02.1/targets/x86/64/
*** Migration of servers to Proxmox VE
https://pve.proxmox.com/wiki/Migration_of_servers_to_Proxmox_VE
*** VLAN
https://engineerworkshop.com/blog/configuring-vlans-on-proxmox-an-introductory-guide/


** DONE Getting Started Configuring VLANs on Proxmox :VLAN:Proxmox:
CLOSED: [2021-11-05 Fri 19:52]
:PROPERTIES:
:EXPORT_FILE_NAME: vlan
:EXPORT_OPTIONS: author:nil
:END:
*** Introduction
A virtualization server allows you to run multiple machines, virtual machines (VMs), on one physical device, also known as the host. There could be many different VMs each for different tasks. In this guide, we will discuss configuring your Proxmox virtualization server to use VLANs so that you can group related VMs onto their own subnet.
*** Motivation
For security, as well as organizational purposes, physical machines are often separated on the network from each other by VLANs. By logically separating devices based on their functionality with these VLANs, we can make sure that our family's personal devices aren't sitting out in the open on the same subnet exposed to our internet-facing web servers. This is fairly easy on your regular network setup because the devices are physically separate from each other and so each ethernet port physically connected to a device can be assigned an individual VLAN.

However, this system starts to break down when faced with virtualization servers. This is because diverse virtual machines are all sitting on the same physical host, forcing each VM to share the same physical connection. With a standard bridge between the individual VM and the host's NIC, we necessarily end up with each VM on the same subnet as the Proxmox host itself. Additionally, we end up with each VM on the same subnet as every other VM on that host. Not ideal.
*** Solution
Thankfully there's a way around this. In Proxmox, you can make your virtual bridge VLAN-aware so you can pass multiple VLANs through to your Proxmox server using only a single physical port. The individual VMs can then be configured to use whichever VLAN you choose.


** DONE Create Proxmox cloud-init template :cloud:init:Proxmox:
CLOSED: [2021-12-03 Fri 18:27]
:PROPERTIES:
:EXPORT_FILE_NAME: clouldInit
:EXPORT_OPTIONS: author:nil
:ID:       632ee4dd-5bdd-4103-bab1-b3c1110aeac6
:END:
*** Overview
In this article, I'll demonstrate how to create a cloud-init enabled Ubuntu 20.04 LTS base image to use on Proxmox VE.

*** Cloud Native Image
The tradition packer builder to build a base image from an ISO file. Modern Linux distributions are increasingly moving away from this install method and preseed files. Rather, disk images are provided with the OS pre-installed, and configuration is performed via cloud-init. We will create a Proxmox KVM base image using Ubuntu's KVM cloud image.

*** Proxmox Script
The Proxmox API doesn't appear to offer the full functionality provided by the native shell commands to create a template, so we will run a script via SSH or Proxmox node's GUI shell.

The Script you can found on my GitHub. There is the Link.

The below sections, I will explain this Script step by steps.

*** Step 1: Download the image
We are downloading the kvm disk image.

*Note*: This is a qcow2 image format with an extension of .img, Promxox doesn't like this so we rename the disk image to .qcow2

#+begin_src bash
SRC_IMG="https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64-disk-kvm.img"
IMG_NAME="focal-server-cloudimg-amd64-disk-kvm.qcow2"
wget -O $IMG_NAME $SRC_IMG
#+end_src

*** Step 2: Add QEMU Guest Agent
The Ubuntu 20.04 image we are going to use does not include the *qemu-guest-agent* package which is needed for the *Guest VM* to report its IP details back to Proxmox. This is required for Packer to communicate with the VM after cloning. The template. *libguestfs-tools* will allow us to embed qemu-guest-agent into the image. You can also add any additional packages you'd like in your base image. Personally, I prefer to customize this base image later with packer so that the packages can live in source control.

#+begin_src bash
apt update
apt install -y libguestfs-tools
virt-customize --install qemu-guest-agent -a $IMG_NAME
#+end_src

*** Step 3: Create a VM in Proxmox with required settings and convert to template
For best performance, virtio "hardware" should be used. Additionally, cloud-init requires a serial console and cloudinit IDE (CDROM) drive. We will set the network config to DHCP so that we get an IP address. Lastly, we will expand the template disk image size so we have space to install items later. It appears packer doesn't support doing this later.

you will need to change the user name, password, and add the ssh public key so we can connect to the VM later using Ansible and terraform. update the variables and click on Regenerate Image

#+begin_src bash
TEMPL_NAME="ubuntu2004-cloud"
VMID="9000"
MEM="512"
DISK_SIZE="32G"
DISK_STOR="local-lvm"
NET_BRIDGE="vmbr0"
qm create $VMID --name $TEMPL_NAME --memory $MEM --net0 virtio,bridge=$NET_BRIDGE
qm importdisk $VMID $IMG_NAME $DISK_STOR
qm set $VMID --scsihw virtio-scsi-pci --scsi0 $DISK_STOR:vm-$VMID-disk-0
qm set $VMID --ide2 $DISK_STOR:cloudinit
qm set $VMID --boot c --bootdisk scsi0
qm set $VMID --serial0 socket --vga serial0
qm set $VMID --ipconfig0 ip=dhcp
qm resize $VMID scsi0 $DISK_SIZE
qm template $VMID
# Remove downloaded image
rm $IMG_NAME
#+end_src

*** References
1. https://gist.github.com/chriswayg/43fbea910e024cbe608d7dcb12cb8466
2. https://whattheserver.com/proxmox-cloud-init-os-template-creation/
3. https://norocketscience.at/deploy-proxmox-virtual-machines-using-cloud-init/
4. https://pve.proxmox.com/wiki/Cloud-Init_Support
5. https://blog.dustinrue.com/2020/05/going-deeper-with-proxmox-cloud-init/
6. https://gist.github.com/mike1237/cce83a74f898b11c2cec911204568cf9


** DONE Creating ProxMox templates with packer :packer:
CLOSED: [2021-12-14 Tue 12:08]
:PROPERTIES:
:EXPORT_FILE_NAME: packer
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction
With proxmox when can create templates for our VMS so we can have a standard starting point to install our applications on top of, these templates can be useful too so that you can pre-install packages for authentication, security, logging and etc without anyone else needing to think about it.

However, creating and managing these templates can become a challenge with how time-consuming and manual it can be. I want to show you how you can make this process more standardized and automated with the use of [[https://www.packer.io/][packer]] to allow you to declare your proxmox templates as code.

*** What is packer
Packer is a utility that allows you to build virtual machine images so that you can define a golden image as code. Packer can be used to create images for almost all of the big cloud providers such as AWS, GCE, Azure and Digital Ocean, or can be used with locally installed hypervisors such as VMWare, Proxmox and a few others.

To build an image with packer we need to define our image through a template file. The file uses the JSON format and comprises of 3 main sections that are used to define and prepare your image.

+ [[https://www.packer.io/docs/terminology#builders][Builders]]: Components of Packer that are able to create a machine image for a single platform. A builder is invoked as part of a build in order to create the actual resulting images.

+ [[https://www.packer.io/docs/terminology#provisioners][Provisioners]]: Install and configure software within a running machine prior to that machine being turned into a static image. Example provisioners include shell scripts, Chef, Puppet, etc.

+ [[https://www.packer.io/docs/terminology#post-processors][Post Processors]]: Take the result of a builder or another post-processor and process that to create a new artifact. Examples of post-processors are compress and upload to compress and upload artifacts respectively, etc.

By using packer we can define our golden VM image as code so that we can easily build identically configured images on demand so that all your machines are running the same image and can also be easily updated to a new image when needed.

*** Packer template
Now that we have our cloud-init enabled image on Proxmox, we can use Packer to create a template based off of this template.
Ensure to set the scsi_controller="virtio-scsi-pci" and qemu_agent=true.

I'd recommend adding the Proxmox variables to a var file.

#+begin_src bash
packer build --var-file=./proxmox.pkvars.hcl --var "proxox_template_name=test-output-template" --var "proxmox_source_template=ubuntu2004-cloud" base.pkr.hcl
#+end_src

*** Final
Now that you've created a template using packer from the base template, you can use Terraform to deploy that VM!

*** References
1. https://dev.to/aaronktberry/creating-proxmox-templates-with-packer-1b35


** DONE Build a Kubernetes cluster on Proxmox via Ansible and Terraform :k8s:proxmox:ansible:terraform:
CLOSED: [2021-12-01 Wed 20:27]
:PROPERTIES:
:EXPORT_FILE_NAME: k8sOnProxmox
:EXPORT_OPTIONS: author:nil
:END:

[[https://miro.medium.com/max/1400/1*jL6SE1nSaPQb4EOWGnbZpw.jpeg]]

*** Overview
Proxmox is an open-source hypervisor that have enterprise capabilities and a large community behind it.

For Terraform and Ansible, I always like the idea of infrastructure as code (iac) and Terraform and Ansible just make it easy to accomplish.

The idea here was to be able to spin up a k3s cluster with minimum effort so I can spin it up and down for ever project that I would like to run.

*** Prerequires
1. read [[id:ee79f2a9-445b-4756-9853-e0819fda588c][DevOps/Terraform Beginner's Guide]]
2. read [[id:5012520e-c7d0-4b8e-8575-6ecf70e819b6][DevOps/Ansible Beginner's Guide]]
3. read [[id:632ee4dd-5bdd-4103-bab1-b3c1110aeac6][Cluster/Create Proxmox cloud-init template]]
4. read [[id:18169cab-9be7-45ba-8cbb-31379873f0c4][DevOps/Install Terrafom with Quick start tutorial]]

*** System requirements
+ The deployment environment must have [[https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html][Ansible]] 2.4.0+
+ Following this article: [[id:18169cab-9be7-45ba-8cbb-31379873f0c4][DevOps/Install Terrafom with Quick start tutorial]] install [[https://learn.hashicorp.com/tutorials/terraform/install-cli][Terraform]].
+ [[https://www.proxmox.com/en/proxmox-ve][Proxmox]] server

*** Step 1: Create Cloud-init VM template
Please, following the step by step document on [[id:632ee4dd-5bdd-4103-bab1-b3c1110aeac6][Cluster/Create Proxmox cloud-init template]]
*** Step 2: Generating public/private key pair
#+begin_src bash
ssh-keygen -t rsa -f ~/.ssh/k3s
# Print Public Key
cat /root/.ssh/k3s.pub
#+end_src
*** Step 3: Add public key, User name and Password to template
This is a example below:
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1639731593/PVE/pve-cloud-init_ef96tq.png]]

When you done, click *Regenerate Image*.

*** Step 2: terraform setup
Clone the repo to get all the files and cd into the folder.
#+begin_src bash
git clone https://github.com/NatiSayada/k3s-proxmox-terraform-ansible
cd k3s-proxmox-terraform-ansible
#+end_src

Our terraform also creates a dynamic host file for Ansible, so we need to create the files first.

#+begin_src bash
cp -R inventory/sample inventory/my-cluster
#+end_src

Rename the file terraform/vars.sample to terraform/vars.tf and update all the vars. there you can select how many nodes would you like to have on your cluster and configure the name of the base image.
to run the Terrafom, you will need to cd into terraform and run:

#+begin_src bash
terraform init
terraform plan
terraform apply
#+end_src

It can take some time to create the servers on Proxmox but you can monitor them over Proxmox. it should look like this now:

Add alt text
[[https://miro.medium.com/max/432/0*vnMepxEQgFND4dOw]]

*** Step 3: Ansible setup
First, update the var file in inventory/my-cluster/group_vars/all.yml and update the user name that you’re selected in the cloud-init setup.
after you run the Terrafom file, your host file should look like this:

#+begin_src yml
[master]
192.168.3.200 Ansible_ssh_private_key_file=~/.ssh/proxk3s
[node]
192.168.3.202 Ansible_ssh_private_key_file=~/.ssh/proxk3s
192.168.3.201 Ansible_ssh_private_key_file=~/.ssh/proxk3s
192.168.3.198 Ansible_ssh_private_key_file=~/.ssh/proxk3s
192.168.3.203 Ansible_ssh_private_key_file=~/.ssh/proxk3s
[k3s_cluster:children]
master
node
#+end_src

Start provisioning of the cluster using the following command:

#+begin_src bash
Ansible-playbook site.yml -i inventory/my-cluster/hosts.ini
#+end_src

this playbook will install k3s in 644 mode and helm.
the 644 mode is the permission needed for the /etc/rancher/k3s/k3s.yaml config file so it can be imported to rancher. so if you would also like to check out rancher.. you are good to go!

*** Step 4: Kubeconfig
To get access to your Kubernetes cluster just copy the k3s yaml file to your kube config file and change the ip address of the server

#+begin_src bash
scp debian@master_ip:/etc/rancher/k3s/k3s.yaml ~/.kube/config
#+end_src

run kubectl get nodes to check you cluster nodes status
[[https://miro.medium.com/max/636/1*JgAE4EKXnCL-bEp7p0kOkg.png]]

*** Summary
Now you should have a full blown k3s cluster running on Proxmox! all you have left is to start running some deployments.

*** References
1. https://medium.com/@ssnetanel/build-a-kubernetes-cluster-using-k3s-on-proxmox-via-ansible-and-terraform-c97c7974d4a5



* DevOps :@DevOps:
** DONE DevOps Beginner's Guide :DevOps:
CLOSED: [2021-12-02 Thu 11:18]
:PROPERTIES:
:EXPORT_FILE_NAME: DevOps
:EXPORT_OPTIONS: author:nil
:ID:       807c90ce-9dfd-44be-861b-b2893282ed5f
:END:

*** Overview
In this blog, I discussed *what is DevOps*, and why it has *gained* so much *traction* in the IT industry lately.

*** What Is DevOps?
It is a combination of practices that *streamline* the *automation* and *integration* of processes between the *software development* and *IT teams*. This will help them to *build*, *test*, and *release software* in a faster and more reliable way.

**** Purpose
The term was formed by combining the words *"development"* and *"operations"* and signifies a cultural shift that *helps bridge the gap between the development and operation teams*.
**** Goal
 The *goal* of DevOps is to change and improve the relationship by advocating better communication and collaboration between these two business units.

*** DevOps Model For Teams
Teams using the DevOps model are able to evolve and improve their products at a higher rate over the organizations that use traditional processes. *Collaboration*, *Communication*, and *Integration* are the key elements of incorporating DevOps into any development and delivery setting.

This speed enables the teams (and in turn their organizations) to better serve their customers and compete more effectively in the market.

[[https://d1.awsstatic.com/product-marketing/DevOps/DevOps_feedback-diagram.ff668bfc299abada00b2dcbdc9ce2389bd3dce3f.png]]

*** DevOps Advantages
Improvement of collaboration between all stakeholders from planning through delivery and automation of the delivery process in order to:

+ Increase deployment frequency
+ Achieve faster time to market
+ Decrease the failure rate of new releases
+ Shorten the lead time between fixes
+ Improve mean time to recovery

According to the State of DevOps Report, "high-performing IT organizations deploy 30x more frequently with 200x shorter lead times; they have 60x fewer failures and recover 168x faster."

*** DevOps Principles
The phrase “The Three Ways” is used to describe the underlying principles of the DevOps movement.

**** The First Way: Principles of Flow
The First Way states the following, about the flow of work:

+ Work should only flow in one direction
+ No known defect should be passed downstream
+ Always seek to increase the flow

**** The Second Way: Principles of Feedback
The Second Way describes the feedback process as the following:

+ Establish an upstream feedback loop
+ Shorten the feedback loop
+ Amplify the feedback loop
  [[https://blog-assets.freshworks.com/freshservice/wp-content/uploads/2019/01/23142830/2.png]]

**** The Third Way: Principles of Continuous Learning
The Third Way describes the environment and culture, as the following practices

+ Promote experimentation
+ Learn from success and failure
+ Constant improvement
+ Seek to achieve mastery through practice

[[https://blog-assets.freshworks.com/freshservice/wp-content/uploads/2019/01/23142856/3.png]]


** DONE Terraform Beginner's Guide :Terraform:
CLOSED: [2021-12-02 Thu 11:09]
:PROPERTIES:
:EXPORT_FILE_NAME: terraform
:EXPORT_OPTIONS: author:nil
:ID:       ee79f2a9-445b-4756-9853-e0819fda588c
:END:
*** Overview
In this blog post, I am going to cover a brief introduction of *Infrastructure as Code (IaC)*, *Terraform*, its *lifecycle*, and all the core concepts that every beginner should know. I have tried to cover all the topics in this beginner’s guide that will give you a quick start for using Terraform.

*** Prerequires
1.[[id:807c90ce-9dfd-44be-861b-b2893282ed5f][DevOps/DevOps Beginner's Guide]]

*** What Is Infrastructure as Code (IaC)?
*Infrastructure as Code (IaC)* is a widespread terminology among DevOps professionals and a key DevOps practice in the industry. It is the process of managing and provisioning the complete IT infrastructure (comprises both physical and virtual machines) using machine-readable definition files. It helps in automating the complete data center by using programming scripts.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/Explanation-of-how-IaC-works.jpg]]

*** Popular IaC Tools:
1. *Terraform*: An open-source declarative tool that offers pre-written modules to build and manage an infrastructure.
2. *Chef*: A configuration management tool that uses cookbooks and recipes to deploy the desired environment. Best used for Deploying and configuring applications using a pull-based approach.
3. *Puppet*: Popular tool for configuration management that follows a Client-Server Model. Puppet needs agents to be deployed on the target machines before the puppet can start managing them.
4. *Ansible*: Ansible is used for building infrastructure as well as deploying and configuring applications on top of them. Best used for Ad hoc analysis.
5. *Packer*: Unique tool that generates VM images (not running VMs) based on steps you provide. Best used for Baking compute images.
6. *Vagrant*: Builds VMs using a workflow. Best used for Creating pre-configured developer VMs within VirtualBox.

*** What Is Terraform?
*Terraform* is one of the most popular *Infrastructure-as-code (IaC) tool*, used by DevOps teams to automate infrastructure tasks. It is used to automate the provisioning of your cloud resources. Terraform is an open-source, cloud-agnostic provisioning tool developed by HashiCorp and written in GO language.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/logo-hashicorp-e1605707253653.png]]

Benefits of using Terraform:

+ Does orchestration, not just configuration management
+ Supports multiple providers such as AWS, Azure, Oracle, GCP, and many more
+ Provide immutable infrastructure where configuration changes smoothly
+ Uses easy to understand language, HCL (HashiCorp configuration language)
+ Easily portable to any other provider

*** Terraform Lifecycle
Terraform lifecycle consists of – *init*, *plan*, *apply*, and *destroy*.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/terraform-lifecycle.png]]

1. *Terraform init* initializes the (local) Terraform environment. Usually executed only once per session.
2. *Terraform plan* compares the Terraform state with the as-is state in the cloud, build and display an execution plan. This does not change the deployment (read-only).
3. *Terraform apply* executes the plan. This potentially changes the deployment.
4. *Terraform destroy* deletes all resources that are governed by this specific terraform environment.

*** Terraform Core Concepts
1. *Variables*: Terraform has input and output variables, it is a key-value pair. Input variables are used as parameters to input values at run time to customize our deployments. Output variables are return values of a terraform module that can be used by other configurations.

   Please, read article on [[https://k21academy.com/terraform-iac/variables-in-terraform/][Terraform Variables]]

2. *Provider*: Terraform users provision their infrastructure on the major cloud providers such as AWS, Azure, OCI, and others. A provider is a plugin that interacts with the various APIs required to create, update, and delete various resources.

   Please, read article to know more about [[https://k21academy.com/terraform-iac/terraform-providers-overview/][Terraform Providers]]

3. *Module*: Any set of Terraform configuration files in a folder is a module. Every Terraform configuration has at least one module, known as its *root module*.

4. *State*: Terraform records information about what infrastructure is created in a Terraform state file. With the state file, Terraform is able to find the resources it created previously, supposed to manage and update them accordingly.

5. *Resources*: Cloud Providers provides various services in their offerings, they are referenced as Resources in Terraform. Terraform resources can be anything from compute instances, virtual networks to higher-level components such as DNS records. Each resource has its own attributes to define that resource.

6. *Data Source*: Data source performs a read-only operation. It allows data to be fetched or computed from resources/entities that are not defined or managed by Terraform or the current Terraform configuration.

7. *Plan*: It is one of the stages in the Terraform lifecycle where it determines what needs to be created, updated, or destroyed to move from the real/current state of the infrastructure to the desired state.

8. *Apply*: It is one of the stages in the Terraform lifecycle where it applies the changes real/current state of the infrastructure in order to achieve the desired state.

Check Out: Our previous blog post on [[https://k21academy.com/terraform-iac/terraform-cheat-sheet/][Terraform Cheat Sheet]].

*** Terraform Installation
Terraform Installation
Before you start working, make sure you have Terraform installed on your machine, it can be installed on any OS, say Windows, macOS, Linux, or others. Terraform installation is an easy process and can be done in a few minutes.

Read our blog to know how to [[https://k21academy.com/terraform-iac/terraform-installation-overview/][install Terraform]] in Linux, Mac, Windows

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/Terraform-installation.jpg]]

*** Terraform Providers
A provider is responsible for understanding API interactions and exposing resources. It is an executable plug-in that contains code necessary to interact with the API of the service. Terraform configurations must declare which providers they require so that Terraform can install and use them.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/Terraform-provider-api-call.png]]

Terraform has over a hundred providers for different technologies, and each provider then gives terraform user access to its resources. So through AWS provider, for example, you have access to hundreds of AWS resources like EC2 instances, the AWS users, etc.

*** Terraform Configuration Files
Configuration files are a set of files used to describe infrastructure in Terraform and have the file extensions .tf and .tf.json. Terraform uses a declarative model for defining infrastructure. Configuration files let you write a configuration that declares your desired state. Configuration files are made up of resources with settings and values representing the desired state of your infrastructure.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/terraform-config-files-e1605834689106.png]]

A Terraform configuration is made up of one or more files in a directory, provider binaries, plan files, and state files once Terraform has run the configuration.

1. *Configuration file (*.tf files)*: Here we declare the provider and resources to be deployed along with the type of resource and all resources specific settings

2. *Variable declaration file (variables.tf or variables.tf.json)*: Here we declare the input variables required to provision resources

3. *Variable definition files (terraform.tfvars)*: Here we assign values to the input variables

4. *State file (terraform.tfstate)*: a state file is created once after Terraform is run. It stores state about our managed infrastructure.

*** Getting started using Terraform
To get started building infrastructure resources using Terraform, there are few things that you should take care of. The general steps to deploy a resource(s) in the cloud are:

1. Set up a Cloud Account on any cloud provider (AWS, Azure, OCI)
2. Install Terraform
3. Add a provider – AWS, Azure, OCI, GCP, or others
4. Write configuration files
5. Initialize Terraform Providers
6. PLAN (DRY RUN) using terraform plan
7. APPLY (Create a Resource) using terraform apply
8. DESTROY (Delete a Resource) using terraform destroy

*** Import Existing Infrastructure
Terraform is one of the great IaC tools with which, you can deploy all your infrastructure’s resources. In addition to that, you can manage infrastructures from different cloud providers, such as AWS, Google Cloud, etc. But what if you have already created your infrastructure manually?

Terraform has a really nice feature for importing existing resources, which makes the migration of existing infrastructure into Terraform a lot easier.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/terraform-import-workflow-diagram.png]]

Currently, Terraform can only import resources into the state. It does not generate a configuration for them. Because of this, prior to running terraform import it is necessary to write manually a resource configuration block for the resource, to which the imported object will be mapped. For example:

#+begin_src file
resource "aws_instance" "import_example" {
  # ...instance configuration...
}
#+end_src

Now terraform import can be run to attach an existing instance to this resource configuration:

#+begin_src console
$ terraform import aws_instance.import_example i-03efafa258104165f
#+end_src

This command locates the AWS instance with ID i-03efafa258104165f (which has been created outside Terraform) and attaches it to the name aws_instance.import_example in the Terraform state.

*** Conclusion
I hope the above gives you an idea about how you can get started with Terraform.

*** Related/References


** DONE Install Terrafom with Quick start tutorial :Terraform:tutorial:
CLOSED: [2021-12-17 Fri 17:39]
:PROPERTIES:
:EXPORT_FILE_NAME: terraformInstall
:EXPORT_OPTIONS: author:nil
:ID:       18169cab-9be7-45ba-8cbb-31379873f0c4
:END:

*** Pre-reading
1. [[id:807c90ce-9dfd-44be-861b-b2893282ed5f][DevOps/DevOps Beginner's Guide]]
2. [[id:ee79f2a9-445b-4756-9853-e0819fda588c][DevOps/Terraform Beginner's Guide]]
3. [[id:54145bec-00b1-4044-9f97-8a9ddd6df50f][DevOps/The comparison and introduction between Terraform and Ansible]]

*** Install
**** Arch Linux
#+begin_src bash
yay -S terraform
#+end_src

**** Ubuntu/Debian
Ensure that your system is up to date, and you have the gnupg, software-properties-common, and curl packages installed. You will use these packages to verify HashiCorp's GPG signature, and install HashiCorp's Debian package repository.
#+begin_src bash
sudo apt-get update && sudo apt-get install -y gnupg software-properties-common curl
#+end_src

Add the HashiCorp GPG key.

#+begin_src bash
curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
#+end_src

Add the official HashiCorp Linux repository.
#+begin_src bash
sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
#+end_src

Update to add the repository, and install the Terraform CLI.
#+begin_src bash
sudo apt-get update && sudo apt-get install terraform
#+end_src

*TIP*: Now that you have added the HashiCorp repository, you can install Vault, Consul, Nomad and Packer with the same command.


*** Verify the installation
Verify that the installation worked by opening a new terminal session and listing Terraform's available subcommands.

#+begin_src bash
terraform -help
#+end_src

Add any subcommand to terraform -help to learn more about what it does and available options.

#+begin_src bash
terraform -help plan
#+end_src

*** Troubleshoot
If you get an error that *terraform* could not be found, your *PATH* environment variable was not set up properly. Please go back and ensure that your *PATH* variable contains the directory where Terraform was installed.

*** Enable tab completion for ZSH :ZSH:zsh:
You can enable tab completion for Terraform commands.

To enable autocomplete, first ensure that a config file exists for your ZSH shell

#+begin_src bash
touch ~/.zshrc
#+end_src

Then install the autocomplete package.

#+begin_src file
terraform -install-autocomplete
#+end_src

Once the autocomplete support is installed, you will need to restart your shell.

*Importance NOTE*: If you want to use the same configuration on your different computers and VMs, please read [[id:5daeba1f-2aa5-492a-b3e7-3343722def0b][Linux/Managing Your Dotfiles With Git and Make]].

*** Quick start tutorial
Now that you've installed Terraform, you can provision an NGINX server in less than a minute using Docker on Mac, Windows, or Linux.

To follow this tutorial on Linux, first install Docker Engine for your distribution.

Create a directory named learn-terraform-docker-container.

#+begin_src bash
$ mkdir learn-terraform-docker-container
#+end_src

Then, navigate into it.

#+begin_src bash
$ cd learn-terraform-docker-container
#+end_src

Paste the following Terraform configuration into a file and name it main.tf.

#+begin_src file
terraform {
  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "~> 2.13.0"
    }
  }
}

provider "docker" {}

resource "docker_image" "nginx" {
  name         = "nginx:latest"
  keep_locally = false
}

resource "docker_container" "nginx" {
  image = docker_image.nginx.latest
  name  = "tutorial"
  ports {
    internal = 80
    external = 8000
  }
}
#+end_src

Initialize the project, which downloads a plugin that allows Terraform to interact with Docker.

#+begin_src bash
$ terraform init
#+end_src

Provision the NGINX server container with apply. When Terraform asks you to confirm type yes and press ENTER.

#+begin_src bash
$ terraform apply
#+end_src

Verify the existence of the NGINX container by visiting localhost:8000 in your web browser or running docker ps to see the container.

To stop the container, run terraform destroy.
#+begin_src bash
$ terraform destroy
#+end_src

You've now provisioned and destroyed an NGINX webserver with Terraform.

*** Reference List
1. https://learn.hashicorp.com/tutorials/terraform/install-cli


** DONE Ansible Beginner's Guide :Ansible:
CLOSED: [2021-12-02 Thu 17:58]
:PROPERTIES:
:EXPORT_FILE_NAME: ansible
:EXPORT_OPTIONS: author:nil
:ID:       5012520e-c7d0-4b8e-8575-6ecf70e819b6
:END:
*** Overview
This article covers all the aspects of Ansible, a tool used in DevOps for the Management, Deployment, and Orchestration of IT Infrastructure.

*** What Is Ansible?
Ansible is a simple configuration management and IT automation engine for multi-tier deployments. It automates both cloud and on-premise provisioning & configuration. It automates cloud provisioning. Rather than managing one system at a time, Ansible uses a model that inter-relates the entire IT infrastructure and enables you to manage everything using something called an Infrastructure as Code (IAC) approach. Ansible is secure and agentless. It relies on OpenSSH and the code written in YAML format. Ansible nodes are run on Unix systems but they can be used to configure changes across Unix as well as Windows systems.

*** Who should learn Ansible?
Ansible is a part of the DevOps stack. Ansible means automation. Ansible seamlessly connects workflow orchestration with configuration management and provisioning deployment. Ansible has various use cases in Provisioning, Configuration Management, Application Deployment, Continuous Deployment, Automation, and Orchestration. So, if you are looking forward to a career in DevOps, IT automation, and managing cloud infrastructure then Ansible is a must-have.

*** Why Use Ansible?
+ *No Agent*: As long as the box can be ssh’d into and it has python, it can be configured with Ansible.
+ *Idempotent*: Ansible’s whole architecture is structured around the concept of idempotency. The core idea here is that you only do things if they are needed and that things are repeatable without side effects.
+ *Declarative Not Procedural*: Other configuration tools tend to be procedural do this and then do that and so on. Ansible works by you writing a description of the state of the machine that you want and then it takes steps to fulfill that description.
+ *Tiny Learning Curve*: Ansible is quite easy to learn. It doesn’t require any extra knowledge.

*** Ansible Use Cases
+ *Provisioning*: Provisioning is creating new infrastructure. Ansible allows for application management, deployment, orchestration, and configuration management.
+ *Continuous Delivery*: Ansible provides a simpler way to automatically deploy applications. All required services for a deployment can be configured from a single system. Continuous Integration (CI) tool can be used to run Ansible playbook which can be used to test and automatically deploy the application to production if tests are passed.
+ *Application Deployment*: Ansible provides a simpler way to deploy applications across the infrastructure. Deployment of multi-tier applications can be simplified and the infrastructure can be easily changed over time.
+ *Ansible for Cloud Computing*: Ansible makes it easy to provision instances across all cloud providers. Ansible contains multiple modules and allows to manage of large cloud infrastructure across the public-private and hybrid cloud.
+ *Ansible for Security and Compliance*: You can define security policies in Ansible which will automate security policy across all machines in the network. Security roles once configured in an Ansible node will be embedded across all machines in the network automatically.

*** Ansible Architecture Diagram
[[https://miro.medium.com/max/564/1*eaY6sN1T9VJiVOrMQMNdRQ.png]]
[[https://miro.medium.com/max/625/0*K9Kqdh4ZLT-fHJeP.png]]
[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/06/Ansible_Diagram2-16-1024x461.png]]

+ *Modules*: Modules are script-like programs written to specify the desired state of the system. These are typically written in a code editor. Modules are written by the developer and executed via SSH. Modules are part of a larger program called Playbook. Ansible module is a standalone script that can be used inside an Ansible Playbook.
+ *Plugins*: Plugins are pieces of code that enhance the core functionality of Ansible. Plugins execute on the control node.
+ *Inventory*: Ansible reads information about the machines you manage from the inventory. Inventory is listed in the file which contains IP addresses, databases, and servers.
+ *Playbook*: Playbooks are files written in YAML. Playbooks describe the tasks to be done by declaring configurations in order to bring a managed node into the desired state.

*** Ansible Playbook
+ Plain-text YAML files that describe the desired state of something
+ Human and Machine-readable
+ Can be used to build the entire application environment

[[https://miro.medium.com/max/463/0*t2iCHi_buMmtKGmw]]

*** What Are Inventories In Ansible?
+ Static lines of servers
+ Dynamic list of servers: AWS, Azure, GCP, etc.
+ Ranges
+ Other custom things

  [[https://miro.medium.com/max/201/1*mLdHcg8SvBvXRDceZIdKeA.png]]
  [[https://miro.medium.com/max/1006/0*E_bhUEFXGoQCOV_K.jpg]]

*** Ansible Modules
+ Over 1000 modules provided by Ansible to automate
+ Modules are like plugins that do the actual work in Ansible, they are what gets executed in each playbook task.
+ Each module is mostly standalone and can be written in a standard scripting language (such as Python, Perl, Ruby, Bash, etc.)

[[https://miro.medium.com/max/793/1*UDC-1_SR4Z26APTYRWDP3w.png]]

*** Ansible Tower
Ansible Tower is a GUI and REST interface for Ansible that supercharges it by adding RBAC, centralized logging, auto-scaling/provisioning call-backs, graphical inventory editing, and more.

*Capabilities*:

This command-line tool sends commands to the Tower API. It is capable of retrieving, creating, modifying, and deleting most resources within the Tower.

+ A few potential uses include:
+ Launching playbook runs (for instance, from Jenkins, TeamCity, Bamboo, etc.)
+ Checking on job statuses
+ Rapidly creating objects like organizations, users, teams, and more.

[[https://www.ansible.com/products/tower]]

*** Ansible Roles
1. Roles are a way to group tasks together into one container. We could have a role for setting up MySQL, another one for configuring ip tables.
2. Roles make it easy to configure hosts. Any role can be performed on any host or group of hosts such as:
    + hosts: all
    + roles:
    + role_1
    + role_2

*** Ansible Variables
There are many different ways to source variables:
+ Playbooks
+ Files
+ Inventories (group vars, host vars)
+ Command-line Discovered Variables
+ Ansible Tower

*** How To Run The Ansible Commands?
*Ad-Hoc*: Ansible <inventory> -m
[[https://miro.medium.com/max/520/1*W8ndyJq6S37tdAPBEHvUbQ.png]]
*Playbooks*: Ansible-playbook
[[https://miro.medium.com/max/519/1*SfmrmCzzcKmf4GO7TApVmg.png]]

*** AD-HOC Commands Examples
*Transferring file to many servers/machines*

#+begin_src console
$ Ansible Abc -m copy -a "src = /etc/yum.conf dest = /tmp/yum.conf"
#+end_src

*Creating a new directory*

#+begin_src console
$ Ansible ABC -m file -a "dest = /path/user1/new mode = 777 owner = user1 group = user1 state = directory"
#+end_src

*Deleting whole directory and files*

#+begin_src console
$ Ansible ABC -m file -a "dest = /path/user1/new state = absent"
#+end_src


** DONE The comparison and introduction between Terraform and Ansible :Terraform:Ansible:
CLOSED: [2021-12-02 Thu 10:53]
:PROPERTIES:
:EXPORT_FILE_NAME: terraformAndAnsible
:EXPORT_OPTIONS: author:nil
:ID:       54145bec-00b1-4044-9f97-8a9ddd6df50f
:END:
*** Overview
The Terraform vs. Ansible battle continues to escalate as the DevOps environment focuses more on automation and orchestration. These two tools help in automating configurations and deploying infrastructure. Terraform offers to deploy Infrastructure as a Code, helps in readability and lift and shift deployments. Ansible is a configuration management tool for automating system configuration and management.

*** Terraform

**** What is Terraform?

Terraform is an open-source tool for building, changing, and versioning infrastructure securely and effectively. It is an Infrastructure as a Code tool that is very straightforward to use. It helps to develop and scale Cloud services and manage the state of the network. Its primary use is in data centers and software-defined networking environments. It does not install and manage software on existing devices; instead, it creates, modifies, and destroys servers and various other cloud services. Slack, Uber, Starbucks, Twitch, all big brands are using Terraform. We can also integrate Terraform with Microsoft Azure, Heroku, and Google Compute Engine, etc.

Now, we will discuss the working of terraform.

**** How does Terraform work?
There are two main working components of terraform.

+ Terraform Core
+ Providers

Terraform is of *declarative nature*. It directly describes the end state of the system without defining the steps to reach there. It works at a high level of abstraction to describe what services and resources should be created and defined.

Terraform core takes two input sources to do its job. The first input source is a *terraform configuration* that is configured by its users. Users define what needs to be provisioned and created. The second input source is a state that holds information about the infrastructure.

So terraform core takes the input and figures out various plans for what steps to follow to get the desired output.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/06/TerraformCore_Diagram-08-1024x421.png]]

The second principal component is providers, such as cloud providers like AWS, GCP, Azure, or other Infrastructure as service platforms. It helps to create infrastructure on different levels. Let’s take an example where users create an AWS infrastructure, deploy Kubernetes on top of it, and then create services inside the cluster of Kubernetes. Terraform has multiple providers for various technologies; users can access resources from these providers through terraform. This is the basic working terminology of terraform that helps to provision and cover the complete application set up from infrastructure to fully developed application.

**** Features of Terraform
As we have discussed the working of Terraform, now we will look at the features of Terraform.

+ Terraform follows a *declarative approach* which makes deployments fast and easy.
+ It is a convenient tool to display the resulting model in a *graphical form*.
+ Terraform also manages *external service providers* such as cloud networks and in-house solutions.
+ It is one of the rare tools to offer *building infrastructure* from scratch, whether public, private or multi-cloud.
+ It helps *manage parallel environments*, making it a good choice for testing, validating bug fixes, and formal acceptance.
+ Modular code helps in achieving *consistency*, *reusability*, and *collaboration*.
+ Terraform can *manage multiple clouds* to increase fault tolerance.


*** Ansible

**** What is Ansible?
Ansible is the most significant way to automate and configure apps and IT infrastructure.  Ansible is an *open-source configuration management tool* mainly designed for provisioning and deploying applications using IaaC.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/06/Ansible-Official-Logo-Black-299x300.png]]

It has its own language to describe system configuration. Ansible is *agentless*, making it manage large deployments across enterprises using Windows Power Shell or SSH to perform its tasks. Ansible is not completely declarative; it is a hybrid of procedural and declarative. It can integrate with Amazon EC2, Docker, and Kubernetes. Companies like Zalando, Revolt, and 9gaga are using Ansible.

**** How does Ansible work?
Ansible is agentless and doesn’t run on target nodes. It makes *connections using SSH* or other authentication methods. It installs various *Python modules* on the target using JSON. These modules are simple instructions that run on the target. These modules are executed and removed once their job is done. This strategy ensures that there is no misuse of resources on target. Python is mandatory to be installed on both the controlling and the target nodes.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/06/Ansible_Diagram-07-1024x564.png]]

Ansible *management node* acts as a controlling node that controls the entire execution of the playbook. This node is the place to run the installations. There is an *inventory file* that provides the host list where the modules need to be run. The management node makes SSH connections to execute the modules on the host machine and installs the product. Modules are removed once they are installed in the system. This is the simple working process of Ansible.

**** Features of Ansible

Now we will discuss various features Ansible provides to benefit its users.

+ Ansible is used for *configuration management* and follows a procedural approach.
+ Ansible deals with *infrastructure platforms* such as bare metal, cloud networks, and virtualized devices like hypervisors.
+ Ansible follows *idempotent behavior* that makes it to place node in the same state every time.
+ It uses *Infrastructure as a Code system configuration* across the infrastructure.
+ It offers *rapid and easy deployment* of multi-tier apps with being agentless.
+ If the code is *interrupted*, it allows entering the code again without any conflicts with other invocations.

*** Difference between Terraform and Ansible Provisioning
Let’s see how Terraform vs. Ansible battle differentiates from each other:

| Terraform                                                                                        | Ansible                                                                                                     |
|--------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------|
| Terraform is a provisioning tool.                                                                | Ansible is a configuration management tool.                                                                 |
| It follows a declarative Infrastructure as a Code approach.                                      | It follows a procedural approach.                                                                           |
| It is the best fit for orchestrating cloud services and setup cloud infrastructure from scratch. | It is mainly used for configuring servers with the right software and updates already configured resources. |
| Terraform does not support bare metal provisioning by default.                                   | Ansible supports the provisioning of bare metal servers.                                                    |
| It does not provide better support in terms of packaging and templating.                         | It provides full support for packaging and templating.                                                      |
| It highly depends on lifecycle or state management.                                              | It does not have lifecycle management at all.                                                               |

*** Configuration Management vs. Orchestration
Terraform and Ansible have so many similarities and differences at the same time. The difference comes when we look at two significant concepts of DevOps: *Orchestration* and *configuration management*.

*Configuration management* tools solve the issues locally rather than replacing the system entirely. Ansible helps to configure each action and instrument and ensures smooth functioning without any damage or error. In addition, Ansible comes up with hybrid capabilities to perform both orchestration and replace infrastructure.

*Orchestration tools* ensure that an environment is in its desired state continuously. Terraform is explicitly designed to store the state of the domain. Whenever there is any glitch in the system, terraform automatically restores and computes the entire process in the system after reloading. It is the best fit in situations where a constant and invariable state is needed. *Terraform Apply* helps to resolve all anomalies effectively.

Let’s have a look at the *Procedural* and *Declarative* nature of Terraform and Ansible.

*** Procedural vs Declarative
There are two main categories of DevOps tools: *Procedural* vs. *Declarative*. These two categories tell the action of tools.

Terraform follows the *declarative approach*, ensuring that if your defined environment suffers changes, it rectifies those changes. This tool attempts to reach the desired end state described by the sysadmin. Puppet also follows the declarative approach. With terraform, we can describe the desired state and figure out how to move from one state to the next automatically.

Ansible is of hybrid nature. It follows both declarative and *procedural style* configuration.  It performs ad-hoc commands to implement procedural-style configurations. Please read the documentation of Ansible very carefully to get in-depth knowledge of its behavior. It’s important to know whether you need to add or subtract resources to get the desired result or need to indicate the resources required explicitly.

*** Terraform vs Ansible Provisioning
[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/07/TerraformVsAnsible-400x224.png]]

*Terraform* deals with *infrastructure automation*. Its current declarative model lacks some features which arise complexity. Using Terraform, the elements of required environments are separately described, including their relationships. It assesses the model, creates a plan based on dependencies, and gives optimized commands to Infrastructure as a Service. If there is no change in the environment or strategy, repeated runs will do nothing. If there is any update in the plan or environment, it will *synchronize* the cloud infrastructure.

*Ansible* follows a *procedural approach*. Various users create playbooks that are evaluated through top to bottom approach and executed in sequence. *Playbooks* are responsible for the configuration of network devices that contributes towards a procedural approach.  Of course, Ansible provisions the cloud infrastructure as well. But its procedural approach limits it to large infrastructure deployments.

*** Which one to choose: Terraform or Ansible?
*Terraform* vs. *Ansible*: Every tool has its unique characteristics and limitations. Let’s check out which one to go with.

*Terraform* comes with good *scheduling capabilities* and is very *user-friendly*. It integrates with docker well, as docker handles the configuration management slightly better than Terraform. But there is no clear evidence of how the target devices are brought to their final state, and sometimes, the final configuration is unnecessary.

*Ansible* comes with better *security* and *ACL functionality*. It is considered a mature tool because it adjusts comfortably with traditional automation frameworks. It offers simple operations and helps to code quickly. But, on the other hand, it is not good at services like logical dependencies, orchestration services, and interconnected applications.

You can now choose between these two, according to the requirement of the situation and the job. For example, if the containerized solution is used to provision software within the cloud, then Terraform is preferable. On the other hand, if you want to gain reasonable control of your devices and find other ways to deploy underlying services, Ansible is more suitable. These tools will provide more comprehensive solutions in the future.

*** Conclusion
It is essential to know which tool is used for which job among Terraform vs. Ansible. Terraform is mainly known for provisioning infrastructure across various clouds. It supports more than 200 providers and a great tool to manage cloud services below the server. In comparison, Ansible is optimized to perform both provisioning and configuration management. Therefore, we can say that both Terraform and Ansible can work hand in hand as standalone tools or work together but always pick up the right tool as per the job requirement.

*** References


* Nix :@Nix:

** DONE Nix Installation :nix:
CLOSED: [2021-12-26 Sun 15:18]
:PROPERTIES:
:EXPORT_FILE_NAME: nixInstallation
:EXPORT_OPTIONS: author:nil
:ID:       2b17d9c7-a5eb-4892-abec-282e74530b0c
:END:

*** Supported Platforms
Nix is currently supported on the following platforms:
+ Linux (i686, x86_64, aarch64).
+ macOS (x86_64).

*** Installing a Binary Distribution
We recommend the multi-user install if it supports your platform and you can authenticate with sudo.

*** Single User Installation

To explicitly select a single-user installation on your system:

#+begin_src bash
sh <(curl -L https://nixos.org/nix/install) --no-daemon
#+end_src

This will perform a single-user installation of Nix, meaning that */nix* is owned by the invoking user. You should run this under your usual user account, not as root. The script will invoke *sudo* to create */nix* if it doesn’t already exist. If you don’t have *sudo*, you should manually create */nix* first as root, e.g.:

#+begin_src bash
mkdir /nix
chown alice /nix
#+end_src

The install script will modify the first writable file from amongst *.bash_profile*, *.bash_login* and *.profile* to source *~/.nix-profile/etc/profile.d/nix.sh*. You can set the *NIX_INSTALLER_NO_MODIFY_PROFILE* environment variable before executing the install script to disable this behaviour.

You can uninstall Nix simply by running:

#+begin_src bash
rm -rf /nix
#+end_src

*** Multi User Installation (recommend)
The multi-user Nix installation creates system users, and a system service for the Nix daemon.

*Supported Systems*
+ Linux running systemd, with SELinux disabled
+ macOS

You can instruct the installer to perform a multi-user installation on your system:

#+begin_src bash
sh <(curl -L https://nixos.org/nix/install) --daemon
#+end_src

The multi-user installation of Nix will create build users between the user IDs 30001 and 30032, and a group with the group ID 30000. You should run this under your usual user account, not as root. The script will invoke sudo as needed.

When you see:

*Alright! We're done!*
Try it! Open a new terminal, and type:
#+begin_src bash
nix-shell -p nix-info --run "nix-info -m"
#+end_src

The installer will modify /etc/bashrc, and /etc/zshrc if they exist. The installer will first back up these files with a .backup-before-nix extension. The installer will also create /etc/profile.d/nix.sh.

You can uninstall Nix with the following commands:
#+begin_src bash
sudo rm -rf /etc/profile/nix.sh /etc/nix /nix ~root/.nix-profile ~root/.nix-defexpr ~root/.nix-channels ~/.nix-profile ~/.nix-defexpr ~/.nix-channels

# If you are on Linux with systemd, you will need to run:
sudo systemctl stop nix-daemon.socket
sudo systemctl stop nix-daemon.service
sudo systemctl disable nix-daemon.socket
sudo systemctl disable nix-daemon.service
sudo systemctl daemon-reload

# If you are on macOS, you will need to run:
sudo launchctl unload /Library/LaunchDaemons/org.nixos.nix-daemon.plist
sudo rm /Library/LaunchDaemons/org.nixos.nix-daemon.plist
There may also be references to Nix in /etc/profile, /etc/bashrc, and /etc/zshrc which you may remove.
#+end_src
*** Sert-up
#+begin_src bash
nix-channel --add https://nixos.org/channels/nixpkgs-unstable nixpkgs
#+end_src

*** Reference List
1. https://nixos.org/manual/nix/stable/installation/installation.html


** DONE Getting Started with Nix :nix:
CLOSED: [2021-12-24 Fri 12:34]
:PROPERTIES:
:EXPORT_FILE_NAME: nix
:EXPORT_OPTIONS: author:nil
:ID:       8c5cdb79-de79-45c6-be67-ebade5414a88
:END:
*** Introduction
The NixOS package manager is a system of its own. You can use it under any Linux Distribution.
*** What is Nix?
Briefly speaking, Nix is a package manager and a build system. Its most important aspect is allowing to write declarative scripts for reproducible software builds. It also helps to test and deploy software systems while using the functional programming paradigm. There is a vast repository of packages for Nix called [[https://github.com/nixos/nixpkgs][nixpkgs]], and a GNU/Linux distribution that extends the ideas of Nix to the OS level called [[https://nixos.org/][NixOS]].

Nix building instructions are called “derivations” and are written in Nix, the programming language. Derivations can be written for packages or even entire systems. After that, they can then be deterministically “realised” (built) via Nix, the package manager. Derivations can only depend on a pre-defined set of inputs, so they are somewhat reproducible.

You can read more about the benefits of Nix in my blog post on [[https://serokell.io/blog/what-is-nix][Nix]].

*** What does NixOS Package Manager do?
Most package managers use a file that contains the executable or source code. They then calculate what it needs on the system and then make sure that it exists. In Nix, things work very similarly. The big difference is that Nix creates all the files, and compiles them if necessary, then put them in one place; the nix-store. The first question you have may be, “Will the files not have the same name?” The system avoids this by having one directory for each version AND naming all files with a hash. To make the application “feel at home”, all dependencies are then linked to their correct directories using ordinary symlinks. A profile keeps track of which version each user runs.
*** NixOS User Installs
With this system, you can have different versions installed in each user’s directory. If they are the same in several users, the administrator can let Nix re-link binaries, so only one exists at a time. This is useful in saving disk space. You can also create specific environments for each version of the package. This is especially useful when you want to test a new version or develop software.

*** Installing for common distribution
[[id:2b17d9c7-a5eb-4892-abec-282e74530b0c][Linux/Nix Installation]]

*** Adding your first program to NixOS
To install software and set when it can be used, you have nix-env. The install option (-i) is the most common one since you use it always and put a package as an argument.

#+begin_src bash
nix-env -i firefox
#+end_src
This looks the same as in other distributions, so does the query argument. The install will take some time, though. The reason is that it must compile the software unless there is a pre-compiled version in the Nix Cache. Reaching the cache is not always very fast either. There is a difference that you should take note of; you can pick a version! If you want a special version, you must find which are available using regular expressions.

#+begin_src bash
nix-env -qa 'firefox.*'
#+end_src

You will receive a list of all the available packages. You can install it the same way but using the value in the list.

#+begin_src bash
nix-env –install 'firefox-78.4.0ser' –preserve-installed
#+end_src

This can fail if you already have an installed version. Option ‘–preserve-installed’ will not erase the installed version. You may end up with two versions of the same priority, which you can fix by setting the priority.

#+begin_src bash
nix-env –set-flag priority 2 'firefox-82.0.2'
#+end_src

Now, you will run the old version the next time you start Firefox. To switch which one you run, you can set the priority accordingly. You can also start a shell to choose a binary. This is a developer’s option, and the command is nix-shell.

*** Updating NixOS
Once you have a collection of software, you want to stay updated. Same as always, you use the same command with an argument. But you must also keep the channel updated. The command is nix-channel.

#+begin_src bash
nix-channel --update
#+end_src

This reads down the current versions of all packages available. After that, you can start upgrading your software with nix-env.

#+begin_src bash
nix-env --upgrade
#+end_src

An upgrade like this will upgrade your old version of the software. In this case, the old Firefox will be replaced with the newest version. You may not want this for whatever reason, usually development.

*** Removing applications from NixOS
Removing applications is equally simple, with a small caveat. No applications are removed by a remove command.

#+begin_src bash
nix-env –uninstall 'firefox-78.4.0ser'
#+end_src

This command will remove the links to the current build of this version of Firefox. All the files will always stay on disk. You have these versions available to do a rollback. A rollback means that you go back to using the old version. This can be useful if you have tried the newest and it has unforeseen problems.

#+begin_src bash
nix-env –rollback
#+end_src

You rollback an entire generation, which means all the programs that were upgraded since the last generation. The option runs two commands; that list and then switches to that old generations. All installed packages exist in a generation on disk.

*** NixOS Roll-back and Cleaning up
The rollback function will lead to a lot of disk space being used by old versions. You can clean this up (you need to clean this up!). When you have had a long enough period, at your own choice, you can also clean up these old generations to save disk space.

#+begin_src bash
nix-env –delete-generations old
#+end_src

With this command, you delete all generations except the two last ones. You can go back and forth in the list with more complex parameters to leave the specific generation that worked best for you. Unless you have many testing or development projects that need many versions for testing, you should use a scheduled removal of all old generations.

A simple script to keep your generations clean comes with a Nix package manager install.

#+begin_src bash
nix-collect-garbage
#+end_src

You should also set up the collector to run automatically using systemd or other systems.


*** Conclusion
Nix package manager is a powerful system that can get you running complex development environments on your machine. You can also use it to keep your software tidy and have a simple way to recover on a new machine, should the catastrophe of a disk crash occur.

*** Reference List
1. https://linuxhint.com/how-to-use-nixos-package-manager/
2. https://serokell.io/blog/what-is-nix
3. https://github.com/NixOS/nix
4. https://nixos.org/learn.html


** DONE Home Manager using Nix
CLOSED: [2021-12-26 Sun 17:30]
:PROPERTIES:
:EXPORT_FILE_NAME: nixHomeManager
:EXPORT_OPTIONS: author:nil
:END:

*** Standalone installation
1. Add the appropriate Home Manager channel. If you are following Nixpkgs master or an unstable channel you can run.

   #+begin_src bash
nix-channel --add https://github.com/nix-community/home-manager/archive/master.tar.gz home-manager
 nix-channel --update
   #+end_src

   #+begin_src bash
cat ~/.nix-channels
nix-channel --list
   #+end_src

   Check your *echo $NIX_PATH*, If your have NOT

   you may have to add
   #+begin_src bash
export NIX_PATH=$HOME/.nix-defexpr/channels${NIX_PATH:+:}$NIX_PATH

/nix/var/nix/profiles/per-user/yanboyang713/channels


export NIX_PATH=nixpkgs=/nix/var/nix/profiles/per-user/yanboyang713/channels/nixpkgs:/nix/var/nix/profiles/per-user/yanboyang713/channels:~/.nix-defexpr/channels
   #+end_src
   to your shell

2. Run the Home Manager installation command and create the first Home Manager generation:

   #+begin_src bash
nix-shell '<home-manager>' -A install
   #+end_src
Once finished, Home Manager should be active and available in your user environment.

3. If you do not plan on having Home Manager manage your shell configuration then you must source the

   #+begin_example
$HOME/.nix-profile/etc/profile.d/hm-session-vars.sh
   #+end_example
file in your shell configuration.
This file can be sourced directly by POSIX.2-like shells such as Bash or Z shell

For example, if you use Bash then add
#+begin_example
. "$HOME/.nix-profile/etc/profile.d/hm-session-vars.sh"
#+end_example

to your ~/.profile file.

You can test that everything worked by typing
#+begin_src bash
home-manager
#+end_src

Congratulations, you are ready to go!

*** Getting started Home Manager
As you have seen in the install prompt, by default Home Manager initializes a new configuration in

#+begin_src bash
$HOME/.config/nixpkgs/home.nix
#+end_src

that should look roughly like this

#+begin_example
{ config, pkgs, ... }:

{
  # Home Manager needs a bit of information about you and the
  # paths it should manage.
  home.username = "yanboyang713";
  home.homeDirectory = "/home/yanboyang713";

  # This value determines the Home Manager release that your
  # configuration is compatible with. This helps avoid breakage
  # when a new Home Manager release introduces backwards
  # incompatible changes.
  #
  # You can update Home Manager without changing this value. See
  # the Home Manager release notes for a list of state version
  # changes in each release.
  home.stateVersion = "22.05";

  # Let Home Manager install and manage itself.
  programs.home-manager.enable = true;
}

#+end_example

Your use of Home Manager is centered around the configuration file.

This configuration file can be built and activated.

#+begin_src bash
home-manager build
#+end_src

Once a configuration is successfully built, it can be activated. The activation performs the steps necessary to make the files, programs, and services available in your user environment.

#+begin_src bash
# The command performs a combined build and activation
home-manager switch
#+end_src

*** Git-it
The first step is to move this configuration to a git repository, I prefer to have it in a different location and use symlinks to expose it to Home Manager.

Run:
#+begin_src bash

mv ~/.config/nixpkgs ~/nixfiles
cd ~/nixfiles
git init
git add .
git commit -m 'getting started with Home Manager'
cd ~/.config
ln -s ~/nixfiles nixpkgs
#+end_src

Then test that you can apply the configuration.
#+begin_src bash
home-manager switch
#+end_src

We have not added anything so this should be a no-op.

Home Manager will let you know that it’s "reusing lastest profile generation".

*** Let’s add our first package
First let’s see how we can use Home Manager to install packages for our user. In this example we’ll add tmux. Edit ~/nixfiles/home.nix as follows:

#+begin_example
{ config, pkgs, ... }:

{
  # Home Manager needs a bit of information about you and the
  # paths it should manage.
  home.username = "yanboyang713";
  home.homeDirectory = "/home/yanboyang713";

  # Packages that should be installed to the user profile.
  home.packages = [
    pkgs.htop
  ];

  # This value determines the Home Manager release that your
  # configuration is compatible with. This helps avoid breakage
  # when a new Home Manager release introduces backwards
  # incompatible changes.
  #
  # You can update Home Manager without changing this value. See
  # the Home Manager release notes for a list of state version
  # changes in each release.
  home.stateVersion = "22.05";

  # Let Home Manager install and manage itself.
  programs.home-manager.enable = true;
}

#+end_example

Then run

#+begin_src bash
home-manager switch
#+end_src

And now try it out

#+begin_src bash
htop
#+end_src

A great place to search for packages is https://search.nixos.org/packages, make sure you pick the right “channel”, if you are following this guide it will be unstable.

*** What is this home-manager switch business?
*home-manager switch* is how you "*activate*" your configuration. Home Manager will evaluate your configuration, build the result and *atomically* switch your old configuration with the new one.

This also means that it’s possible to see all old configurations

#+begin_src console
[yanboyang713@manjaro] ➜ nixpkgs home-manager generations
2021-12-26 19:53 : id 2 -> /nix/store/hgma8yixml6ngwizw31sxdj9n08kmkfg-home-manager-generation
2021-12-26 17:55 : id 1 -> /nix/store/1xkax9m57lbyrcvhybpksqsga28injsf-home-manager-generation
#+end_src

And you can rollback to older versions as well.
#+begin_example
# copy the path from the command above and add /activate
/nix/store/1xkax9m57lbyrcvhybpksqsga28injsf-home-manager-generation/activate
...

# this will create a new generation
Creating profile generation 3
...
#+end_example

You can then switch again to re-apply your changes to go back to the current version of home.nix.

#+begin_src bash
home-manager switch
#+end_src
*** Porting over an existing dotfile
So far we saw how to use Home Manager to install packages for our user, but a perhaps more important use case is manage our user configuration.

First we’ll see how to take an existing configuration file and make it part of Home Manager.

The simplest way to do this is to use the *home.file* option.

Assume that you have a *~/.vimrc* with the following contents:
#+begin_example
call plug#begin()
Plug 'LnL7/vim-nix'
call plug#end()
#+end_example

First let’s move it in our nixfiles repo

#+begin_src bash
mv ~/.vimrc ~/nixfiles/vimrc
#+end_src

You can then edit ~/nixfiles/home.nix as follows

#+begin_example
{ config, pkgs, ... }:

{
  # Home Manager needs a bit of information about you and the
  # paths it should manage.
  home.username = "yanboyang713";
  home.homeDirectory = "/home/yanboyang713";

  # Packages that should be installed to the user profile.
  home.packages = [
    pkgs.htop
  ];

  # Raw configuration files
  home.file.".vimrc".source = ./vimrc;

  # This value determines the Home Manager release that your
  # configuration is compatible with. This helps avoid breakage
  # when a new Home Manager release introduces backwards
  # incompatible changes.
  #
  # You can update Home Manager without changing this value. See
  # the Home Manager release notes for a list of state version
  # changes in each release.
  home.stateVersion = "22.05";

  # Let Home Manager install and manage itself.
  programs.home-manager.enable = true;
}
#+end_example

And run *home-manager switch* again.
Now, let’s check what happened

#+begin_src console
[yanboyang713@manjaro] ➜ nixpkgs ls -l ~/.vimrc
lrwxrwxrwx 1 yanboyang713 yanboyang713 69 Dec 27 13:49 /home/yanboyang713/.vimrc -> /nix/store/mfjw8j9c04wp9zxj4g6xj83jcxlld045-home-manager-files/.vimrc
#+end_src

/.vimrc is now a symlink to a file in the “Nix store”, the place where all the nix things go. Without concering ourself with details, the thing to notice is that if you now change the contents of ~/nixfiles/vimrc and re-run home-manager switch Home Manager will detect the changes, create a new version of .vimrc in the Nix store and update the symlink.

#+begin_src console
$ echo "hello nix" > ~/nixfiles/vimrc
$ home-manager switch
$ ls -l ~/.vimrc
lrwxrwxrwx 1 ghedamat ghedamat 69 Apr  3 05:47 /home/ghedamat/.vimrc -> /nix/store/dsq0da2y4p7w67imwnd95crv4k35d6qb-home-manager-files/.vimrc
#+end_src

It is true that managing configuration in this way will add a step every time you want to edit your vimrc. I believe that this tradeoff is worth it even if you were to decide to not use any other feature offered by Home Manager as you now have a reliable and consistent way to manage all your configuration files and packages.

*** Using Home Manager modules
Using the *home.file* configuration option is my preferred way to port existing configuration files. Once that’s done though Home Manager has much more to offer.

Home Manager comes with a large amount of pre-defined configuration *modules* for a variety of applications ([[https://github.com/nix-community/home-manager/tree/master/modules][full list on github]]). These modules allow you to use a consistent syntax (the Nix language) to configure every application regardless of the output format that each program requires (ini, yaml, custom…).

By using modules you will get *type safety guarantees* about your configuration as it will be now written in Nix and the modules can specify types for each option you pass. This also means that *you can use the Nix language to add logic* (i.e conditionals and functions) as well as the ability to compose your configuration as you would with any other functional program written in Nix.

The downside is that you have to learn at least a small part of the Nix language (mostly how to write sets, which are similar to maps and hash in other languages).

Once you have identified a module you are interested in, all the options available are listed in the [[https://nix-community.github.io/home-manager/][Home Manager Manual]].

**** Porting your git config
Let’s see an example with porting over our *~/.config/git/config* to Home Manager.

# current contents of ~/.config/git/config
[user]
	email = ghedamat@gmail.com
	name = ghedamat
[alias]
	st = "status"


Edit *home.nix* as follows (you can find the full list of options for *programs.git* [[https://nix-community.github.io/home-manager/options.html#opt-programs.git.enable][here]])

#+begin_example
{ config, pkgs, ... }:

{
  # Home Manager needs a bit of information about you and the
  # paths it should manage.
  home.username = "yanboyang713";
  home.homeDirectory = "/home/yanboyang713";

  # Packages that should be installed to the user profile.
  home.packages = [
    pkgs.htop
  ];

  # Raw configuration files
  home.file.".vimrc".source = ./vimrc;

  # Git config using Home Manager modules
  programs.git = {
     enable = true;
     userName = "Boyang Yan";
     userEmail = "yanboyang713@gmail.com";
     aliases = {
        st = "status";
     };
  };

  # This value determines the Home Manager release that your
  # configuration is compatible with. This helps avoid breakage
  # when a new Home Manager release introduces backwards
  # incompatible changes.
  #
  # You can update Home Manager without changing this value. See
  # the Home Manager release notes for a list of state version
  # changes in each release.
  home.stateVersion = "22.05";

  # Let Home Manager install and manage itself.
  programs.home-manager.enable = true;
}
#+end_example

Let’s try to apply this change

#+begin_src bash
home-manager switch
#+end_src

We can verify that the file is now generated by Home Manager (notice the content is slightly different)

#+begin_src console
$ cat ~/.config/git/config
[alias]
        st = "status"

[user]
        email = "yanboyang713@gmail.com"
        name = "Boyang Yan"
#+end_src

*** Structuring your Home Manager config
The author of Home Manager recommends to start with a single home.nix file and I would definitely agree. As you learn more about the Nix language you’ll find about all the different ways to structure your code.

Later, you might want to learn about using imports to break down your configuration into multiple files. A more advance approach is to build your own Nix modules.

I might decide to cover these in a future post.

*** Reference List
1. https://github.com/nix-community/home-manager
2. https://nix-community.github.io/home-manager/index.html#sec-install-standalone
3. https://ghedam.at/24353/tutorial-getting-started-with-home-manager-for-nix


** DONE Nix Flakes :nix:Flakes:
CLOSED: [2021-12-28 Tue 11:23]
:PROPERTIES:
:EXPORT_FILE_NAME: nixFlakes
:EXPORT_OPTIONS: author:nil
:END:
*** Introduction
Nix Flakes are an upcoming feature of the Nix package manager.

Flakes replace stateful channels (which cause much confusion among novices) and introduce a more intuitive and consistent CLI, making them a perfect opportunity to start using Nix.

It contains examples and advice on using flakes for a real-life use case: building applications in various languages.

*** Pre-reading
1. [[id:8c5cdb79-de79-45c6-be67-ebade5414a88][Nix/Getting Started with Nix]]

*** Installing flakes
In order to do anything with flakes, you will first have to get “unstable” Nix up and running on your machine. Don’t mind that it is called unstable: it is not generally dangerous to run on your machine, it simply changes more often than “stable”.

If you have NOT install *NIX*, please following this [[id:2b17d9c7-a5eb-4892-abec-282e74530b0c][Nix/Nix Installation]].

For *Non-NixOS*, Follow the instructions until you have Nix working on your machine, and then update to unstable with:

#+begin_src bash
#On non-nixos systems, install nixFlakes in your environment:
nix-env -iA nixpkgs.nixUnstable
#+end_src

And enable experimental features. Edit either *~/.config/nix/nix.conf* or */etc/nix/nix.conf* and add:

I recommend:

#+begin_src bash
mkdir -p ~/.config/nix
echo 'experimental-features = nix-command flakes' >> ~/.config/nix/nix.conf
#+end_src

This is needed to expose the Nix 2.0 CLI and flakes support that are hidden behind feature-flags.

Finally, if the Nix installation is in multi-user mode, don’t forget to restart the nix-daemon.

#+begin_src bash
sudo systemctl stop nix-daemon.socket
sudo systemctl stop nix-daemon.service
sudo systemctl start nix-daemon.socket
sudo systemctl start nix-daemon.service
#+end_src
*** Getting a feel for flakes
Now that you have a “flaky” Nix installed, it’s time to use it!
**** nix shell
First, let’s enter a shell that has GNU Hello from nixpkgs’ branch nixpkgs-unstable in it:

#+begin_src bash
nix shell 'github:nixos/nixpkgs/nixpkgs-unstable#hello'
#+end_src

Note that this will start the same shell as you are running, but add a directory containing the hello executable to your *$PATH*.

For example:

*Before*:
#+begin_example
[yanboyang713@manjaro] ➜ ~ echo $PATH
/home/yanboyang713/.miniconda/bin:/home/yanboyang713/.fnm:/home/yanboyang713/.local/share/go/bin:/home/yanboyang713/.cargo/bin:/home/yanboyang713/.local/bin:/home/yanboyang713/.nix-profile/bin:/nix/var/nix/profiles/default/bin:/nix/store/wxrplk88a4k9cvam0fz2x6m7hl01cpd7-user-environment/bin:/home/yanboyang713/.nix-profile/bin:/home/yanboyang713/.miniconda/bin:/home/yanboyang713/.fnm:/home/yanboyang713/.local/share/go/bin:/home/yanboyang713/.cargo/bin:/home/yanboyang713/.local/bin:/home/yanboyang713/.nix-profile/bin:/nix/var/nix/profiles/default/bin:/home/yanboyang713/.nix-profile/bin:/nix/var/nix/profiles/default/bin:/home/yanboyang713/.local/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/home/yanboyang713/.local/bin/:/home/yanboyang713/.nix-profile/bin/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/var/lib/snapd/snap/bin
#+end_example

*After*:
#+begin_example
[yanboyang713@manjaro] ➜ ~ echo $PATH
/home/yanboyang713/.miniconda/bin:/home/yanboyang713/.fnm:/home/yanboyang713/.local/share/go/bin:/home/yanboyang713/.cargo/bin:/home/yanboyang713/.local/bin:/home/yanboyang713/.nix-profile/bin:/nix/var/nix/profiles/default/bin:/nix/store/xcp9cav49dmsjbwdjlmkjxj10gkpx553-hello-2.10/bin:/home/yanboyang713/.miniconda/bin:/home/yanboyang713/.fnm:/home/yanboyang713/.local/share/go/bin:/home/yanboyang713/.cargo/bin:/home/yanboyang713/.local/bin:/home/yanboyang713/.nix-profile/bin:/nix/var/nix/profiles/default/bin:/nix/store/wxrplk88a4k9cvam0fz2x6m7hl01cpd7-user-environment/bin:/home/yanboyang713/.nix-profile/bin:/home/yanboyang713/.miniconda/bin:/home/yanboyang713/.fnm:/home/yanboyang713/.local/share/go/bin:/home/yanboyang713/.cargo/bin:/home/yanboyang713/.local/bin:/home/yanboyang713/.nix-profile/bin:/nix/var/nix/profiles/default/bin:/home/yanboyang713/.nix-profile/bin:/nix/var/nix/profiles/default/bin:/home/yanboyang713/.local/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/home/yanboyang713/.local/bin/:/home/yanboyang713/.nix-profile/bin/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/var/lib/snapd/snap/bin
#+end_example

you can found:
#+begin_example
/nix/store/xcp9cav49dmsjbwdjlmkjxj10gkpx553-hello-2.10/bin
#+end_example

The shell shouldn’t look any different from how it was outside the nix shell, so don’t panic if it looks like nothing is happening! The executable is not installed anywhere per se, it gets downloaded and unpacked in what you can consider a cache directory.

Now, inside that shell, try running *hello*.

Let’s go through what this command does. *nix shell* is a nix subcommand that is used to run a shell with some packages available in *$PATH*. Those packages can be specified as arguments in the "*installable*" format. Each installable contains two parts: the URL (*github:nixos/nixpkgs/master* in this case) and an "attribute path" (*hello* here).

There are a few URL schemes supported:
+ github:owner/repo/[revision or branch] and gitlab:owner/repo/[revision or branch] (for public repositories on github.com and gitlab.com; note that that the branch name cannot contain slashes).
+ https://example.com/path/to/tarball.tar.gz for tarballs.
+ git+https://example.com/path/to/repo.git and git+ssh://example.com/path/to/repo.git for plain git repositories (you can, of course, use this for GitHub and GitLab). You can specify the branch or revision by adding ?ref=<branch name here>.
+ file:///path/to/directory or /path/to/directory or ./path/to/relative/directory for a local directory.
+ flake-registry-value for a value from a flake registry (I won’t talk about flake registries in this article).

So, there are some other ways to get the same shell:

#+begin_src bash
nix shell 'https://github.com/nixos/nixpkgs/archive/nixpkgs-unstable.tar.gz#hello'
nix shell 'git+https://github.com/nixos/nixpkgs?ref=nixpkgs-unstable#hello'
nix shell 'nixpkgs#hello' # nixpkgs is specified in the default registry to be github:nixos/nixpkgs
#+end_src

As for the attribute path, for now, just know that it’s a period-separated list of Nix "*attribute names*" that selects a flake output according to some simple logic.

Note that in this case, Nix did not have to build anything since it could just fetch GNU Hello and its dependencies from the binary cache. To achieve this, Nix evaluates a derivation from the expression, hashes its contents, and queries all the caches it knows to see if someone has the derivation with this hash cached. Nix uses all the dependencies and all the instructions as the input for this hash! If some binary cache has a version ready, it can be substituted (downloaded). Otherwise, Nix will build the derivation by first realising (substituting or building) all the dependencies and then executing the build instructions.

You might be wondering where exactly is the executable installed. Well, try *command -v hello* to see that it is located in a subdirectory of */nix/store*. In fact, all Nix derivations have "*store paths*" (paths located in /nix/store) as inputs and outputs.

**** nix build
If you just want to build something instead of entering a shell with it, try *nix build*:

#+begin_src bash
nix build 'nixpkgs#hello'
#+end_src

This will build Hello (or fetch it from the binary cache if available) and then symlink it to result in your current directory. You can then explore result, e.g.

#+begin_src console
[yanboyang713@manjaro] ➜ ~ ls -la
lrwxrwxrwx  1 yanboyang713 yanboyang713    54 Dec 29 11:11  result -> /nix/store/xcp9cav49dmsjbwdjlmkjxj10gkpx553-hello-2.10
#+end_src

#+begin_src console
$ ./result/bin/hello
Hello, world!
#+end_src

**** nix develop
Despite the use of binary caches, Nix is a sourcecode-first package manager. This means that it has the ability to provide a build environment for its derivations. So, you can use Nix to manage your build environments for you! To enter a shell with all runtime and buildtime dependencies of GNU Hello, use:

#+begin_src bash
nix develop 'nixpkgs#hello'
#+end_src

Inside that shell, you can call *unpackPhase* to place GNU Hello sources in the current directory, then *configurePhase* to run *configure* script with correct arguments and finally *buildPhase* to build.

**** nix profile
Nix implements stateful "profiles" to allow users to "*permanently*" install stuff.

For example:

#+begin_src bash
nix profile install 'nixpkgs#hello'
nix profile list
nix profile update hello
nix profile remove hello
#+end_src
If you’re already familiar with Nix, this is a replacement for *nix-env*.

**** nix flake
*nix flake* set of subcommands is used to observe and manipulate flakes themselves rather than their outputs.

***** nix flake show
This command takes a flake URI and prints all the outputs of the flake as a nice tree structure, mapping attribute paths to the types of values.

For example:

#+begin_src console
[yanboyang713@manjaro] ➜ ~ nix flake show github:nixos/nixpkgs
github:nixos/nixpkgs/41d4fbf65287038fcd88fce734282e522e2a6d33
├───checks
│   └───x86_64-linux
│       └───tarball: derivation 'nixpkgs-tarball-22.05pre20211229.41d4fbf'
├───htmlDocs: unknown
├───legacyPackages
warning: │   ├───aarch64-darwin: omitted (use '--legacy' to show)
warning: │   ├───aarch64-linux: omitted (use '--legacy' to show)
warning: │   ├───armv6l-linux: omitted (use '--legacy' to show)
warning: │   ├───armv7l-linux: omitted (use '--legacy' to show)
warning: │   ├───i686-linux: omitted (use '--legacy' to show)
warning: │   ├───mipsel-linux: omitted (use '--legacy' to show)
warning: │   ├───x86_64-darwin: omitted (use '--legacy' to show)
warning: │   └───x86_64-linux: omitted (use '--legacy' to show)
├───lib: unknown
└───nixosModules
    └───notDetected: NixOS module
#+end_src

***** nix flake clone
*nix flake clone* will clone the flake source to a local directory, similar to *git clone*.

Let’s clone some simple flake and use some other *nix flake* subcommands on it:

#+begin_src bash
nix flake clone git+https://github.com/balsoft/hello-flake/ -f hello-flake
cd hello-flake
#+end_src

***** nix flake lock (*previously* nix flake update)
Every time you call a Nix command on some flake in a local directory, Nix will make sure that the contents of *flake.lock* satisfy the *inputs* in *flake.nix*. If you want to do just that, without actually building (or even evaluating) any outputs, use *nix flake lock*.

There are also some arguments for flake input manipulation that can be passed to most Nix commands:

+ *--override-input* takes an input name that you have specified in *inputs* of *flake.nix* and a flake URI to provide as this input; - *--update-input* will take an input name and update that input to the latest version satisfying the flake URI from *flake.nix*.

*** Writing your own
Now that you know how to interact with flakes, it’s time to write one.

**** Nix language refresher
The widely used data type in Nix is an attribute set: a data type for storing key-value pairs. It is similar to a JSON object or a hashmap in many languages. Its syntax is confusingly similar to a list of statements in C-like languages:

#+begin_example
{
  hello = "world";
  foo = "bar";
}
#+end_example

The set above is equivalent to this JSON object:

#+begin_example
{
    "hello": "world",
    "foo": "bar"
}
#+end_example

*hello* and *foo* are commonly referred to as "attributes" or "attribute names"; *"world"* and *"bar"* are "attribute values".

To get an attribute value from an attribute set, use .. For example:

#+begin_example
let
  my_attrset = { foo = "bar"; };
in my_attrset.foo
#+end_example

(*let ... in* is a way to create bindings; the syntax inside it is identical to that of an attribute set)

You can also abbreviate your attribute set by setting specific attributes with . instead of defining the entire set:

#+begin_example
{
  foo.bar = "baz";
}
#+end_example

Is equivalent to

#+begin_example
{
  foo = { bar = "baz"; };
}
#+end_example

Other types include strings (*"foo"*), numbers (1, 3.1415), heterogenous lists (*[ 1 2 "foo" ]*) and – quite importantly – functions (*x: x + 1*).

Functions support pattern matching on attribute sets. For example, this function:

#+begin_example
{ a, b }: a + b
#+end_example

When called with *{ a = 10; b = 20; }* will return 30.

Function application is done in ML style:

#+begin_example
let
  f = { a, b }: a + b;
in f { a = 10; b = 20; }
#+end_example

The function itself comes first. Then there is a whitespace-separated list of arguments.

If you want to have a function of multiple arguments, use currying:

#+begin_example
let
  f = a: b: a + b;
in f 10 20
#+end_example

In this example, *f 10* evaluates to *b: 10 + b*, and then *f 10 20* evaluates to *30*.

If you want to learn more about Nix, check out the [[https://nixos.org/manual/nix/stable/expressions/writing-nix-expressions.html][corresponding manual section]] and [[https://nixos.org/guides/nix-pills/][Nix Pills]].

**** Basic flake structure
The language description you got above is far from complete or formal, but it should help you understand and, more importantly, write some simple Nix expressions and, even more importantly, a flake.

A Nix flake is a directory that contains a flake.nix file. That file must contain an attribute set with one required attribute – outputs – and optionally description and inputs.

outputs is a function that takes an attribute set of inputs (there’s always at least one input – self – which refers to the flake that Nix is currently evaluating; this is possible due to laziness). So, the most trivial flake possible is this:

#+begin_example
{
  outputs = { self }: { };
}
#+end_example


*** What are Nix flakes?
Flakes are self-contained units that have inputs (dependencies) and outputs (packages, deployment instructions, Nix functions for use in other flakes). You can think about them as Rust crates or Go modules but language-independent. Flakes have great reproducibility because they are only allowed to depend on their inputs and they pin the exact versions of said inputs in a lockfile.

If you’re already familiar with Nix, flakes are to Nix expressions what derivations are to build instructions.


Flakes allow you to specify your code's dependencies (e.g. remote Git repositories) in a declarative way, simply by listing them inside a *flake.nix* file:

#+begin_example
{
  inputs = {
    home-manager.url = "github:nix-community/home-manager";
  };
}
#+end_example
Each dependency gets then pinned, that is: its commit hash gets automatically stored into a file - named flake.lock - making it easy to, say, upgrade it:

#+begin_src bash
nix flake lock --update-input home-manager
#+end_src
(if you're familiar with modern packages managers like cargo or npm, then the overall mechanism shouldn't surprise you - Nix works in a similar way, although without a centralized repository.)

Flakes replace the nix-channels command and things like ad-hoc invocations of builtins.fetchgit - no more worrying about keeping your channels in sync, no more worrying about forgetting about a dependency deep down in your tree: everything's at hand right inside flake.lock.

*** Reference List
1. https://serokell.io/blog/practical-nix-flakes


* Linux :@Linux:
** DONE Arch Linux/Manjaro sound and sound card configuration :arch:linux:manjaro:sound:card:
CLOSED: [2021-12-20 Mon 19:04]
:PROPERTIES:
:EXPORT_FILE_NAME: configSound
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction
After install Arch Linux/Manjaro, although alsa-utils has been installed, it may still not play sound correctly. Here is a solution.

*** First try
#+begin_src bash
amixer sset Master unmute
#+end_src

If you use alsamixer to unmute the master sound, you still can't play the sound.

Please, following the steps at the below.

*NOTE*: If you faced maixer: Unable to find simple cintrol 'Master', 0, that means, it maybe that the sound card device cannot be found or the sound card driver us not installed.

*** Finding sound card ID and set-up .asoundrc file
Using below commands to get the sound card ID and device ID of the sound card you want to use.

#+begin_src bash
aplay -l
#+end_src

There is a example for select the sound card with the sound card ID of 1 and the device ID of 0 for configuration.

Add the following three lines to the configuration file ~/.asoundrc

#+begin_example
defaults.pcm.card 1
defaults.pcm.device 0
defaults.ctl.card 1
#+end_example

The *PCM* option determines the device used to play audio, and the *CTL* option determines which sound card can be used by the control tool (alsamixer)

If you want to manage configuration files in a unified way, please have a read [[id:5daeba1f-2aa5-492a-b3e7-3343722def0b][Linux/Managing Your Dotfiles With Git and Make]].


** DONE Getting Started Dynamic Window Manager (DWM) :DWM:
CLOSED: [2021-11-30 Tue 17:50]
:PROPERTIES:
:EXPORT_FILE_NAME: dwm
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction
dwm is a dynamic window manager for X. It manages windows in tiled, monocle and floating layouts. All of the layouts can be applied dynamically, optimising the environment for the application in use and the task performed.

*** Installing
**** Install Xorg

#+begin_src bash
pacman -S xorg-server xorg-xinit xorg-xrandr xorg-xsetroot
#+end_src

**** Install DWM
#+begin_src bash
git clone git://git.suckless.org/dwm ~/.config/dwm
git clone https://github.com/WouterSpekkink/dwm.git




git clone git://git.suckless.org/st ~/.config/st
git clone https://github.com/WouterSpekkink/st.git



git clone git://git.suckless.org/dmenu ~/.config/dmenu
git clone https://github.com/WouterSpekkink/dmenu.git
#+end_src

#+begin_src bash
cd ~/.config/dwm && sudo make install
cd ~/.config/st && sudo make install
cd ~/.config/dmenu && sudo make install
#+end_src
**** Installing a Display Manager (DM)
#+begin_src bash
pacman -S lightdm

pacman -S lightdm-gtk-greeter

pacman -S lightdm-gtk-greeter-settings
#+end_src
**** Enable lightdm service
#+begin_src bash
sudo systemctl enable lightdm
#+end_src
**** Adding an entry for DWM in the DM
Create this file and add the following.
#+begin_src bash
mkdir /usr/share/xsessions

sudo vim /usr/share/xsessions/dwm.desktop
#+end_src

#+begin_src file
[Desktop Entry]
Encoding=UTF-8
Name=Dwm
Comment=the dynamic window manager
Exec=dwm
Icon=dwm
Type=XSession
#+end_src

*** DWM patches
There are two types of patches: The ones that fit to your personal taste and the ones you think should be included in mainline.

patch filename format
The expected format for patches is:

For git revisions:

toolname-patchname-YYYYMMDD-SHORTHASH.diff
dwm-allyourbase-20160617-3465bed.diff
The YYYYMMDD date should correspond to the last time the patch has been modified.

**** diff generation
For git users:

cd program-directory
git add filechanges...
git commit (write a clear patch description)
git format-patch --stdout HEAD^ > toolname-patchname-YYYYMMDD-SHORTHASH.diff

**** patch program
For git users, use -3 to fix the conflict easily:

cd program-directory
git apply path/to/patch.diff
For patches formatted with git format-patch:

cd program-directory
git am path/to/patch.diff
For tarballs:

cd program-directory
patch -p1 < path/to/patch.diff

*** picom
picom is a standalone compositor for Xorg, suitable for use with window managers that do not provide compositing. picom is a fork of compton, which is a fork of xcompmgr-dana, which in turn is a fork of xcompmgr.

#+begin_src bash
yay -S picom
#+end_src

*** Multi-monitor setup
If configured to use Xinerama libraries in config.mk, dwm can automatically detect configured screen outputs (monitor, overhead projector, etc.) and their resolutions and draw the windows in the output area accordingly.

One of the easiest ways to configure screen outputs is via the RandR X server extension using the xrandr tool. Without arguments it will list the current configuration of screen outputs.

#+begin_src bash
xrandr
#+end_src

For each connected output the supported resolution modes will be printed.

This is a example for set-up xrandr. You can put below content into ~/.xprofile, when system run X windows will set-up montors automatically.
#+begin_src bash
#!/bin/bash

###############################
# Set Monitor                 #
###############################
xrandr --output DP-1 --primary --mode 1920x1080 --pos 0x0 --rotate left --output HDMI-1 --mode 2560x1440 --pos 1080x0 --rotate normal --output DVI-D-1 --off
#+end_src

*** Status Monotor
pre-required
#+begin_src bash
yay -S pamixer

#+end_src
https://github.com/LukeSmithxyz/dwmblocks

#+begin_src bash
xsetroot -name "Boyang Yan"
xsetroot -name "$(uptime)"
#+end_src

*NOTE*: If show *zsh: command not found: xsetroot*, you need install *xorg-xsetroot*.
#+begin_src bash
sudo pacman -S xorg-xsetroot
#+end_src

#+begin_src bash
git clone https://github.com/WouterSpekkink/dwmblocks.git
cd dwmblocks

sudo make clean install
dwmblocks &
#+end_src

*NOTE*:
1. If you faced *ERROR* message like this *sh: line 1: sb-internet: command not found*
      you should check your default paths in file: profile and your *sb-** program.
2. If there are some *ERROR* show out. You need kill dwmblocks and run again.
#+begin_src bash
ps aux | grep dwmblocks
kill -9 6416
#+end_src
















https://dwm.suckless.org/status_monitor/

#+begin_src bash
git clone git://git.suckless.org/dwmstatus
cd dwmstatus
make
sudo make PREFIX=/usr install
#+end_src

add
#+begin_example
dwmstatus 2>&1 >/dev/null &
#+end_example

to your .xinitrc

*** slock :slock:
Simple X display locker. This is the simplest X screen locker we are aware of.
#+begin_src bash
git clone https://git.suckless.org/slock
cd slock
#+end_src

Before, your *sudo make install*
You need

#+begin_src bash
vim config.def.h
#+end_src

change to
#+begin_example
/* user and group to drop privileges to */
static const char *user  = "yanboyang713";
static const char *group = "yanboyang713";
#+end_example


slock: getgrnam nogroup: group entry not found

Test, it is working.

#+begin_src console
[yanboyang713@manjaro] ➜ system whereis slock
slock: /usr/local/bin/slock
#+end_src

/etc/systemd/system/slock@.service

#+begin_example
[Unit]
Description=Lock X session using slock for user %i
Before=sleep.target

[Service]
User=%i
Environment=DISPLAY=:0
ExecStartPre=/usr/bin/xset dpms force suspend
ExecStart=/usr/bin/slock

[Install]
WantedBy=sleep.target
#+end_example

systemctl daemon-reload

sudo systemctl enable slock@yanboyang713.service

*** Basic Commands
+ Moving between windows: *[Alt]+[j]* or *[Alt]+[k]*
+ To move a terminal to another tag: *[Shift]+[Alt]+[<TAG_NUMBER>]*
+ To focus on another tag: *[Alt]+[tag number]*
+ To change the amount of windows in the master area: *[Alt]+[d]* (Decrease) or *[Alt]+[i]* (Increase)
+ To toggle a window between the master and stack area: *[Alt]+[Return]*
+ To kill a window: *[Shift]+[Alt]+[c]*
+ Click another tag with the right mouse button to bring those windows into your current focus.
*** Layouts
*Note*: By default dwm is in tiled layout mode.

+ Tiled: *[Alt]+[t]*
+ Floating: *[Alt]+[f]*
+ Monocle: *[Alt]+[m]*

Further layout modes can be included through patches.

*** Floating
To resize the floating window: *[Alt]+[right mouse button]*

To move it around *[Alt]+[left mouse button]*

Floating in Tiled Layout

+ Toggle floating mode on the active window: *[Alt]+[Shift]+[space]*
+ Resize the window: *[Alt]+[right mouse button]*
+ toggle it in being floating *[Alt]+[middle mouse button]*

If you want to set some type of window to be always floating, look at the config.def.h and the rules array, where the last but one element defines this behaviour.

*** Quitting
To quit dwm cleanly: *[Shift]+[Alt]+[q]*


** DONE Getting Started Fcitx with chinese input method in Linux :Fcitx:chinese:linux:
CLOSED: [2021-12-06 Mon 13:56]
:PROPERTIES:
:EXPORT_FILE_NAME: chineseInputMethod
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction
Fcitx is a lightweight input method framework aimed at providing environment independent language support for Linux. It supports a lot of different languages and also provides many useful non-CJK features.

In this article, I will introduction:
1. How to install Fcite in Manjaro/Arch Linux.
2. How to Config Environmental variables

*** Install fcite packages in Manjaro

#+begin_src bash
yay -Syu adobe-source-han-sans-otc-fonts adobe-source-han-serif-otc-fonts
#+end_src

#+begin_src bash
yay -Syu fcitx fcitx-googlepinyin fcitx-im fcitx-configtool
#+end_src

*** Config Environmental variables

#+begin_src bash
vim ~/.profile
#+end_src

add:

#+begin_src file
export GTK_IM_MODULE=fcitx
export QT_IM_MODULE=fcitx
export XMODIFIERS=@im=fcitx

fcitx &
#+end_src

#+begin_src bash
source .profile
#+end_src


** DONE Managing Your Dotfiles With Git and Make :Dotfiles:
CLOSED: [2021-12-17 Fri 18:59]
:PROPERTIES:
:EXPORT_FILE_NAME: dotfiles
:EXPORT_OPTIONS: author:nil
:ID:       5daeba1f-2aa5-492a-b3e7-3343722def0b
:END:

*** Introduction
User-specific application configuration is traditionally stored in so called dotfiles (files whose filename starts with a dot). It is common practice to track dotfiles with a version control system such as Git to keep track of changes and synchronize dotfiles across various hosts.
There are various approaches to managing your dotfiles. In this article, I will talk about How to use Make with Git to management your dotfiles and package.

make - Unix, Linux Command is widely in used, so you can easier doing symlinking/copying/generating files.

*** Creating a dotfiles repository
The advantage of using a versioning system for this, is that it’s easy to copy your changes to the other machines. As an extra, you can go back to a previous version in case you broke the configuration files, or you can share your awesome preferences with other developers.

1. Create a directory of your dotfiles and navigate to the root directory.
   #+begin_src bash
mkdir dotfiles
cd dotfiles/
   #+end_src

2. Initialize the local directory as a Git repository.
   #+begin_src console
[yanboyang713@Boyang-PC dotfiles]$ git init -b main
Initialized empty Git repository in /home/yanboyang713/dotfiles/.git/
   #+end_src

3. To create a repository for your project on GitHub, use the gh repo create subcommand. Replace project-name with the desired name for your repository. If you want your project to belong to an organization instead of to your user account, specify the organization name and project name with organization-name/project-name. Follow the interactive prompts.
   #+begin_src bash
gh repo create project-name
   #+end_src

*NOTE:* If you facing any issue in this section, please have a look [this document]({{< ref "./git.md" >}}) first.

4. (Option) Pull changes from the new repository that you created. (If you created a .gitignore or LICENSE file in the previous step, this will pull those changes to your local directory.)
   #+begin_src console
[yanboyang713@Boyang-PC dotfiles]$ git pull --set-upstream origin main
From https://github.com/yanboyang713/DotFiles
 * branch            main       -> FETCH_HEAD
   #+end_src

5. When you done your edit. Go ahead. Stage, commit, and push all of the files in your project.

   #+begin_src bash
git add . && git commit -m "initial commit" && git push
   #+end_src

*** Create a main Make File
#+begin_src bash
touch Makefile
#+end_src

*** Package Management

#+begin_src file

#+end_src


* Research :@Research:
** DONE Getting Started with Zotero :zotero:
CLOSED: [2021-12-07 Tue 12:46]
:PROPERTIES:
:EXPORT_FILE_NAME: zotero
:EXPORT_OPTIONS: author:nil
:ID:       2fbd7db1-7307-4837-ad23-93c57ce2c463
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :summary Set-up Zotero with Better Bibtex, zotfile and Scite.
:END:

*** Introduction

*** Why Use Zotero?
+ Be Organized: Keep all of your research and citations in one place
+ Save time: Format fewer citations by hand
+ Collaborate: Work with anyone in the world, anytime
+ It's Free: No cost even after you

*** Zotero Installation

#+begin_src bash
yay -S zotero
#+end_src

*** Launch Zotero :zotero-sync:

**** Create a Zotero Account
If you haven’t already created a Zotero account, please take a few moments to register now [[https://www.zotero.org/user/register][ *Here* ]]. It’s a free way to sync and access your library from anywhere, and it lets you join groups and back up all your attached files.

**** Set up Zotero syncing
You can now set up Zotero syncing to sync your data across multiple computers, access your library online, or collaborate in group libraries. Follow these steps to get started.

1. Open the Sync pane of the Zotero preferences
Goto "Edit" and click "Preferences"

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1618958146/zotero/Wed_Apr_21_08_33_25_AM_AEST_2021_yoifbp.png]]

2. Enter your username and password
Enter your username and password into the Sync preferences and click “Set Up Syncing”. Zotero will now automatically sync your data as you make changes.

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1618958273/zotero/Wed_Apr_21_08_37_31_AM_AEST_2021_on28w2.png]]

*** Zotero Connector
Go to this [[https://chrome.google.com/webstore/detail/zotero-connector/ekhagklcjbdpajgpjgmbionohlpdbjgc][link]].

*** Better Bibtex :better-bibtex:

**** Instaliion
Install by downloading the [[https://github.com/retorquere/zotero-better-bibtex/releases/tag/v5.4.29][latest release]] and save the XPI file, just clicking it and then in Zotero:

1. In the main menu go to Tools > Add-ons
2. Select ‘Extensions’
3. Click on the gear in the top-right corner and choose ‘Install Add-on From File…’
4. Choose .xpi that you’ve just downloaded, click ‘Install’
5. Restart Zotero

**** Settings
1. Go to Edit -> Preferences -> Better BibTex

***** citation key format
You can set key format gengeration the same format key with Google scholar

#+begin_src file
[auth:lower][year][veryshorttitle:lower]
#+end_src

Zotero:
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1631529034/zotero/citationKey_000_gnoo2l.png]]

Google Scholar:
#+begin_src file
@article{jordan2015machine,
  title={Machine learning: Trends, perspectives, and prospects},
  author={Jordan, Michael I and Mitchell, Tom M},
  journal={Science},
  volume={349},
  number={6245},
  pages={255--260},
  year={2015},
  publisher={American Association for the Advancement of Science}
}
#+end_src

**** Export
1. In the main menu go to File > Export Library
2. Format you can choose Better BibTex.
**Inportance Note**: Don't forget choose **keep updated**
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1631527429/zotero/export_ilg1il.png]]
3. Choose folder
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1631527595/zotero/exportFile_rtlngo.png]]
4.  There is a example:
   #+begin_src console
[yanboyang713@Boyang-PC ~]$ head reference.bib

@misc{ActionCommandsBasler,
  title = {Action {{Commands}} | {{Basler}}},
  howpublished = {https://docs.baslerweb.com/action-commands\#action-group-mask},
  file = {/home/yanboyang713/Zotero/storage/NKXFFZRJ/action-commands.html}
}

@misc{ActionCommandsBaslera,
  title = {Action {{Commands}} | {{Basler}}},
  howpublished = {https://docs.baslerweb.com/action-commands},
   #+end_src

*** Zotfile :zotfile:
Install by downloading the [[https://github.com/jlegewie/zotfile/releases/][latest release]] and save the XPI file, just clicking it – and then in Zotero:

1. In the main menu go to Tools > Add-ons
2. Select ‘Extensions’
3. Click on the gear in the top-right corner and choose ‘Install Add-on From File…’
4. Choose .xpi that you’ve just downloaded, click ‘Install’
5. Restart Zotero

**** Settings
1. In the main menu go to Tools -> ZotFile Preferences

***** Location of Files
1. Set **Custom Location**. For example: /home/yanboyang713/papers
2. Use subfolder defined by **/%a**, mean author name.

***** Renaming Rules
1. Set *Format for all Item Types except Patents*: {%b}
This will rename file same with your Citation Key.
2. Set *Maximum number of authors* choose 1
3. Uncheck *Add suffix when authors are omitted*

*** Scite :scite:
Install by downloading the [[https://github.com/scitedotai/scite-zotero-plugin/releases][latest release]]  and save the XPI file, just clicking it – and then in Zotero:

1. In the main menu go to Tools > Add-ons
2. Select ‘Extensions’
3. Click on the gear in the top-right corner and choose ‘Install Add-on From File…’
4. Choose .xpi that you’ve just downloaded, click ‘Install’
5. Restart Zotero

*** Reference List
1. http://zotfile.com/
2. https://github.com/scitedotai/scite-zotero-plugin
3. https://retorque.re/zotero-better-bibtex/


** DONE Building a Second Brain :second-brain:org-mode:emacs:
CLOSED: [2021-12-07 Tue 10:18]
:PROPERTIES:
:EXPORT_FILE_NAME: SecondBrain
:EXPORT_OPTIONS: author:nil
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: subtitle: Capturing, Organizing, and Sharing Knowledge for Scholars and Researchers
:END:

*** Introduction
Building a Second Brain is about creating a reliable system – outside your physical skin-and-bone bodily boundaries – for storing, organising, digesting, and eventually transforming information into Good Creative Output.

Much like any well-integrated tool, I am currently using *Emacs Org Mode* as Second Brain *core* tool. This system also includes many other peripheral components that make up the complete *Second Brain* system.

I will follow the below overview diagram introducte each components one by one.

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1638851036/research/orgmode2_firjld.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1638851036/research/orgMode_asb3an.png]]

*** Pre-reading
1. [[id:48390f93-f8f0-435c-8938-acf20f581e46][Emacs/Getting Started with Doom Emacs]]

*** Org-mode modules
**** Planning
+ Task management (pomodoro method; time-blocking)
+ Time management (appointments; time-blocking)
***** TODO’s and tags
+ These are identifiers in an org-file as tasks or reminders.
+ The types of TODO’s can either be set globally in your init file, or they can be file/buffer specific.
+ They are created as a subtree (think ‘heading’), or in-line ('org-inlinetask).
+ You can assign deadlines, scheduled date and time, active timestamps, and inactive timestamps
***** Org-capture
These are customizable org-headings that you can create on-the-go.
They can be regular TODO’s or just notes.
***** Org-agenda
Populates all your TODO’s and appointments into a singular view.
Default is week-view.
Using org-super-agenda, I set up my agenda as a daily view with
appointments, deadlines, and a habit tracker.
***** Org-sidebar
Another way of accessing your TODO’s that are outside of your agenda. I
am using it to keep my project-specific TODO’s in the main project org
file.
**** Writing
***** org-roam
A note-taking package that replicates Roam Research which is based on
the Zettelkasten method. I use it to build my literature review and I use
org-roam-server to visualize my notes into a network.
It builds on the strength of org-mode’s hyperlinking properties.
***** Org-roam-bibtex
Utilizes a combination of org-ref, helm-bibtex, and
bibtex-completion to streamline note-taking workflow with references
within the org-roam ecosystem.
***** Org-noter
I use it to annotate PDFs and take notes within the same buffer.
+ Works extremely well with PDF-tools.
+ org-noter-create-skeleton

***** Org-transclusion
An effective way of “copy/pasting” text from one org file (let’s say
an org-roam note or a section of your thesis/dissertation) into your
main org file.
It will export all the transcluded text.
Sort of equivalent to “#+INCLUDE:”

**** Reference Management
   [[id:2fbd7db1-7307-4837-ad23-93c57ce2c463][Research/Getting Started with Zotero]]


* Blog :@Blog:
** DONE Creating a Hugo blog :hugo:
CLOSED: [2021-12-14 Tue 17:31]
:PROPERTIES:
:EXPORT_FILE_NAME: hugo
:EXPORT_OPTIONS: author:nil
:ID:       ccc106d1-2c4b-45b4-a878-08ee337440d6
:END:

*** Overview
This is a tutorial on creating a blog or website using [[https://gohugo.io/][hugo]] (a static site generator).

We will use Docker as our development environment. Also, we will storge our Hugo development code in Github main branche and automate the whole process with Github Action Workflow for Github Pages.

*** Prerequisites
You will need to have the following:

1. Docker installation. Docker is available for Mac/Windows/Linux and is easily installed. You can following the link at the below for install Docker.
 link "content/posts/dockerinstall.md" install Docker "go!"
2. Domain Name (optional). I will be using a domain name for my configuration. If you wish to follow along fully, you’ll need a domain name but, you can just as easily stick with the free subdomain provided by Github pages, i.e. name.github.io, and skip the domain name related aspects in this tutorial. If you don’t have a domain name but wish to purchase one, [[https://www.namecheap.com/][namecheap]] is an excellent provider and fits in well with this tutorial.
3. Github account, sign up for free on github.com

In the subsequent sections, we will be covering the complete setup of https://yanboyang.com

*** Set-up
**** On Github, create a repository called something.github.io
In my case, I created yanboyang.github.io
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1608974742/hexo/Sat_Dec_26_05_24_25_PM_CST_2020_zbblkc.png]]

**** Clone this repository to Local
#+begin_src console
[yanboyang713@boyang Documents]$ git clone https://github.com/yanboyang713/yanboyang.github.io.git
Cloning into 'yanboyang.github.io'...
remote: Enumerating objects: 760, done.
remote: Counting objects: 100% (760/760), done.
remote: Compressing objects: 100% (148/148), done.
remote: Total 760 (delta 289), reused 743 (delta 276), pack-reused 0
Receiving objects: 100% (760/760), 4.13 MiB | 1.88 MiB/s, done.
Resolving deltas: 100% (289/289), done.
#+end_src

#+begin_src bash
# Go to your blog root folder
cd yanboyang.github.io
#+end_src

**** Create a new Hugo site
#+begin_src bash
docker run --rm -it \
  -v $(pwd):/src \
  -w="/src" \
  klakegg/hugo:latest \
  "new site . --force"
#+end_src

**** Chown User and Group Recursively
#+begin_src bash
sudo chown -R yanboyang713:yanboyang713 .
#+end_src

**** Add the theme to your blog
#+begin_src bash
git submodule add https://github.com/sunt-programator/CodeIT.git themes/CodeIT
#+end_src

**** Basic Configuration
The following is a basic configuration for the CodeIT theme:

#+begin_src toml
baseURL = "http://example.org/"
# [en, zh-cn, fr, ...] determines default content language
defaultContentLanguage = "en"
# language code
languageCode = "en"
title = "My New Hugo Site"

# Change the default theme to be use when building the site with Hugo
theme = "CodeIT"

[params]
  # DoIt theme version
  version = "0.2.X"

[menu]
  [[menu.main]]
    identifier = "posts"
    # you can add extra information before the name (HTML format is supported), such as icons
    pre = ""
    # you can add extra information after the name (HTML format is supported), such as icons
    post = ""
    name = "Posts"
    url = "/posts/"
    # title will be shown when you hover on this menu link
    title = ""
    weight = 1
  [[menu.main]]
    identifier = "tags"
    pre = ""
    post = ""
    name = "Tags"
    url = "/tags/"
    title = ""
    weight = 2
  [[menu.main]]
    identifier = "categories"
    pre = ""
    post = ""
    name = "Categories"
    url = "/categories/"
    title = ""
    weight = 3

# Markup related configuration in Hugo
[markup]
  # Syntax Highlighting (https://gohugo.io/content-management/syntax-highlighting)
  [markup.highlight]
    # false is a necessary configuration (https://github.com/dillonzq/LoveIt/issues/158)
    noClasses = false
#+end_src

**** Create Your First Post
Here is the way to create your first post:
#+begin_src bash
docker run --rm -it \
  -v $(pwd):/src \
  -w="/src" \
  klakegg/hugo:latest \
  "new posts/first_post.md"

sudo chown -R yanboyang713:yanboyang713 content
#+end_src

*NOTE:* Go to your first post and change the *draft: false*

**** Launching the Website in Docker

#+begin_src bash
docker run --rm -it \
  -v $(pwd):/src \
  -p 1313:1313 \
  klakegg/hugo:latest \
  server
#+end_src

Web Server is available at http://localhost:1313/

**** Build Hugo With GitHub Action
#+begin_src bash
mkdir -p .github/workflows
vim .github/workflows/main.yml
#+end_src

#+begin_src yml
name: github pages

on:
  push:
    branches:
      - main  # Set a branch to deploy

jobs:
  deploy:
    runs-on: ubuntu-18.04
    steps:
      - name: Git checkout
        uses: actions/checkout@v2
        with:
          submodules: true  # Fetch Hugo themes (true OR recursive)
          fetch-depth: 0    # Fetch all history for .GitInfo and .Lastmod

      - name: Setup Hugo
        uses: peaceiris/actions-hugo@v2
        with:
          hugo-version: 'latest'
          extended: true

      - name: Build
        run: hugo --minify

      - name: Deploy
        uses: peaceiris/actions-gh-pages@v3
        with:
          personal_token: ${{ secrets.TOKEN }}
          external_repository: yanboyang713/yanboyang713.github.io
          publish_dir: ./public
          publish_branch: gh-pages
          keep_files: true
          user_name: yanboyang713
          user_email: yanboyang713@gmail.com
          cname: yanboyang.com
#+end_src

*** Post your blog
I written a bash script for push your blog to Github evertime.

*NOTE:* If you need add your public key.
1. Go to Repository Settings
2. Go to Deploy Keys and add your public key with the Allow write access

#+begin_src bash
#!/bin/bash
###############################################################
## AUTHOR Name: Boyang Yan                                   ##
## KEYWORDs: Hugo Depolyment                                 ##
## PURPOSE: This program for push Hugo to github             ##
## ENVIRONment: mac, Linux                                   ##
## COMMENTs:                                                 ##
## Last Modified Date: 25.05.2021                            ##
###############################################################

printf "\033[0;32mDeploying updates to GitHub...\033[0m\n"

deploy () {

    echo "$1"

    # get Repositorie Name
    RepositorieName=$(grep -oP '(?<=[[:space:]]).*?(?=!)' <<< "$1")

    echo "Your Github Repositorir Name is: $RepositorieName"

    #set git remote URL
    git remote set-url origin git@github.com:"$RepositorieName".git

    # Add changes to git.
    git add -A

    # Commit changes.
    msg="Published on $(date +'%Y-%m-%d %H:%M:%S')"

    echo "$msg"
    git commit -m "$msg"

    # Push source and build repos.
    git push

}

initSSHkey () {
    echo "***** App .ssh directory is empty, initialising ssh key and configuring known_hosts for common git repositories (github/gitlab) *****"
    rm -rf ~/.ssh/*
    ssh-keygen -t rsa -f ~/.ssh/id_rsa -q -P ""
    ssh-keyscan github.com > ~/.ssh/known_hosts 2>/dev/null
    ssh-keyscan gitlab.com >> ~/.ssh/known_hosts 2>/dev/null

    echo "Init SSH key Done!!!"
}

testConnectionAndDeploy () {
    # Testing Github connection success or not
    testConnection=$(ssh -T git@github.com 2>&1)

    # success connect to Github
    if [[ $testConnection =~ successfully ]]; then
        echo "***** You can connect to Github successfully **********"
        deploy "$testConnection"
    else
        echo "***** Could NOT connect to Github!!! Please, update your Github deploy keys *******"
        echo "***** You can add the below public key as your deploy key *****"
        cat ~/.ssh/id_rsa.pub

        read -p "Are you done update your deploy key (Y/n)? " -n 1 -r
        echo    # (optional) move to a new line
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            testConnection=$(ssh -T git@github.com 2>&1)
            if [[ $testConnection =~ successfully ]]; then
                deploy "$testConnection"
            else
                echo "Please wait a few minutes, and try again !!!"
            fi
        fi
    fi
}


# Have ssh key
if [ "$(ls -A ~/.ssh 2>/dev/null)" ]; then
    echo "***** .ssh directory exists and has content, continuing *****";
    testConnectionAndDeploy
# Have NOT ssh key
else
    initSSHkey
    testConnectionAndDeploy

fi;
#+end_src

*** Set up your Google Analytics account
Head over to https://analytics.google.com/ and make an account (or sign in with your Google account).

Set up your “Property”, give it a name, and point it to the URL of the site you plan on tracking.

Click through the basic options until you land on a page with a Tracking Code. This is the value we need to be keeping track of.
If you’re having trouble with this step then I recommend checking out [[http://cloudywithachanceofdevops.com/posts/2018/05/17/setting-up-google-analytics-on-hugo/][this]] tutorial, which has very detailed step by step instructions for configuring your GA account.

**** Configuring Hugo
Hugo has a built in template for Google Analytics. All we need to do is make sure the template gets included in all our pages, so we can have more detailed tracking.

**** header.html
The easiest way to do this is to insert the template into the header.html file used by your Hugo theme of choice.

Since it is common to use a git submodule as a theme directory, it could be undesirable and messy to write changes directly into the submodule.

The solution to this is to create another directory layouts in the root of your hugo directory. Hugo looks at the contents of this folder and uses it to overwrites the styles defined by your theme.

For example I use the Terminal theme. The header file is located at <HUGO ROOT>/themes/terminal/layouts/partials/header.html

We can copy this file to our new directory, so it sits at <HUGO ROOT>/layouts/partials/header.html.

Next we can edit our header.html file to include the template for google analytics. Your header.html will look different depending on the theme you’re using, but there should be a <head> section at the top. If not we can simply create it.

#+begin_src html
<head>
  {{ template "_internal/google_analytics.html" . }}
  {{ template "_internal/google_analytics_async.html" . }}
</head>
#+end_src

Adding these two lines inside <head> will make the google tracking code embed in the HTML of all of your pages.

config.toml
Finally we need to use that tracking code from earlier. At the top level of your config.toml, add the line

#+begin_src toml
googleAnalytics = "UA-302012394-1"
#+end_src

Replacing the string with your Google Analytics tracking code. After you rebuild the site, everything should work as expected! You should also be able to detect the traffic by running the server locally as well, before you push to production.

Note!
I had quite the time figuring out why I couldn’t see any activity on my site. Being the paranoid privacy nut that I am, I have a multitude of tracker/script blocker plugins on my browsers, to block nefarious ads/scripts.

This also blocks the tracking I do want, wouldn’t you know. I switched over to a private window without plugins and the traffic was instantly visible!

*** Write Blog with Emacs and ox-hugo package
[[https://ox-hugo.scripter.co/][ox-hugo]] is an awesome way to blog from org-mode. It makes it possible for posts in org-mode format to kept separate, and it generates the Markdown files for Hugo. Hugo supports org files, but using ox-hugo has multiple advantages:

+ Parsing is done by org-mode natively, not by an external library. Although goorgeous (used by Hugo) is very good, it still lacks in many areas, which leads to text being interpreted differently as by org-mode.
+ Hugo is left to parse a native Markdown file, which means that many of its features such as shortcodes, TOC generation, etc., can still be used on the generated file.

**** Prerequisites
Currently, I am using Doom Emacs, which includes and configures ox-hugo as part of its *(:lang org +hugo)* module, so all that’s left is to configure some parameters to my liking.

I set org-hugo-use-code-for-kbd so that I can apply a custom style to keyboard bindings in my blog.

#+begin_src emacs-lisp
(after! ox-hugo
  (setq org-hugo-use-code-for-kbd t))
#+end_src

**** Auto-export the whole project on Saving
***** Step 1: Enable minor mode org-hugo-auto-export-mode
This minor mode is disabled by default. It can be enabled separately at project level or file level.

*Note* that once you update the .dir-locals.el file or file-local Variables as shown below, you will be prompted by Emacs to tell it if those settings are safe. Hit ! in that prompt to says yes and to save that choice for future Emacs sessions.

***** Step 2: Enable for the whole project
If you want to enable auto-exporting for the whole project, add this to the *.dir-locals.el* file in the project root:

#+begin_src emascs-lisp
(("content-org/"
  . ((org-mode . ((eval . (org-hugo-auto-export-mode)))))))
#+end_src

Above assumes that the Org files are in the “content-org"/ directory (at any nested level in there) relative to that .dir-locals.el file:

#+begin_src file
<HUGO_BASE_DIR>
  ├── config.toml
  ├── content
  ├── content-org      <-- Org files in there
  ├── static
  ├── themes
  └── .dir-locals.el
#+end_src

***** Step 3: Create a post with org mode
In content-org directory, create a xxx.org file.

#+begin_src org

#+title: Boyang Yan's Blog
#+hugo_base_dir: ~/blog/
#+hugo_section: posts
#+hugo_front_matter_format: yaml

*Emacs :@Emacs:
**DONE Getting Started with Doom Emacs
CLOSED: [2021-10-05 Tue 03:44]
:PROPERTIES:
:EXPORT_FILE_NAME: doom
:EXPORT_OPTIONS: author:nil
:END:
***Prerequisites
****Instation Dependencies
#### Arch Linux
#+begin_src bash
# required dependencies
pacman -S git emacs ripgrep
# optional dependencies
pacman -S fd
yay -S emacs-pdf-tools-git
#+end_src

#+end_src

When you save this file, you will found there are doom file created in your blog->content->posts automately.

**** Export bindings
The common ox-hugo export bindings are:

***** For both one-post-per-subtree and one-post-per-file flows
+ *C-c C-e H H*
  Export “What I Mean”.
  If point is in a valid Hugo post subtree, export that subtree to a Hugo post in Markdown.
  A valid Hugo post subtree is an Org subtree that has the *EXPORT_FILE_NAME* property set.
  If the file is intended to be exported as a whole (i.e. has the #+title keyword), export the whole Org file to a Hugo post in Markdown.

+ *C-c C-e H A*
  Export all “What I Mean”
  If the Org file has one or more ‘valid Hugo post subtrees’, export them to Hugo posts in Markdown.
  If the file is intended to be exported as a whole (i.e. no ‘valid Hugo post subtrees’ at all, and has the *#+title* keyword), export the whole Org file to a Hugo post in Markdown.
***** For only the one-post-per-file flow
+ *C-c C-e H h*
    Export the Org file to a Hugo post in Markdown.


** DONE Exporting Org Roam notes to hugo :hugo:org-roam:
CLOSED: [2021-12-14 Tue 13:20]
:PROPERTIES:
:EXPORT_FILE_NAME: roamTOhugo
:EXPORT_OPTIONS: author:nil
:END:
*** Introduction

*** Pre-reading
1. Read this article: [[id:ccc106d1-2c4b-45b4-a878-08ee337440d6][Blog/Creating a Hugo blog]] and Set-up your HUGO blog

*** References
1. https://sidhartharya.me/exporting-org-roam-notes-to-hugo/


** DONE Verify Your Site For Baidu Webmaster Tools - Baidu SEO :SEO:Baidu:
CLOSED: [2021-12-15 Wed 13:13]
:PROPERTIES:
:EXPORT_FILE_NAME: baiduSEO
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction
*Baidu Webmaster Tools*, or Baidu Ziyuan, is a resource platform that tracks and enhances your site’s organic performance on Baidu, China’s #1 search engine with 222+ million active users.

In addition to enabling [[https://www.theegg.com/seo/china/using-baidus-mobile-platform-tool-to-accelerate-indexation/][faster mobile indexation on Baidu]] and improved monitoring of your site’s crawlability, Baidu Ziyuan (Baidu Webmaster Tools) identifies and recommends critical SEO best practices and solutions unique to your site to improve your visibility on the most-used search engine in China: Baidu.

But before you can leverage any of the myriad instrumental metrics and functions of Baidu Webmaster Tools, Baidu must first verify your website.

Here, discover the 4 steps to successfully verify your site on Baidu to drive high-quality traffic and engage China’s behemoth and captive search audience.

*** *Step 1*: LOGIN TO YOUR BAIDU ACCOUNT
Firstly, you must [[https://ziyuan.baidu.com/login/index?u=/site/index][login to your Baidu account]]. If you don’t have an account, or need more information on getting started, check out our [[https://www.theegg.com/seo/china/how-to-set-up-baidu-webmaster-tools/][Baidu Webmaster Tools setup guide]].

Once you’re logged in, the account dashboard will appear. Click the blue button to submit your website for verification.

[[https://www.theegg.com/wp-content/uploads/2018/09/1.-Baidu-Webmaster-Tools-Submit-your-website-for-verification-1536x607.jpg]]

*** Step 2: REGISTER YOUR DOMAIN ON BAIDU
Secondly, you’ll need to register your website’s domain name on Baidu.

Remember to input the appropriate protocol header (http:// or https://) before your domain (www. is optional).

Once the domain is registered, you’ll be able to add any subdomains in batches and view their metrics without need for re-verification. For example, if Baidu has verified https://www.example.com, you can add https://sub.example.com and track its performance without having to re-verify.

*Pro Tip*: For quicker verification, Baidu recommends proving that you are the legal owner of your domain name.

[[https://www.theegg.com/wp-content/uploads/2018/09/2.-Baidu-Webmaster-Tools-Register-your-domain-on-Baidu-1536x598.jpg]]

After clicking “下一步” (“Next Step”), a safety verification will pop-up. Drag the scrollbar from left to right to confirm and proceed.

[[https://www.theegg.com/wp-content/uploads/2018/09/3.-Baidu-Webmaster-Tools-Confirm-your-domain-name-registration-on-Baidu.jpg]]

*** Step 3: SELECT YOUR SITE CATEGORIES
Baidu lets you pick from 20+ categories that best describe your site’s content, including film and video animation, domestic services, music, travel, and much more.

You can pick up to 3 categories.

[[https://www.theegg.com/wp-content/uploads/2018/09/4.-Baidu-Webmaster-Tools-Pick-3-categories-that-best-match-your-website-content.jpg]]

You can view and modify your [[https://ziyuan.baidu.com/property/index][category settings]] by going to the top navigation bar and clicking:

“搜索服务” (Search Service) > “搜索展现” (Search Display) > “站点属性” (Site Categories)

[[https://www.theegg.com/wp-content/uploads/2018/09/5.-Baidu-Webmaster-Tools-How-to-view-and-modify-your-site-categories-1536x617.jpg]]

*Important Note*: Input your domain name correctly since you can only modify it once every 30 days once your site is verified.
*** Step 4: VERIFY YOUR WEBSITE
There are 3 ways to verify your website for Baidu Webmaster Tools:

1. File verification
2. HTML tag verification
3. CNAME (Canonical Name) verification

1.   FILE VERIFICATION
Verifying your website through file verification is a 5-step process:

1. Select “File Verification”
2. Download the verification file, which is in HTML format
3. Upload the verification file to the root directory of your site
4. Ensure that the file is valid
5. Submit to complete the verification

*Important Note*: Do not remove the verification file from the directory, even after your site gets verified.

[[https://www.theegg.com/wp-content/uploads/2018/09/6.-Baidu-Webmaster-Tools-Verifying-your-site-via-file-verification.jpg]]

1. HTML TAG VERIFICATION
   There are 3 steps to verify your website through HTML tag verification:

   1. Select “HTML Tag Verification”
   2. Insert the tag to the HTML code of your homepage, between the <head> and </head> tags.
   3. Submit to complete the verification

      [[https://www.theegg.com/wp-content/uploads/2018/09/7.-Baidu-Webmaster-Tools-Verifying-your-site-via-HTML-tag.jpg]]

2. CNAME VERIFICATION
  To verify your website for Baidu through CNAME verification, follow these 3 steps:

   1. Select “CNAME Verification”
   2. Add a new CNAME record using the value provided by Baidu, and resolve it to ziyuan.baidu.com
   3. Submit to complete the verification

*Important Note*: It may take up to 72 hours for the domain name system (DNS) change to take effect. If the verification is not successful immediately, you may try again later.

[[https://www.theegg.com/wp-content/uploads/2018/09/8.-Baidu-Webmaster-Tools-Verifying-your-site-via-CNAME.jpg]]

Getting your site successfully verified for Baidu Webmaster Tools (Baidu Ziyuan) leaves you with game-changing SEO metrics to power your site performance—all while making it more visible and searchable by the world’s largest audience.

So, make sure your site is verified on Baidu to capture this valuable traffic!


** DONE Verify Your Site For Bing Webmaster Tools - Bing SEO :SEO:bing:
CLOSED: [2021-12-15 Wed 20:22]
:PROPERTIES:
:EXPORT_FILE_NAME: bingSEO
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction
[[https://www.bing.com/toolbox/webmaster][Bing Webmaster Tools]] optimizes your site and monitors its performance in Bing search engine results.

You should verify your site with Bing Webmaster Tools after launching your site.

Before you start, create a [[https://signup.live.com/][Microsoft account]].



*** Steps
1. [[https://www.bing.com/toolbox/webmaster][Log in to Bing]] with with your Microsoft Account.
2. Under *My Sites*, enter your site's URL. Click *Add*.
3. On the *Add a site* page, in *Add a sitemap*, enter http://yoursite.com/sitemap_index.xml. Optionally select the time of day when you anticipate the most traffic to your site, then click Add.
On the Verify ownership page, under Option 2, copy the verification code in between content="xxxxxx".
Open a new browser tab and log in to your site.
Navigate to Site Settings > SEO.
Under Search Engines and Analytics, paste the verification code into the Bing Webmaster Tools Verification field. Click Submit.
Return to your Bing Webmaster Tools browser tab. Click Verify. If you receive an "Bing could not verify ownership of this site" message, you may need to wait a few hours and try again.


Once your site is verified, we recommend following these steps:

In Bing Webmaster Tools, navigate to Configure My Site > Geo-Targeting. Change the dropdown from Directory to Domain, then select the country of your target audience. Click Submit.
Navigate to Configure My Site > Users. Under Would you like to add a user?, enter support@lexblog.com in the Email field, then click Add. This allows LexBlog to perform maintenance or troubleshoot your site.


* Windows :@Windows:
** DONE Virtualize Windows 11 with Proxmox :windows:11:proxmox:pve:
CLOSED: [2021-12-20 Mon 12:37]
:PROPERTIES:
:EXPORT_FILE_NAME: windows11PVE
:EXPORT_OPTIONS: author:nil
:END:
*** Introduction
As Microsoft released Windows 11, it was made clear that TPM 2.0 and Secure boot is no longer optional but mandatory, without these capabilities on the hardware/virtual hardware, Windows 11 refuse to install.


*** Step 1: Download Windows 11 ISO
You can click [[https://www.microsoft.com/software-download/windows11][Here]] for download ISO file.

*** Step 2: Download virtio-win drivers disk
You can click [[https://github.com/virtio-win/virtio-win-pkg-scripts][Here]], download Latest virtio-win ISO.

*** Step 3: Upload Windows 11 ISO and Latest virtio-win ISO to ProxMox
login to ProxMox -> choose Node -> local (pve) -> ISO Images -> Upload

*** Step 4: Create VM
Click *Create VM* -> *General*: name: Windows 11

-> *OS*: choose boot OS: ISO image choose Win11 ISO; Type: Microsoft Windows; Version: 11/2022

-> System: SCSI Controller: VirtIO SCSI
Machine: q35
Qemu Agent: Enable
BIOS: OVMF (UEFI)
Add TPM: Enable Version: v2.0
EFI Storage and TPM Storage *should use same storage*

-> Disks:
Bus/Device: VirtIO Block
Disk size (GiB): 64

-> CPU:
Cores: 2
Type: host

-> Memory:
Memory (MiB): 8192

-> Network:
Bridge: vmbr0
Model: VirtIO (paravirtualized)

OK, we done! However, *Don't Start it*
*** Step 5: Add virtio-win ISO
Hardware -> Add -> CD/DVD Drive -> choose IDE and your downloaded ISO

*** Step 6: Check Boot Order
Options -> Boot Order
Make sure your boot Win11 at the begging

Now, ready to start this VM.

*** Step 6: Start

*** Reference List
1. https://www.youtube.com/watch?app=desktop&v=fupuTkkKPDU
2. https://dannyda.com/2021/10/08/how-to-install-windows-11-on-proxmox-ve-pve-without-workarounds/
