#+title: Boyang Yan's Blog
#+hugo_base_dir: ~/blog/
#+hugo_section: posts
#+hugo_front_matter_format: yaml

* Emacs :@Emacs:
** DONE Getting Started with Doom Emacs
CLOSED: [2021-10-05 Tue 03:44]
:PROPERTIES:
:EXPORT_FILE_NAME: doom
:EXPORT_OPTIONS: author:nil
:END:
*** Prerequisites
**** Instation
***** Arch Linux
#+begin_src bash
# required dependencies
pacman -S git emacs ripgrep
# optional dependencies
pacman -S fd
yay -S emacs-pdf-tools-git
```
#+end_src

With Emacs and Doom’s dependencies installed, next is to install Doom Emacs itself:
#+begin_src bash
git clone https://github.com/hlissner/doom-emacs ~/.emacs.d
~/.emacs.d/bin/doom install
#+end_src

*** Sart-up
https://zzamboni.org/post/my-doom-emacs-configuration-with-commentary/

Install the icons to avoid having weird symbols.
*M-x all-the-icons-install-fonts*

*Note*: M-x is SPC :

Open a file

**SPC f f** OR *SPC .*

*** The bin/doom utility
This utility is your new best friend. It won’t spot you a beer, but it’ll shoulder much of the work associated with managing and maintaining your Doom Emacs configuration, and then some. Not least of which is installation of and updating Doom and your installed packages.

It exposes a variety of commands. *bin/doom* help will list them all, but here is a summary of the most important ones:

+ *doom sync*: This synchronizes your config with Doom Emacs. It ensures that needed packages are installed, orphaned packages are removed and necessary metadata correctly generated. Run this whenever you modify your doom! block or packages.el file. You’ll need doom sync -u if you override the recipe of package installed by another module.
+ *doom upgrade*: Updates Doom Emacs (if available) and all its packages.
doom env: (Re)generates an “envvar file”, which is a snapshot of your shell environment that Doom loads at startup. If your app launcher or OS launches Emacs in the wrong environment you will need this. **This is required for GUI Emacs users on MacOS.**
+ *doom doctor*: If Doom misbehaves, the doc will diagnose common issues with your installation, system and environment.
+ *doom purge*: Over time, the repositories for Doom’s plugins will accumulate. Run this command from time to time to delete old, orphaned packages, and with the -g switch to compact existing package repos.
Use doom help to see an overview of the available commands that doom provides, and doom help COMMAND to display documentation for a particular COMMAND.

I recommend you add a couple of *alias* to your ZSH configuration.
#+begin_example
# Doom Emacs
alias doomsync="$HOME/.emacs.d/bin/doom sync"
alias doomupgrade="$HOME/.emacs.d/bin/doom upgrade"
alias doomdoctor="$HOME/.emacs.d/bin/doom doctor"
alias doompurge="$HOME/.emacs.d/bin/doom purge"
alias doomclean="$HOME/.emacs.d/bin/doom clean"
alias doombuild="$HOME/.emacs.d/bin/doom build"
#+end_example

*** Doom config file overview
We already know how to open a file and how to use doom utility, so let's we take a overview for Doom configuration.

Doom Emacs at the least uses three config files.

+ *init.el* defines which of the existing Doom modules are loaded. A Doom module is a bundle of packages, configuration and commands, organized into a unit that can be toggled easily from this file. You also can design your own *Module*.
+ *packages.el* defines which packages should be installed, beyond those that are installed and loaded as part of the enabled modules.
+ *config.el* contains all custom configuration and code. when you have get so many configuration contains, you may need seperate for each category. There is a example. Click [[https://github.com/yanboyang713/doom.git][Here]]

There are other files that can be loaded, but theses are the main ones. The load order of different files is defined depending on the type of session being started.

**** Config file headers
We start by simply defining the standard headers used by the three files. These headers come from the initial files generated by doom install, and contain either some Emacs-LISP relevant indicators like lexical-binding, or instructions about the contents of the file.

+ init.el
+ packages.el
+ config.el

#+begin_example
;;; ../../dotfiles/doom/+research.el -*- lexical-binding: t; -*-
#+end_example

**** Customized variables
Doom [[https://github.com/hlissner/doom-emacs/blob/develop/docs/getting_started.org#configure][does not recommend the Emacs customize mechanism]] :

*Note*: do not use M-x customize or the customize API in general. Doom is designed to be configured programmatically from your config.el, which can conflict with Customize’s way of modifying variables.

All necessary settings are therefore set by hand as part of this configuration file. The only exceptions are “safe variable” and “safe theme” settings, which are automatically saved by Emacs in custom.el, but this is OK as they don’t conflict with anything else from the config.

*** General configuration
My user information.
#+begin_src emacs-lisp
(setq user-full-name "Boyang Yan"
      user-mail-address "yanboyang713@gamil.com")
#+end_src

*** Projects with Projectile, File Explorer with Treemacs & EShell
**** Projectile
Doom Emacs have used package [[https://github.com/bbatsov/projectile][Projectile]] to management our project.

*SPC p p* - Switch to project.
*SPC SPC* - Find a File in a project

#+begin_src emacs-lisp
projectile-project-search-path '("~/Project/" "~/dotfiles/" "~/blog/content-org/")
#+end_src

**** Treemacs
SPC o p - Open
When Treemacs is opened, you can type *q* to close.


** DONE Getting Started with send and receive Email with Doom Emacs
CLOSED: [2021-12-06 Mon 13:44]
:PROPERTIES:
:EXPORT_FILE_NAME: sendAndReceiveEmailWithdoom
:EXPORT_OPTIONS: author:nil
:END:
*** Receive Email
#+begin_src console
yay -S mu mbsync-git
#+end_src

https://devanswers.co/create-application-specific-password-gmail/

#+begin_src console
gpg2 -c xxxxxxx
#+end_src

#+begin_src console
time mu init --maildir=~/MailDir --my-address='yanboyang713@gmail.com'
mu index

time mbsync -c ~/.config/mu4e/mbsyncrc -a

#+end_src

[yanboyang713@Boyang-PC] ➜ ~ time mu init --maildir=~/MailDir --my-address='yanboyang713@gmail.com'
error: failed to open store @ /home/yanboyang713/.cache/mu/xapian: Unable to get write lock on /home/yanboyang713/.cache/mu/xapian: already locked
mu init --maildir=~/MailDir --my-address='yanboyang713@gmail.com'  0.00s user 0.00s system 66% cpu 0.006 total
[yanboyang713@Boyang-PC] ➜ ~ mu index
error: failed to open store @ /home/yanboyang713/.cache/mu/xapian: Unable to get write lock on /home/yanboyang713/.cache/mu/xapian: already locked


pkill -2 -u $UID mu
sleep 1
mu index



Using GPG for mbsync passwords
The basic idea is that every time a password is needed, an particular file is decrypted and loaded. The key for the decryption can be prompted for and be stashed by gpg-agent. The first step is to create a GPG key, which is covered very well elsewhere. The standard authentication mechanism for gnus and smtpmail can be reused to store login information for mbsync. For any one account, the password for IMAP access and the password for sending email (usually the same) can be added to ~/.authinfo.gpg:

machine imap.gmail.com login MyAccountName@gmail.com password MYPASSWORD machine smtp.gmail.com login MyAccountName@gmail.com password MYPASSWORD The first line is used by mbsync and the second by smtpmail. The line: PassCmd “gpg2 -q –for-your-eyes-only –no-tty -d ~/.authinfo.gpg | awk ’machine imap.gmail.com login MyAccountName@gmail.com {print $NF}’”

*** Send Email

#+begin_src console
yay -S msmtp msmtp-mta s-nail
#+end_src
echo "hello there username." | msmtp -a default username@domain.com

~/.mailrc
set mta=/usr/bin/msmtp


*** Doom Emacs Set-up
(package! mu4e)

*** Usage
https://cheatography.com/ddoherty03/cheat-sheets/mu4e-with-gmail-hints/


* Computer Vision :@ComputerVision:

** DONE Getting started with FFmpeg
CLOSED: [2021-11-01 Mon 15:03]
:PROPERTIES:
:EXPORT_FILE_NAME: ffmpeg
:EXPORT_OPTIONS: author:nil
:END:
*** Overview
FFmpeg is a free software project and is the leading software for everything related to multimedia like video encoding, streaming and muxing.

FFmpeg - "FF" mean "Fast Forward", "mpeg" mean "Moving Picture Expers Group"

*** Installation
https://github.com/jrottenberg/ffmpeg

https://www.whoishostingthis.com/compare/ffmpeg/resources/

alias ffmpeg='docker run -v=`pwd`:/tmp/ffmpeg opencoconut/ffmpeg'

#+begin_src bash
yay -S ffmpeg
#+end_src

*** Using Linux Terminal to Install VLC in Ubuntu
sudo snap install vlc

*** Invert the video stream to a virtual video camera
If your video stream is inverted, you can make a new virtual video camera which inverts the inverted video. You need to install v4l-utils and also v4l2loopback-dkms.

#+begin_src bash
yay -S v4l-utils v4l2loopback-dkms
#+end_src

*** Create the virtual video camera:
#+begin_src bash
modprobe v4l2loopback
#+end_src
https://askubuntu.com/questions/881305/is-there-any-way-ffmpeg-send-video-to-dev-video0-on-ubuntu

Check the name of the newly created camera:

#+begin_src console
[yanboyang713@boyang ~]$ v4l2-ctl --list-devices
Dummy video device (0x0000) (platform:v4l2loopback-000):
	/dev/video0
#+end_src

*** Image to virtual camera
ffmpeg -re -loop 1 -i input.jpg -vf format=yuv420p -f v4l2 /dev/video0


Then you can run ffmpeg to read from your actual webcam (here /dev/video0) and invert it and feed it to the virtual camera:

$ ffmpeg -f v4l2 -i /dev/video0 -vf "vflip" -f v4l2 /dev/video1
You can use the "Dummy" camera in your applications instead of the "Integrated" camera.

Bad image quality
If you experience images being too bright, too dark, too exposed or any other, you can install v4l2ucpAUR to tweak your image output.


** TODO Image Compression Based on Principal Component Analysis (PCA)
:PROPERTIES:
:EXPORT_FILE_NAME: PCAforImage
:EXPORT_OPTIONS: author:nil
:END:
*** Introduction
Principal Component Analysis (PCA) is a linear dimensionality reduction technique (algorithm) that transform a set of correlated variables (p) into a smaller k (k<p) number of uncorrelated variables called principal components while keeping as much of the variability in the original data as possible.

One of the use cases of PCA is that it can be used for image compression — a technique that minimizes the size in bytes of an image while keeping as much of the quality of the image as possible.



** DONE Beginning Explore artificial intelligence and computer vision
CLOSED: [2021-10-07 Thu 19:17]
:PROPERTIES:
:EXPORT_FILE_NAME: firstExploreAIandComputerVision
:EXPORT_OPTIONS: author:nil
:END:

*** What is artificial intelligence?
**** Explore into artificial intelligence
For the definition of artificial intelligence, academic research area always have different understandings. The widely accepted definition is:

+ *Artificial intelligence is the use of machines to simulate human cognitive abilities technology*.

Artificial intelligence involves a wide range of insights, learning, reasoning and decision-making.

From the perspective of industry application, the core ability of artificial intelligence ability is to make judgments or predictions based on given input.

The rise of deep learning and the three booms of AI.
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633594626/cv/MLThreeBooms_xnexn3.png]]

The Turing test, the cornerstone of artificial intelligence
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633607665/cv/turingTest_000_wuxoka.png]]


**** Three core elements of artificial intelligence
Three core elements of AI: data, algorithm and compute resource.
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633609084/cv/threeCoreElements_pj0xlg.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633609181/cv/threeCoreElementsOne_vn5zm9.png]]

***** Data
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633607997/cv/data_yoauah.png]]

***** Algorith
When you give a computer a task, you tell it not only what to do, but how to do it and a set of instructions about how to do it is called an algorithm.

+ Traditional algorithms -- traversal
+ Smarter algorithms -- gradient descent
+ More complex algorithms -- machine learning

***** Compute Resource/Power
 Breakthrough in computing power -- traditional CPU and new computing acceleration technology.

 [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633608323/cv/cpu_tkdhfn.png]]

 [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633608393/cv/fpga_bsknu0.png]]

 [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633608486/cv/compare_ou1gus.png]]

 smart chip
 [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633608633/cv/smartChip_ev498y.png]]


**** Artificial intelligence technonly relationship
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633609345/cv/relationship_nugk48.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673844/cv/AIrelationship_myqcj6.png]]

+ *Machine learning*: a way to achieve artificial intelligence

It is a multi-field interdisciplinary subject, involving probability theory, statistics, approximation theory, convex analysis, algorithm complexity theory and other subjects. Machine learning is the core of artificial intelligence, the fundamental way to make computers intelligent, and its applications are widespread
In all fields of artificial intelligence, it mainly uses induction and synthesis rather than deduction.

+ *Deep learning*: a technology that implements machine learning.

It uses a deep neural network to process the model more complex, so that the model has a deeper understanding of the data. It is a method of machine learning based on data representation learning. The motivation is to establish and simulate the human brain to analyzing the learning neural network, it imitates the mechanism of the human brain to interpret data, such as images, sounds and texts. The essence of deep learning is to learn more by building a machine learning model with many hidden layers and massive training data. Use the features to ultimately improve the accuracy of classification or prediction.

+ *Artificial neural network*: a machine learning algorithm

Neural networks generally have input layer -> hidden layer -> output layer. Generally speaking, a neural network with more than two hidden layers is called a deep neural network. Deep learning is a machine that uses a deep architecture like a deep neural network. Learn method.

***** What is machine Learning
*Artificial intelligence is a technology that uses machines to simulate human cognitive abilities*.

+ Traditional artificial intelligence methods: logical reasoning, expert systems (answering questions based on manually defined rules), etc.;

+ Contemporary artificial intelligence generally acquires the ability to make predictions and judgments through learning-machine learning

#+begin_example
Normal cat: round head, short face, five fingers on the forelimbs, four toes on the hind limbs, with sharp and curved claws at the ends of the toes,
The claws can stretch. Nocturnal. ---Baidu Encyclopedia
#+end_example

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633610156/cv/ml1_zrobow.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633610156/cv/ml2_lb82ew.png]]

***** Typical machine learning process
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633610289/cv/ml3_tzhdmh.png]]

***** What is Neural Network
****** How do people think? --Biological Neural Network

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633610519/cv/neuron_lhvbvb.png]]

sensor:
1. External stimulation passes through nerve endings and turns converted into electrical signals, transduced to nerve cells (Also called neuron)
2. Numerous neurons form the nerve center
3. The nerve center integrates various signals to do judgement.
4. According to the instructions of the nerve center, the human body respond to external stimuli.

****** How does the machine think? --Artificial neural networks

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633672744/cv/arNeuron_awz0bq.png]]

Artificial neuron

Input: x1,x2,x3
Output: output
Simplified model: It is agreed that each input has only two possible 1 or 0

All inputs are 1, which means that various conditions are met, and the output is 1;

All inputs are 0, which means that the condition is not true, and the output is 0

#+begin_example
Is watermelon good or bad?
Color: green; root: curled up; knock: voiced thoughts. ---Good melon
#+end_example

#+begin_example
Family Spring Outing?
Price: high and low; weather: good or bad; family: can you travel
#+end_example

****** The logical architecture of the neural network

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673214/cv/architectureNeuralNetworkOne_sijbou.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673216/cv/architectureNeuralNetworkTwo_l4dsh1.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673225/cv/architectureNeuralNetworkThree_cuepcr.png]]



***** What is Deep Learning
Deep neural network & deep learning

+ The traditional neural network has developed to a situation with multiple hidden layers,

+ Neural networks with multiple hidden layers are called deep neural networks, and machine learning research based on deep neural networks is called deep learning.

  [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673700/cv/DeepLearning_tq9n1e.png]]

**** The foreseeable future of artificial intelligence
***** Computer vision

+ Typical technology:
Face detection, tracking, recognition and attribute analysis, pedestrian and vehicle detection, tracking, recognition and attribute analysis, text detection and recognition, object detection and recognition

+ Typical application:
Face authentication, intelligent transportation, robot vision (such as drones), image search engine, image and video understanding, image and video beautification

***** Speech Recognition

+ Typical technology:
Voice recognition, voiceprint recognition, multi-microphone array system

+ Typical application:
Voice input, voice control, intelligent assistant, machine translation, robot hearing

***** natural language

+ Typical technology:
Words and sentences embedded, semantic modeling

+ Typical application:
Chatbot, smart assistant, smart customer service, video Frequency understanding, machine translation

*** Computer vision (CV)
**** What is CV
Several more rigorous definitions:

+ "Construct a clear and meaningful description of the objective objects in the image" (Ballard & Brown, 1982)

+ "Calculate the characteristics of the three-dimensional world from one or more digital images" (Trucco & Verri, 1998)

+ "Based on perceptual images to make useful decisions for objective objects and scenes" (Sockman & Shapiro, 2001)

Overview in one sentence:

It means that the computer has the ability to see, know, and think. It can be said that the computer has vision, that is, computer vision.

**** Deep learning and CV
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633674756/cv/computerVisionOne_kjgmyf.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633674755/cv/computerVisionTwo_l2tvqa.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633674756/cv/computerVisionThree_rdqpta.png]]

**** Application of CV
***** Image Classification

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678185/cv/classificationOne_lqnjcd.png]]

Image Classification - Neural Neural Network (CNN)

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678184/cv/classificationTwo_pu6kuc.png]]

Linear rectifier layer--RELU

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678183/cv/classificationThree_zhnzgk.png]]

Pooling layer-pool

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678183/cv/classificationFour_k9lyy9.png]]

***** Target Detection

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678492/cv/detactionOne_rsz53f.png]]

R-CNN

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678492/cv/detactionTwo_lnellb.png]]

***** Target Tracking
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678587/cv/tracking_rjkb2z.png]]

***** Semantic Image Segmentation
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678681/cv/SegmentationOne_zfgix9.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v163367868e/cv/SegmentationTwo_orkdal.png]]

***** Instance Segmentation

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678767/cv/instanceOne_duswl8.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678767/cv/instanceTwo_osehjy.png]]

**** CV skills tree construction

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633680105/cv/treeOne_kbmiwg.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633680106/cv/treeTwo_wchdqb.png]]


** DONE Gettting Started SRS(Simple Realtime Server)
CLOSED: [2021-11-13 Sat 11:43   ]
:PROPERTIES:
:EXPORT_FILE_NAME: srs
:EXPORT_OPTIONS: author:nil
:END:

*** Overview
SRS is a simple, high efficiency and realtime video server, supports RTMP/WebRTC/HLS/HTTP-FLV/SRT.


* Networking :@Networking:
** DONE Configuring Network Bonding :bonding:
CLOSED: [2021-12-12 Sun 17:57]
:PROPERTIES:
:EXPORT_FILE_NAME: bonding
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction
Network bonding refers to the combination of network interfaces on one host for redundancy and/or increased throughput.

Redundancy is the key factor:
we want to protect our virtualized environment from loss of service due to failure of a single physical link. This network bonding is the same as the Linux network bonding.

Using network bonding in *OpenVswitch OVS* require some switch configuration.

In this article, I will demonstrate How to use *Networking Bonding* between OVSBridge and Ubiquiti.

There are three modes of network bonding:
+ *Active-Passive*: there is one NIC active while another NIC is asleep. If the active NIC goes down, another NIC becomes active.
+ *Link Aggregation*: aggregated NICs act as one NIC which results in a higher throughput.
+ *Load Balanced*: the network traffic is equally balanced over the NICs of the machine.

*** Ubiquiti bonding Set-up
Steps for configure *Link Aggregation Groups*

1. Navigate to the *Devices* section in the UniFi Network application and click on the switch to open the Properties Panel.
2. In the Properties Panel, go to the *Ports* section and select a port that will participate in the link aggregation group by selecting Edit (pencil icon) when hovering over it.
3. Click *Profile Overrides* to expand section.
4. Under Operations, select *Aggregate*. This will expose some Aggregate options.
5. Under the Aggregate Ports input which ports to include in the LAG.

https://help.ui.com/hc/en-us/articles/360007279753-UniFi-USW-Configuring-Link-Aggregation-Groups-LAG-

*** OpenVswitch Bonding
https://docs.openvswitch.org/en/latest/topics/bonding/


** DONE Getting Started OpenWrt :openwrt:
CLOSED: [2021-12-12 Sun 11:42]
:PROPERTIES:
:EXPORT_FILE_NAME: openwrt
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction
The OpenWrt Project is a Linux operating system targeting embedded devices. Instead of trying to create a single, static firmware, OpenWrt provides a fully writable filesystem with package management. This frees you from the application selection and configuration provided by the vendor and allows you to customize the device through the use of packages to suit any application.

OpenWrt official Website [[https://openwrt.org/][Here]].

In this article, I will talk about:
1. Compile Openwrt from Source Code.
2. Install Openwrt on ProxMox.
3. Basic Set-Up for Openwrt.

*** Compile OpenWrt
 1. Make sure your have a avaiable Linux/MacOS system, offers recommand Ubuntu 18 LTS x64.
 2. Install required packages.
    #+begin_src bash
sudo apt-get update

sudo apt-get -y install build-essential asciidoc binutils bzip2 curl gawk gettext git libncurses5-dev libz-dev patch python3.5 python2.7 unzip zlib1g-dev lib32gcc1 libc6-dev-i386 subversion flex uglifyjs git-core gcc-multilib p7zip p7zip-full msmtp libssl-dev texinfo libglib2.0-dev xmlto qemu-utils upx libelf-dev autoconf automake libtool autopoint device-tree-compiler g++-multilib antlr3 gperf
    #+end_src
 3. Getting Source Code and enter direction.
    #+begin_src bash
git clone -b main --single-branch https://github.com/Lienol/openwrt openwrt
cd openwrt
    #+end_src
 4. Add additional package/plugin to Source Code, such as *Passwall*.
    #+begin_src bash
vim feeds.conf.default
    #+end_src

    Adding src-git at the end of file.
    #+begin_src file
src-git passwall https://github.com/xiaorouji/openwrt-passwall
    #+end_src
 5. Update the feeds
    #+begin_src bash
./scripts/feeds clean
./scripts/feeds update -a
./scripts/feeds install -a
    #+end_src
 6. Configure the firmware image
    #+begin_src bash
make menuconfig
    #+end_src
    *NOTE*:
    1. First three menu is very inportance, Please, carefully choose with your correct CPU architecture. For example, x86.

    2. Settings your package/plugins luci-app, such as, luci-app-passwall

 7. Downloading DL library.
    #+begin_src bash
make -j8 download v=s
    #+end_src
 8. Start Compile
    #+begin_src bash
make -j1 V=s
    #+end_src
    *NOTE*: -j1 is followed by the number of threads. Single thread is recommended for the first compilation.
 9. Output path after compilation.
    *openwrt/bin/targets*

*** Installation
https://www.77bx.com/34.html

**** Firstly, Upload your compiled IMG File to ProxMox
sftp into ProxMox.
#+begin_src console
[yanboyang713@manjaro] ➜ 64 (U main) sftp root@192.168.1.2
The authenticity of host '192.168.1.2 (192.168.1.2)' can't be established.
ED25519 key fingerprint is SHA256:VPD220yr70tQsDuIn/z41hTWzte0bZ1k6wF8JjBzjiw.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.1.2' (ED25519) to the list of known hosts.
root@192.168.1.2's password:
Connected to 192.168.1.2.
#+end_src

Uploading
#+begin_src bash
sftp> put openwrt-x86-64-generic-squashfs-combined.img.gz
Uploading openwrt-x86-64-generic-squashfs-combined.img.gz to /root/openwrt-x86-64-generic-squashfs-combined.img.gz
openwrt-x86-64-generic-squashfs-combined.img.gz                                                          100%   53MB 111.4MB/s   00:00
sftp>
#+end_src

*NOTE*:
1. List local directory.
   #+begin_src console
sftp> lls
openwrt-x86-64-generic-squashfs-combined.img.gz
   #+end_src
2. List remote directory.
    #+begin_src console
sftp> ls
ROSinstall.sh  interfaces     temp
    #+end_src

**** Secondly, ssh into ProxMox and using the below Script create a new VM.
#+begin_src bash
#!/bin/bash

#vars
vmID="nil"

echo "############## Start of Script ##############

#List already existing VM's and ask for vmID

echo "== Printing list of VM's on this hypervisor!"
qm list
echo ""
read -p "Please Enter free vm ID to use:" vmID
echo ""

# Create storage dir for VM if needed.
if [ -d /var/lib/vz/images/$vmID ]
then
    echo "-- VM Directory exists! Ideally try another vm ID!"
    read -p "Please Enter free vm ID to use:" vmID
else
    echo "-- Creating VM image dir!"
    mkdir /var/lib/vz/images/$vmID
fi


# Creating VM
echo "-- Creating new CHR VM"
qm create $vmID \
  --name chr-$version \
  --net0 virtio,bridge=vmbr0 \
  --bootdisk virtio0 \
  --ostype l26 \
  --memory 2048 \
  --onboot no \
  --sockets 1 \
  --cores 1 \
  --virtio0 local-lvm:vm-$vmID-disk-0

# Decompression image
gzip -d /root/openwrt-x86-64-generic-squashfs-combined.img.gz

# Resize image
qemu-img resize /root/openwrt-x86-64-generic-squashfs-combined.img +10G

echo "-- Import RAW image to local-lvm"
qm importdisk $vmID /root/openwrt-x86-64-generic-squashfs-combined.img local-lvm

echo "############## End of Script ##############"

#+end_src

*** Set-up
1. Change Password
   #+begin_src bash
passwd
   #+end_src
2. Set *Lan* IP Address, Gateway and DNS

network -> interface -> edit

Content need to set-up:
+ General Settings:
IPv4 IP Address: 192.168.1.252
IPv4 gateway: 192.168.1.253
IPv4 bradcast: 192.168.1.0
+ Advantages Settings:
DNS set as public DNS Server: 114.114.114.114, 114,114,115,115

1. If this Openwrt as your bypass router, please follow at the below settings.
   *LAN Settings*: Let lede only be used as a pure bypass route, DHCP and IPv6 are both allocated by the main route.

   + DHCP Server -> General Settings -> choose Ignore this interface.
   + IPv6 Settings -> RA Service - Disable
   + IPv6 Settings -> DHCPv6 Service - Disable

2. Let lede obtain IPv6 information normally.
add new interface -> name: IPv6; Protocol: DHCPv6 Client Device: @lan -> create interface -> Firewall settings: lan -> create interface.


* Cluster :@Cluster:
** DONE Getting Started Set-up OVS for Proxmox
CLOSED: [2021-12-06 Mon 12:58]
:PROPERTIES:
:EXPORT_FILE_NAME: ProxmoxOVS
:EXPORT_OPTIONS: author:nil
:ID:       ffb1b001-6dba-40f3-a222-4260015a6863
:END:

*** Introduction

*** Install Open vSwitch
Update the package index and then install the Open vSwitch packages by executing:

#+begin_src console
 apt update
 apt install ifupdown2
 apt install openvswitch-switch
#+end_src

root@pve-home:~# cat /etc/network/interfaces
https://karneliuk.com/2021/08/infrastructure-1-building-virtualized-environment-with-debian-linux-and-proxmox-on-hp-and-supermicro/

ifreload -a
ifup vmbr0

#+begin_src file
auto lo
iface lo inet loopback

auto enp6s0
iface enp6s0 inet manual

auto enp1s0
iface enp1s0 inet manual

auto enp2s0
iface enp2s0 inet manual

auto enp3s0
iface enp3s0 inet manual

auto enp5s0
iface enp5s0 inet manual

auto ens9
iface ens9 inet manual

auto vlan1
iface vlan1 inet static
        address 192.168.1.2/24
        gateway 192.168.1.1
        ovs_type OVSIntPort
        ovs_bridge vmbr0
        ovs_options vlan_mode=access
        ovs_extra set interface ${IFACE} external-ids:iface-id=$(hostname -s)-${IFACE}-vif
        dns-nameservers 192.168.1.1 8.8.8.8 8.8.4.4

auto bond0
iface bond0 inet manual
        ovs_bonds enp1s0 enp2s0 enp3s0 ens9 enp5s0
        ovs_type OVSBond
        ovs_bridge vmbr0
        ovs_options vlan_mode=native-untagged bond_mode=balance-slb

auto vmbr0
iface vmbr0 inet manual
        ovs_type OVSBridge
        ovs_ports bond0 vlan1
#+end_src


** DONE Proxmox PCI Passthrough :passthrough:proxmox:
CLOSED: [2021-12-06 Mon 16:04]
:PROPERTIES:
:EXPORT_FILE_NAME: ProxmoxPassthrough
:EXPORT_OPTIONS: author:nil
:ID:       0a2b0901-86a5-4a83-b6a7-e9f18516053a
:END:

*** Introduction
PCI passthrough allows you to use a physical PCI device (graphics card, network card) inside a VM (KVM virtualization only).

If you "*PCI passthrough*" a device, the device is not available to the host anymore.

*Note*:
PCI passthrough is an experimental feature in Proxmox VE! VMs with passthroughed devices cannot be *migrated*.

*** Enable the IOMMU
First open your bootloader kernel command line config file.

#+begin_src bash
vim /etc/default/grub
#+end_src

Find line *GRUB_CMDLINE_LINUX_DEFAULT="quiet"*

Change to:

*GRUB_CMDLINE_LINUX_DEFAULT="quiet intel_iommu=on"*

Then save the changes and update grub:

#+begin_src bash
update-grub
#+end_src

and than, reboot your PVE
#+begin_src bash
reboot
#+end_src

Verify IOMMU is enabled

after reboot, then run:
#+begin_src bash
dmesg | grep -e DMAR -e IOMMU
#+end_src

There should be a line that looks like "DMAR: IOMMU enabled". If there is no output, something is wrong.

Add *PT* Mode,
Both Intel and AMD chips can use the additional parameter "iommu=pt", added in the same way as above to the kernel cmdline.
#+begin_src file
GRUB_CMDLINE_LINUX_DEFAULT="quiet intel_iommu=on iommu=pt"
#+end_src

This enables the IOMMU translation only when necessary, the adapter does not need to use DMA translation to the memory, and can thus improve performance for hypervisor PCIe devices (which are not passthroughed to a VM)

than, update grub and root
#+begin_src bash
update-grub
reboot
#+end_src

*** Add required Modules
add to /etc/modules (default is empty)

#+begin_src file
vfio
vfio_iommu_type1
vfio_pci
vfio_virqfd
#+end_src

Then, reboot. Well Done


** DONE Getting Started MikroTik Cloud Hosted Router (CHR) on Proxmox
CLOSED: [2021-12-06 Mon 12:28]
:PROPERTIES:
:EXPORT_FILE_NAME: CHRonProxmox
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction
Cloud Hosted Router (CHR) is a RouterOS version intended for running as a virtual machine. It supports the x86 64-bit architecture and can be used on most of the popular hypervisors such as VMWare, Hyper-V, VirtualBox, KVM and others. CHR has full RouterOS features enabled by default but has a different licensing model than other RouterOS versions.

*** Prerequires
1. read [[id:ffb1b001-6dba-40f3-a222-4260015a6863][Cluster/Getting Started Set-up OVS for Proxmox]]
2. read [[id:0a2b0901-86a5-4a83-b6a7-e9f18516053a][Cluster/Proxmox PCI Passthrough]]

*** System Minimal Requirements
+ Package version: RouterOS v6.34 or newer
+ Host CPU: 64-bit with virtualization support
+ RAM: 128MB or more
+ Disk: 128MB disk space for the CHR virtual hard drive (Max: 16GB)

*NOTE*: The minimum required RAM depends on interface count and CPU count. You can get an approximate number by using the following formula: RAM = 128 + [ 8 × (CPU_COUNT) × (INTERFACE_COUNT - 1) ]

*** The CHR has 4 license levels:

+ free
+ *p1* perpetual-1 ($45)
+ *p10* perpetual-10 ($95)
+ *p-unlimited* perpetual-unlimited ($250)

Perpetual is a lifetime license (buy once, use forever). It is possible to transfer a perpetual license to another CHR instance. A running CHR instance will indicate the time when it has to access the account server to renew it's license. If the CHR instance will not be able to renew the license it will behave as if the trial period has ran out and will not allow an upgrade of RouterOS to a newer version.

After licensing a running trial system, you must manually run the */system license renew* function from the CHR to make it active. Otherwise the system will not know you have licensed it in your account. If you do not do this before the system deadline time, the trial will end and you will have to do a complete fresh CHR installation, request a new trial and then license it with the license you had obtained.

**** Paid licenses
***** p1
p1 (perpetual-1) license level allows CHR to run indefinitely. It is limited to 1Gbps upload per interface. All the rest of the features provided by CHR are available without restrictions. It is possible to upgrade p1 to p10 or p-unlimited After the upgrade is purchased the former license will become available for later use on your account.

***** p10
p10 (perpetual-10) license level allows CHR to run indefinitely. It is limited to 10Gbps upload per interface. All the rest of the features provided by CHR are available without restrictions. It is possible to upgrade p10 to p-unlimited After the upgrade is purchased the former license will become available for later use on your account.

***** p-unlimited
The p-unlimited (perpetual-unlimited) license level allows CHR to run indefinitely. It is the highest tier license and it has no enforced limitations.

***** Free licenses
The free license level allows CHR to run indefinitely. It is limited to 1Mbps upload per interface. All the rest of the features provided by CHR are available without restrictions. To use this, all you have to do is download disk image file from our download page and create a virtual guest.

*** CHR ProxMox installation
**** Step 1: Registration a new mikrotik account, if you have NOT it.
https://mikrotik.com/client

**** Step 2: Installation
I recommand using the below Bash script to install. You need to *ssh* into your ProxMox and run below script.

Before run this script, Please do some research, which version of ROS you want to install. Please, check this [[https://mikrotik.com/download][link]].

What this script does:
+ Stores tmp files in: /root/temp dir.
+ Downloads raw image archive from MikroTik download page.
+ Converts image file to qcow format.
+ Creates basic VM that is attached to MGMT bridge.

*Important Note*:
1. Make sure you have a MGMT bridge, which named *vmbr0*. If you have NOT  avaiable bridge, please have a look [[id:ffb1b001-6dba-40f3-a222-4260015a6863][Cluster/Getting Started Set-up OVS for Proxmox]]
2. If your network card is Intel i211, Please install RouterOS 7, not RouterOS 6. RouterOS 6 does NOT support i211 network card.

#+begin_src bash
#!/bin/bash

#vars
version="nil"
vmID="nil"

echo "############## Start of Script ##############

## Checking if temp dir is available..."
if [ -d /root/temp ]
then
    echo "-- Directory exists!"
else
    echo "-- Creating temp dir!"
    mkdir /root/temp
fi

# apt install unzip
echo "Install unzip"
apt update
apt install unzip -y

# Ask user for version
echo "## Preparing for image download and VM creation!"
read -p "Please input CHR version to deploy ( 6.49.1 (Stable) 6.49rc2 (Testing) 7.1 (Testing)):" version
# Check if image is available and download if needed
if [ -f /root/temp/chr-$version.img ]
then
    echo "-- CHR image is available."
else
    echo "-- Downloading CHR $version image file."
    cd  /root/temp
    echo "---------------------------------------------------------------------------"
    wget https://download.mikrotik.com/routeros/$version/chr-$version.img.zip
    unzip chr-$version.img.zip
    echo "---------------------------------------------------------------------------"
fi
# List already existing VM's and ask for vmID
echo "== Printing list of VM's on this hypervisor!"
qm list
echo ""
read -p "Please Enter free vm ID to use:" vmID
echo ""
# Create storage dir for VM if needed.
if [ -d /var/lib/vz/images/$vmID ]
then
    echo "-- VM Directory exists! Ideally try another vm ID!"
    read -p "Please Enter free vm ID to use:" vmID
else
    echo "-- Creating VM image dir!"
    mkdir /var/lib/vz/images/$vmID
fi

# Resize image
qemu-img resize /root/temp/chr-$version.img +10G

# Creating VM
echo "-- Creating new CHR VM"
qm create $vmID \
  --name chr-$version \
  --net0 virtio,bridge=vmbr0 \
  --bootdisk virtio0 \
  --ostype l26 \
  --memory 256 \
  --onboot no \
  --sockets 1 \
  --cores 1 \
  --virtio0 local-lvm:vm-$vmID-disk-0

# import image
echo "-- Import RAW image to local-lvm"
qm importdisk $vmID /root/temp/chr-$version.img local-lvm

# remove downloaded raw image and zip
rm /root/temp/chr-$version.img.zip
rm /root/temp/chr-$version.img

echo "############## End of Script ##############"
#+end_src

*NOTE*: ERROR: storage 'local' does not support content-type 'images'
*NOTE*: Useful snippet to clean up the BASH script from Windows formatting that may interfere with script if it's edited on a Windows workstation:

#+begin_src console
sed -i -e 's/\r$//' *.sh
#+end_src

**** Step 3: Add WAN port to ROS
I am add a passthrough NIC as WAM, so before you read this section. Please, read [[id:0a2b0901-86a5-4a83-b6a7-e9f18516053a][Cluster/Proxmox PCI Passthrough]] first.

When you *DONE* set-up passthrough, now lets we list network interfaces name with PCI ID and add WAN.

***** List network interface name with PCI ID

#+begin_src bash
apt install lshw
lshw -class network
#+end_src

For example, you can found interface name with bus id at below.
#+begin_src file
*-network
       description: Ethernet interface
       product: I211 Gigabit Network Connection
       vendor: Intel Corporation
       physical id: 0
       bus info: pci@0000:06:00.0
       logical name: enp6s0
       version: 03
       serial: 00:90:27:e5:8d:09
       capacity: 1Gbit/s
       width: 32 bits
       clock: 33MHz
       capabilities: pm msi msix pciexpress bus_master cap_list ethernet physical tp 10bt 10bt-fd 100bt 100bt-fd 1000bt-fd autonegotiation
       configuration: autonegotiation=on broadcast=yes driver=igb driverversion=5.13.19-2-pve firmware=0. 6-1 latency=0 link=no multicast=yes port=twisted pair
       resources: irq:17 memory:df000000-df01ffff ioport:9000(size=32) memory:df020000-df023fff
#+end_src

***** Add WAN
now lets we add WAN to ROS.

Go to *Hardware* Section -> *Add* -> *PCI Device*

Choose your WAN need to add in.

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1638780489/PVE/hardwareAdd_aqh8vq.png]]


**** Step 4: Start your ROS
check interface name:
#+begin_src bash
interface print
#+end_src

check IP address:
#+begin_src bash
ip export
#+end_src

remove IP address
#+begin_src bash
ip address remove IDnum
#+end_src

Assign IP address:
#+begin_src bash
ip address add address=192.168.1.253/24 interface=ether2
#+end_src
**** Step 5: Disable API, API-SSL, Telnet, FTP, WWW and WWW-SSL
#+begin_src bash
ip service disable api,api-ssl,ftp,ssh,telnet,www,www-ssl
#+end_src

**** Step 6: interface rename
#+begin_src bash
interface set ether2 name="LAN"
interface set ether1 name="WAN"
#+end_src


**** Step 7: add dhcp client
#+begin_src bash
ip dhcp-client print detail

ip dhcp-client set interface=WAN disable=no use-peer-dns=no

ip dhcp-client print detail
#+end_src

**** Step 8: add DNS
https://wiki.mikrotik.com/wiki/Manual:IP/DNS

#+begin_src bash
ip dns set servers=192.168.1.252 max-udp-packet-size=8192
#+end_src

#+begin_src bash
ip dns static add name=ros address=192.168.1.253
#+end_src
**** Step 9: firwall NAT
#+begin_src bash
ip firewall nat add chain=srcnat action=masquerade
#+end_src

**** Step 10: IP Pool
https://wiki.mikrotik.com/wiki/Manual:IP/Pools
#+begin_src bash
ip pool add name=ip-pool ranges=192.168.1.100-192.168.1.200
#+end_src

**** Step 11: DHCP Server
#+begin_src bash
ip dhcp-server add name=LANDHCP interface=LAN address-pool=ip-p
ool
#+end_src

#+begin_src bash
ip dhcp-server network add address=192.168.1.0/24 gateway=192.1
68.1.252
#+end_src


** DONE Getting Started ROS and Openwrt with Proxmox
CLOSED: [2021-11-05 Fri 19:52]
:PROPERTIES:
:EXPORT_FILE_NAME: ROSandOpenwrtProxmox
:EXPORT_OPTIONS: author:nil
:END:

**
*** mikrotik
https://mikrotik.com/software


*** OpenWrt
https://openwrt.org/downloads
https://downloads.openwrt.org/releases/21.02.1/targets/x86/
https://downloads.openwrt.org/releases/21.02.1/targets/x86/64/
*** Migration of servers to Proxmox VE
https://pve.proxmox.com/wiki/Migration_of_servers_to_Proxmox_VE
*** VLAN
https://engineerworkshop.com/blog/configuring-vlans-on-proxmox-an-introductory-guide/


** DONE Getting Started Configuring VLANs on Proxmox
CLOSED: [2021-11-05 Fri 19:52]
:PROPERTIES:
:EXPORT_FILE_NAME: vlan
:EXPORT_OPTIONS: author:nil
:END:
*** Introduction
A virtualization server allows you to run multiple machines, virtual machines (VMs), on one physical device, also known as the host. There could be many different VMs each for different tasks. In this guide, we will discuss configuring your Proxmox virtualization server to use VLANs so that you can group related VMs onto their own subnet.
*** Motivation
For security, as well as organizational purposes, physical machines are often separated on the network from each other by VLANs. By logically separating devices based on their functionality with these VLANs, we can make sure that our family's personal devices aren't sitting out in the open on the same subnet exposed to our internet-facing web servers. This is fairly easy on your regular network setup because the devices are physically separate from each other and so each ethernet port physically connected to a device can be assigned an individual VLAN.

However, this system starts to break down when faced with virtualization servers. This is because diverse virtual machines are all sitting on the same physical host, forcing each VM to share the same physical connection. With a standard bridge between the individual VM and the host's NIC, we necessarily end up with each VM on the same subnet as the Proxmox host itself. Additionally, we end up with each VM on the same subnet as every other VM on that host. Not ideal.
*** Solution
Thankfully there's a way around this. In Proxmox, you can make your virtual bridge VLAN-aware so you can pass multiple VLANs through to your Proxmox server using only a single physical port. The individual VMs can then be configured to use whichever VLAN you choose.


** DONE Create Proxmox cloud-init template
CLOSED: [2021-12-03 Fri 18:27]
:PROPERTIES:
:EXPORT_FILE_NAME: clouldInit
:EXPORT_OPTIONS: author:nil
:ID:       632ee4dd-5bdd-4103-bab1-b3c1110aeac6
:END:
*** Overview
In this article, I'll demonstrate how to create a cloud-init enabled Ubuntu 20.04 LTS base image to use on Proxmox VE.

*** Cloud Native Image
The tradition packer builder to build a base image from an ISO file. Modern Linux distributions are increasingly moving away from this install method and preseed files. Rather, disk images are provided with the OS pre-installed, and configuration is performed via cloud-init. We will create a Proxmox KVM base image using Ubuntu's KVM cloud image.

*** Proxmox Script
The Proxmox API doesn't appear to offer the full functionality provided by the native shell commands to create a template, so we will run a script via SSH or Proxmox node's GUI shell.

The Script you can found on my GitHub. There is the Link.

The below sections, I will explain this Script step by steps.

*** Step 1: Download the image
We are downloading the kvm disk image.

*Note*: This is a qcow2 image format with an extension of .img, Promxox doesn't like this so we rename the disk image to .qcow2

#+begin_src bash
SRC_IMG="https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64-disk-kvm.img"
IMG_NAME="focal-server-cloudimg-amd64-disk-kvm.qcow2"
wget -O $IMG_NAME $SRC_IMG
#+end_src

*** Step 2: Add QEMU Guest Agent
The Ubuntu 20.04 image we are going to use does not include the *qemu-guest-agent* package which is needed for the *Guest VM* to report its IP details back to Proxmox. This is required for Packer to communicate with the VM after cloning. The template. *libguestfs-tools* will allow us to embed qemu-guest-agent into the image. You can also add any additional packages you'd like in your base image. Personally, I prefer to customize this base image later with packer so that the packages can live in source control.

#+begin_src bash
apt update
apt install -y libguestfs-tools
virt-customize --install qemu-guest-agent -a $IMG_NAME
#+end_src

*** Step 3: Create a VM in Proxmox with required settings and convert to template
For best performance, virtio "hardware" should be used. Additionally, cloud-init requires a serial console and cloudinit IDE (CDROM) drive. We will set the network config to DHCP so that we get an IP address. Lastly, we will expand the template disk image size so we have space to install items later. It appears packer doesn't support doing this later.

#+begin_src bash
TEMPL_NAME="ubuntu2004-cloud"
VMID="9000"
MEM="512"
DISK_SIZE="32G"
DISK_STOR="local-lvm"
NET_BRIDGE="vmbr0"
qm create $VMID --name $TEMPL_NAME --memory $MEM --net0 virtio,bridge=$NET_BRIDGE
qm importdisk $VMID $IMG_NAME $DISK_STOR
qm set $VMID --scsihw virtio-scsi-pci --scsi0 $DISK_STOR:vm-$VMID-disk-0
qm set $VMID --ide2 $DISK_STOR:cloudinit
qm set $VMID --boot c --bootdisk scsi0
qm set $VMID --serial0 socket --vga serial0
qm set $VMID --ipconfig0 ip=dhcp
qm resize $VMID scsi0 $DISK_SIZE
qm template $VMID
# Remove downloaded image
rm $IMG_NAME
#+end_src

*** Step 4: Packer template
Now that we have our cloud-init enabled image on Proxmox, we can use Packer to create a template based off of this template.
Ensure to set the scsi_controller="virtio-scsi-pci" and qemu_agent=true.

I'd recommend adding the Proxmox variables to a var file.

#+begin_src bash
packer build --var-file=./proxmox.pkvars.hcl --var "proxox_template_name=test-output-template" --var "proxmox_source_template=ubuntu2004-cloud" base.pkr.hcl
#+end_src

*** Final
Now that you've created a template using packer from the base template, you can use Terraform to deploy that VM!

*** References
1. https://gist.github.com/chriswayg/43fbea910e024cbe608d7dcb12cb8466
2. https://whattheserver.com/proxmox-cloud-init-os-template-creation/
3. https://norocketscience.at/deploy-proxmox-virtual-machines-using-cloud-init/
4. https://pve.proxmox.com/wiki/Cloud-Init_Support
5. https://blog.dustinrue.com/2020/05/going-deeper-with-proxmox-cloud-init/
6. https://gist.github.com/mike1237/cce83a74f898b11c2cec911204568cf9



** DONE Build a Kubernetes cluster on Proxmox via Ansible and Terraform
CLOSED: [2021-12-01 Wed 20:27]
:PROPERTIES:
:EXPORT_FILE_NAME: k8sOnProxmox
:EXPORT_OPTIONS: author:nil
:END:

[[https://miro.medium.com/max/1400/1*jL6SE1nSaPQb4EOWGnbZpw.jpeg]]


*** Overview
Proxmox is an open-source hypervisor that have enterprise capabilities and a large community behind it.

For Terraform and Ansible, i always like the idea of infrastructure as code (iac) and Terraform and Ansible just make it easy to accomplish.

The idea here was to be able to spin up a k3s cluster with minimum effort so i can spin it up and down for ever project that i would like to run.
*** Prerequires
1. read [[id:ee79f2a9-445b-4756-9853-e0819fda588c][DevOps/Terraform Beginner's Guide]]
2. read [[id:5012520e-c7d0-4b8e-8575-6ecf70e819b6][DevOps/Ansible Beginner's Guide]]
3. read [[id:632ee4dd-5bdd-4103-bab1-b3c1110aeac6][Cluster/Create Proxmox cloud-init template]]
*** System requirements
+ The deployment environment must have [[https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html][Ansible]] 2.4.0+
+ [[https://learn.hashicorp.com/tutorials/terraform/install-cli][Terraform]] installed
+ [[https://www.proxmox.com/en/proxmox-ve][Proxmox]] server

*** Proxmox setup


*** References
1. https://medium.com/@ssnetanel/build-a-kubernetes-cluster-using-k3s-on-proxmox-via-ansible-and-terraform-c97c7974d4a5



* DevOps :@DevOps:
** DONE DevOps Beginner's Guide :DevOps:
CLOSED: [2021-12-02 Thu 11:18]
:PROPERTIES:
:EXPORT_FILE_NAME: DevOps
:EXPORT_OPTIONS: author:nil
:ID:       807c90ce-9dfd-44be-861b-b2893282ed5f
:END:

*** Overview
In this blog, I discussed *what is DevOps*, and why it has *gained* so much *traction* in the IT industry lately.

*** What Is DevOps?
It is a combination of practices that *streamline* the *automation* and *integration* of processes between the *software development* and *IT teams*. This will help them to *build*, *test*, and *release software* in a faster and more reliable way.

**** Purpose
The term was formed by combining the words *"development"* and *"operations"* and signifies a cultural shift that *helps bridge the gap between the development and operation teams*.
**** Goal
 The *goal* of DevOps is to change and improve the relationship by advocating better communication and collaboration between these two business units.

*** DevOps Model For Teams
Teams using the DevOps model are able to evolve and improve their products at a higher rate over the organizations that use traditional processes. *Collaboration*, *Communication*, and *Integration* are the key elements of incorporating DevOps into any development and delivery setting.

This speed enables the teams (and in turn their organizations) to better serve their customers and compete more effectively in the market.

[[https://d1.awsstatic.com/product-marketing/DevOps/DevOps_feedback-diagram.ff668bfc299abada00b2dcbdc9ce2389bd3dce3f.png]]

*** DevOps Advantages
Improvement of collaboration between all stakeholders from planning through delivery and automation of the delivery process in order to:

+ Increase deployment frequency
+ Achieve faster time to market
+ Decrease the failure rate of new releases
+ Shorten the lead time between fixes
+ Improve mean time to recovery

According to the State of DevOps Report, "high-performing IT organizations deploy 30x more frequently with 200x shorter lead times; they have 60x fewer failures and recover 168x faster."

*** DevOps Principles
The phrase “The Three Ways” is used to describe the underlying principles of the DevOps movement.

**** The First Way: Principles of Flow
The First Way states the following, about the flow of work:

+ Work should only flow in one direction
+ No known defect should be passed downstream
+ Always seek to increase the flow

**** The Second Way: Principles of Feedback
The Second Way describes the feedback process as the following:

+ Establish an upstream feedback loop
+ Shorten the feedback loop
+ Amplify the feedback loop
  [[https://blog-assets.freshworks.com/freshservice/wp-content/uploads/2019/01/23142830/2.png]]

**** The Third Way: Principles of Continuous Learning
The Third Way describes the environment and culture, as the following practices

+ Promote experimentation
+ Learn from success and failure
+ Constant improvement
+ Seek to achieve mastery through practice

[[https://blog-assets.freshworks.com/freshservice/wp-content/uploads/2019/01/23142856/3.png]]


** DONE Terraform Beginner's Guide :Terraform:
CLOSED: [2021-12-02 Thu 11:09]
:PROPERTIES:
:EXPORT_FILE_NAME: terraform
:EXPORT_OPTIONS: author:nil
:ID:       ee79f2a9-445b-4756-9853-e0819fda588c
:END:
*** Overview
In this blog post, I am going to cover a brief introduction of *Infrastructure as Code (IaC)*, *Terraform*, its *lifecycle*, and all the core concepts that every beginner should know. I have tried to cover all the topics in this beginner’s guide that will give you a quick start for using Terraform.

*** Prerequires
1.[[id:807c90ce-9dfd-44be-861b-b2893282ed5f][DevOps/DevOps Beginner's Guide]]

*** What Is Infrastructure as Code (IaC)?
*Infrastructure as Code (IaC)* is a widespread terminology among DevOps professionals and a key DevOps practice in the industry. It is the process of managing and provisioning the complete IT infrastructure (comprises both physical and virtual machines) using machine-readable definition files. It helps in automating the complete data center by using programming scripts.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/Explanation-of-how-IaC-works.jpg]]

*** Popular IaC Tools:
1. *Terraform*: An open-source declarative tool that offers pre-written modules to build and manage an infrastructure.
2. *Chef*: A configuration management tool that uses cookbooks and recipes to deploy the desired environment. Best used for Deploying and configuring applications using a pull-based approach.
3. *Puppet*: Popular tool for configuration management that follows a Client-Server Model. Puppet needs agents to be deployed on the target machines before the puppet can start managing them.
4. *Ansible*: Ansible is used for building infrastructure as well as deploying and configuring applications on top of them. Best used for Ad hoc analysis.
5. *Packer*: Unique tool that generates VM images (not running VMs) based on steps you provide. Best used for Baking compute images.
6. *Vagrant*: Builds VMs using a workflow. Best used for Creating pre-configured developer VMs within VirtualBox.

*** What Is Terraform?
*Terraform* is one of the most popular *Infrastructure-as-code (IaC) tool*, used by DevOps teams to automate infrastructure tasks. It is used to automate the provisioning of your cloud resources. Terraform is an open-source, cloud-agnostic provisioning tool developed by HashiCorp and written in GO language.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/logo-hashicorp-e1605707253653.png]]

Benefits of using Terraform:

+ Does orchestration, not just configuration management
+ Supports multiple providers such as AWS, Azure, Oracle, GCP, and many more
+ Provide immutable infrastructure where configuration changes smoothly
+ Uses easy to understand language, HCL (HashiCorp configuration language)
+ Easily portable to any other provider

*** Terraform Lifecycle
Terraform lifecycle consists of – *init*, *plan*, *apply*, and *destroy*.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/terraform-lifecycle.png]]

1. *Terraform init* initializes the (local) Terraform environment. Usually executed only once per session.
2. *Terraform plan* compares the Terraform state with the as-is state in the cloud, build and display an execution plan. This does not change the deployment (read-only).
3. *Terraform apply* executes the plan. This potentially changes the deployment.
4. *Terraform destroy* deletes all resources that are governed by this specific terraform environment.

*** Terraform Core Concepts
1. *Variables*: Terraform has input and output variables, it is a key-value pair. Input variables are used as parameters to input values at run time to customize our deployments. Output variables are return values of a terraform module that can be used by other configurations.

   Please, read article on [[https://k21academy.com/terraform-iac/variables-in-terraform/][Terraform Variables]]

2. *Provider*: Terraform users provision their infrastructure on the major cloud providers such as AWS, Azure, OCI, and others. A provider is a plugin that interacts with the various APIs required to create, update, and delete various resources.

   Please, read article to know more about [[https://k21academy.com/terraform-iac/terraform-providers-overview/][Terraform Providers]]

3. *Module*: Any set of Terraform configuration files in a folder is a module. Every Terraform configuration has at least one module, known as its *root module*.

4. *State*: Terraform records information about what infrastructure is created in a Terraform state file. With the state file, Terraform is able to find the resources it created previously, supposed to manage and update them accordingly.

5. *Resources*: Cloud Providers provides various services in their offerings, they are referenced as Resources in Terraform. Terraform resources can be anything from compute instances, virtual networks to higher-level components such as DNS records. Each resource has its own attributes to define that resource.

6. *Data Source*: Data source performs a read-only operation. It allows data to be fetched or computed from resources/entities that are not defined or managed by Terraform or the current Terraform configuration.

7. *Plan*: It is one of the stages in the Terraform lifecycle where it determines what needs to be created, updated, or destroyed to move from the real/current state of the infrastructure to the desired state.

8. *Apply*: It is one of the stages in the Terraform lifecycle where it applies the changes real/current state of the infrastructure in order to achieve the desired state.

Check Out: Our previous blog post on [[https://k21academy.com/terraform-iac/terraform-cheat-sheet/][Terraform Cheat Sheet]].

*** Terraform Installation
Terraform Installation
Before you start working, make sure you have Terraform installed on your machine, it can be installed on any OS, say Windows, macOS, Linux, or others. Terraform installation is an easy process and can be done in a few minutes.

Read our blog to know how to [[https://k21academy.com/terraform-iac/terraform-installation-overview/][install Terraform]] in Linux, Mac, Windows

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/Terraform-installation.jpg]]

*** Terraform Providers
A provider is responsible for understanding API interactions and exposing resources. It is an executable plug-in that contains code necessary to interact with the API of the service. Terraform configurations must declare which providers they require so that Terraform can install and use them.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/Terraform-provider-api-call.png]]

Terraform has over a hundred providers for different technologies, and each provider then gives terraform user access to its resources. So through AWS provider, for example, you have access to hundreds of AWS resources like EC2 instances, the AWS users, etc.

*** Terraform Configuration Files
Configuration files are a set of files used to describe infrastructure in Terraform and have the file extensions .tf and .tf.json. Terraform uses a declarative model for defining infrastructure. Configuration files let you write a configuration that declares your desired state. Configuration files are made up of resources with settings and values representing the desired state of your infrastructure.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/terraform-config-files-e1605834689106.png]]

A Terraform configuration is made up of one or more files in a directory, provider binaries, plan files, and state files once Terraform has run the configuration.

1. *Configuration file (*.tf files)*: Here we declare the provider and resources to be deployed along with the type of resource and all resources specific settings

2. *Variable declaration file (variables.tf or variables.tf.json)*: Here we declare the input variables required to provision resources

3. *Variable definition files (terraform.tfvars)*: Here we assign values to the input variables

4. *State file (terraform.tfstate)*: a state file is created once after Terraform is run. It stores state about our managed infrastructure.

*** Getting started using Terraform
To get started building infrastructure resources using Terraform, there are few things that you should take care of. The general steps to deploy a resource(s) in the cloud are:

1. Set up a Cloud Account on any cloud provider (AWS, Azure, OCI)
2. Install Terraform
3. Add a provider – AWS, Azure, OCI, GCP, or others
4. Write configuration files
5. Initialize Terraform Providers
6. PLAN (DRY RUN) using terraform plan
7. APPLY (Create a Resource) using terraform apply
8. DESTROY (Delete a Resource) using terraform destroy

*** Import Existing Infrastructure
Terraform is one of the great IaC tools with which, you can deploy all your infrastructure’s resources. In addition to that, you can manage infrastructures from different cloud providers, such as AWS, Google Cloud, etc. But what if you have already created your infrastructure manually?

Terraform has a really nice feature for importing existing resources, which makes the migration of existing infrastructure into Terraform a lot easier.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/terraform-import-workflow-diagram.png]]

Currently, Terraform can only import resources into the state. It does not generate a configuration for them. Because of this, prior to running terraform import it is necessary to write manually a resource configuration block for the resource, to which the imported object will be mapped. For example:

#+begin_src file
resource "aws_instance" "import_example" {
  # ...instance configuration...
}
#+end_src

Now terraform import can be run to attach an existing instance to this resource configuration:

#+begin_src console
$ terraform import aws_instance.import_example i-03efafa258104165f
#+end_src

This command locates the AWS instance with ID i-03efafa258104165f (which has been created outside Terraform) and attaches it to the name aws_instance.import_example in the Terraform state.

*** Conclusion
I hope the above gives you an idea about how you can get started with Terraform.

*** Related/References


** DONE Ansible Beginner's Guide :Ansible:
CLOSED: [2021-12-02 Thu 17:58]
:PROPERTIES:
:EXPORT_FILE_NAME: ansible
:EXPORT_OPTIONS: author:nil
:ID:       5012520e-c7d0-4b8e-8575-6ecf70e819b6
:END:
*** Overview
This article covers all the aspects of Ansible, a tool used in DevOps for the Management, Deployment, and Orchestration of IT Infrastructure.

*** What Is Ansible?
Ansible is a simple configuration management and IT automation engine for multi-tier deployments. It automates both cloud and on-premise provisioning & configuration. It automates cloud provisioning. Rather than managing one system at a time, Ansible uses a model that inter-relates the entire IT infrastructure and enables you to manage everything using something called an Infrastructure as Code (IAC) approach. Ansible is secure and agentless. It relies on OpenSSH and the code written in YAML format. Ansible nodes are run on Unix systems but they can be used to configure changes across Unix as well as Windows systems.

*** Who should learn Ansible?
Ansible is a part of the DevOps stack. Ansible means automation. Ansible seamlessly connects workflow orchestration with configuration management and provisioning deployment. Ansible has various use cases in Provisioning, Configuration Management, Application Deployment, Continuous Deployment, Automation, and Orchestration. So, if you are looking forward to a career in DevOps, IT automation, and managing cloud infrastructure then Ansible is a must-have.

*** Why Use Ansible?
+ *No Agent*: As long as the box can be ssh’d into and it has python, it can be configured with Ansible.
+ *Idempotent*: Ansible’s whole architecture is structured around the concept of idempotency. The core idea here is that you only do things if they are needed and that things are repeatable without side effects.
+ *Declarative Not Procedural*: Other configuration tools tend to be procedural do this and then do that and so on. Ansible works by you writing a description of the state of the machine that you want and then it takes steps to fulfill that description.
+ *Tiny Learning Curve*: Ansible is quite easy to learn. It doesn’t require any extra knowledge.

*** Ansible Use Cases
+ *Provisioning*: Provisioning is creating new infrastructure. Ansible allows for application management, deployment, orchestration, and configuration management.
+ *Continuous Delivery*: Ansible provides a simpler way to automatically deploy applications. All required services for a deployment can be configured from a single system. Continuous Integration (CI) tool can be used to run Ansible playbook which can be used to test and automatically deploy the application to production if tests are passed.
+ *Application Deployment*: Ansible provides a simpler way to deploy applications across the infrastructure. Deployment of multi-tier applications can be simplified and the infrastructure can be easily changed over time.
+ *Ansible for Cloud Computing*: Ansible makes it easy to provision instances across all cloud providers. Ansible contains multiple modules and allows to manage of large cloud infrastructure across the public-private and hybrid cloud.
+ *Ansible for Security and Compliance*: You can define security policies in Ansible which will automate security policy across all machines in the network. Security roles once configured in an Ansible node will be embedded across all machines in the network automatically.

*** Ansible Architecture Diagram
[[https://miro.medium.com/max/564/1*eaY6sN1T9VJiVOrMQMNdRQ.png]]
[[https://miro.medium.com/max/625/0*K9Kqdh4ZLT-fHJeP.png]]
[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/06/Ansible_Diagram2-16-1024x461.png]]

+ *Modules*: Modules are script-like programs written to specify the desired state of the system. These are typically written in a code editor. Modules are written by the developer and executed via SSH. Modules are part of a larger program called Playbook. Ansible module is a standalone script that can be used inside an Ansible Playbook.
+ *Plugins*: Plugins are pieces of code that enhance the core functionality of Ansible. Plugins execute on the control node.
+ *Inventory*: Ansible reads information about the machines you manage from the inventory. Inventory is listed in the file which contains IP addresses, databases, and servers.
+ *Playbook*: Playbooks are files written in YAML. Playbooks describe the tasks to be done by declaring configurations in order to bring a managed node into the desired state.

*** Ansible Playbook
+ Plain-text YAML files that describe the desired state of something
+ Human and Machine-readable
+ Can be used to build the entire application environment

[[https://miro.medium.com/max/463/0*t2iCHi_buMmtKGmw]]

*** What Are Inventories In Ansible?
+ Static lines of servers
+ Dynamic list of servers: AWS, Azure, GCP, etc.
+ Ranges
+ Other custom things

  [[https://miro.medium.com/max/201/1*mLdHcg8SvBvXRDceZIdKeA.png]]
  [[https://miro.medium.com/max/1006/0*E_bhUEFXGoQCOV_K.jpg]]

*** Ansible Modules
+ Over 1000 modules provided by Ansible to automate
+ Modules are like plugins that do the actual work in Ansible, they are what gets executed in each playbook task.
+ Each module is mostly standalone and can be written in a standard scripting language (such as Python, Perl, Ruby, Bash, etc.)

[[https://miro.medium.com/max/793/1*UDC-1_SR4Z26APTYRWDP3w.png]]

*** Ansible Tower
Ansible Tower is a GUI and REST interface for Ansible that supercharges it by adding RBAC, centralized logging, auto-scaling/provisioning call-backs, graphical inventory editing, and more.

*Capabilities*:

This command-line tool sends commands to the Tower API. It is capable of retrieving, creating, modifying, and deleting most resources within the Tower.

+ A few potential uses include:
+ Launching playbook runs (for instance, from Jenkins, TeamCity, Bamboo, etc.)
+ Checking on job statuses
+ Rapidly creating objects like organizations, users, teams, and more.

[[https://www.ansible.com/products/tower]]

*** Ansible Roles
1. Roles are a way to group tasks together into one container. We could have a role for setting up MySQL, another one for configuring ip tables.
2. Roles make it easy to configure hosts. Any role can be performed on any host or group of hosts such as:
    + hosts: all
    + roles:
    + role_1
    + role_2

*** Ansible Variables
There are many different ways to source variables:
+ Playbooks
+ Files
+ Inventories (group vars, host vars)
+ Command-line Discovered Variables
+ Ansible Tower

*** How To Run The Ansible Commands?
*Ad-Hoc*: Ansible <inventory> -m
[[https://miro.medium.com/max/520/1*W8ndyJq6S37tdAPBEHvUbQ.png]]
*Playbooks*: Ansible-playbook
[[https://miro.medium.com/max/519/1*SfmrmCzzcKmf4GO7TApVmg.png]]

*** AD-HOC Commands Examples
*Transferring file to many servers/machines*

#+begin_src console
$ Ansible Abc -m copy -a "src = /etc/yum.conf dest = /tmp/yum.conf"
#+end_src

*Creating a new directory*

#+begin_src console
$ Ansible ABC -m file -a "dest = /path/user1/new mode = 777 owner = user1 group = user1 state = directory"
#+end_src

*Deleting whole directory and files*

#+begin_src console
$ Ansible ABC -m file -a "dest = /path/user1/new state = absent"
#+end_src


** DONE The comparison and introduction between Terraform and Ansible :Terraform:Ansible:
CLOSED: [2021-12-02 Thu 10:53]
:PROPERTIES:
:EXPORT_FILE_NAME: terraformAndAnsible
:EXPORT_OPTIONS: author:nil
:END:
*** Overview
The Terraform vs. Ansible battle continues to escalate as the DevOps environment focuses more on automation and orchestration. These two tools help in automating configurations and deploying infrastructure. Terraform offers to deploy Infrastructure as a Code, helps in readability and lift and shift deployments. Ansible is a configuration management tool for automating system configuration and management.

*** Terraform

**** What is Terraform?

Terraform is an open-source tool for building, changing, and versioning infrastructure securely and effectively. It is an Infrastructure as a Code tool that is very straightforward to use. It helps to develop and scale Cloud services and manage the state of the network. Its primary use is in data centers and software-defined networking environments. It does not install and manage software on existing devices; instead, it creates, modifies, and destroys servers and various other cloud services. Slack, Uber, Starbucks, Twitch, all big brands are using Terraform. We can also integrate Terraform with Microsoft Azure, Heroku, and Google Compute Engine, etc.

Now, we will discuss the working of terraform.

**** How does Terraform work?
There are two main working components of terraform.

+ Terraform Core
+ Providers

Terraform is of *declarative nature*. It directly describes the end state of the system without defining the steps to reach there. It works at a high level of abstraction to describe what services and resources should be created and defined.

Terraform core takes two input sources to do its job. The first input source is a *terraform configuration* that is configured by its users. Users define what needs to be provisioned and created. The second input source is a state that holds information about the infrastructure.

So terraform core takes the input and figures out various plans for what steps to follow to get the desired output.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/06/TerraformCore_Diagram-08-1024x421.png]]

The second principal component is providers, such as cloud providers like AWS, GCP, Azure, or other Infrastructure as service platforms. It helps to create infrastructure on different levels. Let’s take an example where users create an AWS infrastructure, deploy Kubernetes on top of it, and then create services inside the cluster of Kubernetes. Terraform has multiple providers for various technologies; users can access resources from these providers through terraform. This is the basic working terminology of terraform that helps to provision and cover the complete application set up from infrastructure to fully developed application.

**** Features of Terraform
As we have discussed the working of Terraform, now we will look at the features of Terraform.

+ Terraform follows a *declarative approach* which makes deployments fast and easy.
+ It is a convenient tool to display the resulting model in a *graphical form*.
+ Terraform also manages *external service providers* such as cloud networks and in-house solutions.
+ It is one of the rare tools to offer *building infrastructure* from scratch, whether public, private or multi-cloud.
+ It helps *manage parallel environments*, making it a good choice for testing, validating bug fixes, and formal acceptance.
+ Modular code helps in achieving *consistency*, *reusability*, and *collaboration*.
+ Terraform can *manage multiple clouds* to increase fault tolerance.


*** Ansible

**** What is Ansible?
Ansible is the most significant way to automate and configure apps and IT infrastructure.  Ansible is an *open-source configuration management tool* mainly designed for provisioning and deploying applications using IaaC.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/06/Ansible-Official-Logo-Black-299x300.png]]

It has its own language to describe system configuration. Ansible is *agentless*, making it manage large deployments across enterprises using Windows Power Shell or SSH to perform its tasks. Ansible is not completely declarative; it is a hybrid of procedural and declarative. It can integrate with Amazon EC2, Docker, and Kubernetes. Companies like Zalando, Revolt, and 9gaga are using Ansible.

**** How does Ansible work?
Ansible is agentless and doesn’t run on target nodes. It makes *connections using SSH* or other authentication methods. It installs various *Python modules* on the target using JSON. These modules are simple instructions that run on the target. These modules are executed and removed once their job is done. This strategy ensures that there is no misuse of resources on target. Python is mandatory to be installed on both the controlling and the target nodes.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/06/Ansible_Diagram-07-1024x564.png]]

Ansible *management node* acts as a controlling node that controls the entire execution of the playbook. This node is the place to run the installations. There is an *inventory file* that provides the host list where the modules need to be run. The management node makes SSH connections to execute the modules on the host machine and installs the product. Modules are removed once they are installed in the system. This is the simple working process of Ansible.

**** Features of Ansible

Now we will discuss various features Ansible provides to benefit its users.

+ Ansible is used for *configuration management* and follows a procedural approach.
+ Ansible deals with *infrastructure platforms* such as bare metal, cloud networks, and virtualized devices like hypervisors.
+ Ansible follows *idempotent behavior* that makes it to place node in the same state every time.
+ It uses *Infrastructure as a Code system configuration* across the infrastructure.
+ It offers *rapid and easy deployment* of multi-tier apps with being agentless.
+ If the code is *interrupted*, it allows entering the code again without any conflicts with other invocations.

*** Difference between Terraform and Ansible Provisioning
Let’s see how Terraform vs. Ansible battle differentiates from each other:

| Terraform                                                                                        | Ansible                                                                                                     |
|--------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------|
| Terraform is a provisioning tool.                                                                | Ansible is a configuration management tool.                                                                 |
| It follows a declarative Infrastructure as a Code approach.                                      | It follows a procedural approach.                                                                           |
| It is the best fit for orchestrating cloud services and setup cloud infrastructure from scratch. | It is mainly used for configuring servers with the right software and updates already configured resources. |
| Terraform does not support bare metal provisioning by default.                                   | Ansible supports the provisioning of bare metal servers.                                                    |
| It does not provide better support in terms of packaging and templating.                         | It provides full support for packaging and templating.                                                      |
| It highly depends on lifecycle or state management.                                              | It does not have lifecycle management at all.                                                               |

*** Configuration Management vs. Orchestration
Terraform and Ansible have so many similarities and differences at the same time. The difference comes when we look at two significant concepts of DevOps: *Orchestration* and *configuration management*.

*Configuration management* tools solve the issues locally rather than replacing the system entirely. Ansible helps to configure each action and instrument and ensures smooth functioning without any damage or error. In addition, Ansible comes up with hybrid capabilities to perform both orchestration and replace infrastructure.

*Orchestration tools* ensure that an environment is in its desired state continuously. Terraform is explicitly designed to store the state of the domain. Whenever there is any glitch in the system, terraform automatically restores and computes the entire process in the system after reloading. It is the best fit in situations where a constant and invariable state is needed. *Terraform Apply* helps to resolve all anomalies effectively.

Let’s have a look at the *Procedural* and *Declarative* nature of Terraform and Ansible.

*** Procedural vs Declarative
There are two main categories of DevOps tools: *Procedural* vs. *Declarative*. These two categories tell the action of tools.

Terraform follows the *declarative approach*, ensuring that if your defined environment suffers changes, it rectifies those changes. This tool attempts to reach the desired end state described by the sysadmin. Puppet also follows the declarative approach. With terraform, we can describe the desired state and figure out how to move from one state to the next automatically.

Ansible is of hybrid nature. It follows both declarative and *procedural style* configuration.  It performs ad-hoc commands to implement procedural-style configurations. Please read the documentation of Ansible very carefully to get in-depth knowledge of its behavior. It’s important to know whether you need to add or subtract resources to get the desired result or need to indicate the resources required explicitly.

*** Terraform vs Ansible Provisioning
[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/07/TerraformVsAnsible-400x224.png]]

*Terraform* deals with *infrastructure automation*. Its current declarative model lacks some features which arise complexity. Using Terraform, the elements of required environments are separately described, including their relationships. It assesses the model, creates a plan based on dependencies, and gives optimized commands to Infrastructure as a Service. If there is no change in the environment or strategy, repeated runs will do nothing. If there is any update in the plan or environment, it will *synchronize* the cloud infrastructure.

*Ansible* follows a *procedural approach*. Various users create playbooks that are evaluated through top to bottom approach and executed in sequence. *Playbooks* are responsible for the configuration of network devices that contributes towards a procedural approach.  Of course, Ansible provisions the cloud infrastructure as well. But its procedural approach limits it to large infrastructure deployments.

*** Which one to choose: Terraform or Ansible?
*Terraform* vs. *Ansible*: Every tool has its unique characteristics and limitations. Let’s check out which one to go with.

*Terraform* comes with good *scheduling capabilities* and is very *user-friendly*. It integrates with docker well, as docker handles the configuration management slightly better than Terraform. But there is no clear evidence of how the target devices are brought to their final state, and sometimes, the final configuration is unnecessary.

*Ansible* comes with better *security* and *ACL functionality*. It is considered a mature tool because it adjusts comfortably with traditional automation frameworks. It offers simple operations and helps to code quickly. But, on the other hand, it is not good at services like logical dependencies, orchestration services, and interconnected applications.

You can now choose between these two, according to the requirement of the situation and the job. For example, if the containerized solution is used to provision software within the cloud, then Terraform is preferable. On the other hand, if you want to gain reasonable control of your devices and find other ways to deploy underlying services, Ansible is more suitable. These tools will provide more comprehensive solutions in the future.

*** Conclusion
It is essential to know which tool is used for which job among Terraform vs. Ansible. Terraform is mainly known for provisioning infrastructure across various clouds. It supports more than 200 providers and a great tool to manage cloud services below the server. In comparison, Ansible is optimized to perform both provisioning and configuration management. Therefore, we can say that both Terraform and Ansible can work hand in hand as standalone tools or work together but always pick up the right tool as per the job requirement.

*** References


* Linux :@Linux:
** DONE Getting Started Dynamic Window Manager (DWM) :DWM:
CLOSED: [2021-11-30 Tue 17:50]
:PROPERTIES:
:EXPORT_FILE_NAME: dwm
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction


*** Installing
**** Install Xorg

#+begin_src console
pacman -S xorg-server xorg-xinit xorg-xrandr xorg-xsetroot
#+end_src
**** Install DWM
#+begin_src bash
git clone git://git.suckless.org/dwm ~/.config/dwm
git clone git://git.suckless.org/st ~/.config/st
git clone git://git.suckless.org/dmenu ~/.config/dmenu
#+end_src

#+begin_src bash
cd ~/.config/dwm && sudo make install
cd ~/.config/st && sudo make install
cd ~/.config/dmenu && sudo make install
#+end_src
**** Installing a Display Manager (DM)
#+begin_src bash
pacman -S lightdm

pacman -S lightdm-gtk-greeter

pacman -S lightdm-gtk-greeter-settings
#+end_src
**** Enable lightdm service
#+begin_src bash
sudo systemctl enable lightdm
#+end_src
**** Adding an entry for DWM in the DM
Create this file and add the following.
#+begin_src bash
mkdir /usr/share/xsessions

sudo vim /usr/share/xsessions/dwm.desktop
#+end_src

#+begin_src file
[Desktop Entry]
Encoding=UTF-8
Name=Dwm
Comment=the dynamic window manager
Exec=dwm
Icon=dwm
Type=XSession
#+end_src

*** Multi-monitor setup
If configured to use Xinerama libraries in config.mk, dwm can automatically detect configured screen outputs (monitor, overhead projector, etc.) and their resolutions and draw the windows in the output area accordingly.

One of the easiest ways to configure screen outputs is via the RandR X server extension using the xrandr tool. Without arguments it will list the current configuration of screen outputs.

#+begin_src bash
xrandr
#+end_src

For each connected output the supported resolution modes will be printed.

This is a example for set-up xrandr. You can put below content into ~/.xprofile, when system run X windows will set-up montors automatically.
#+begin_src bash
#!/bin/bash

###############################
# Set Monitor                 #
###############################
xrandr --output DP-1 --primary --mode 1920x1080 --pos 0x0 --rotate left --output HDMI-1 --mode 2560x1440 --pos 1080x0 --rotate normal --output DVI-D-1 --off
#+end_src


*** Basic Commands
+ Moving between windows: *[Alt]+[j]* or *[Alt]+[k]*
+ To move a terminal to another tag: *[Shift]+[Alt]+[<TAG_NUMBER>]*
+ To focus on another tag: *[Alt]+[tag number]*
+ To change the amount of windows in the master area: *[Alt]+[d]* (Decrease) or *[Alt]+[i]* (Increase)
+ To toggle a window between the master and stack area: *[Alt]+[Return]*
+ To kill a window: *[Shift]+[Alt]+[c]*
+ Click another tag with the right mouse button to bring those windows into your current focus.
*** Layouts
*Note*: By default dwm is in tiled layout mode.

+ Tiled: *[Alt]+[t]*
+ Floating: *[Alt]+[f]*
+ Monocle: *[Alt]+[m]*

Further layout modes can be included through patches.

*** Floating
To resize the floating window: *[Alt]+[right mouse button]*

To move it around *[Alt]+[left mouse button]*

Floating in Tiled Layout

+ Toggle floating mode on the active window: *[Alt]+[Shift]+[space]*
+ Resize the window: *[Alt]+[right mouse button]*
+ toggle it in being floating *[Alt]+[middle mouse button]*

If you want to set some type of window to be always floating, look at the config.def.h and the rules array, where the last but one element defines this behaviour.

*** Quitting
To quit dwm cleanly: *[Shift]+[Alt]+[q]*

** DONE Getting Started Fcitx with chinese input method in Linux :Fcitx:chinese:linux:
CLOSED: [2021-12-06 Mon 13:56]
:PROPERTIES:
:EXPORT_FILE_NAME: chineseInputMethod
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction
Fcitx is a lightweight input method framework aimed at providing environment independent language support for Linux. It supports a lot of different languages and also provides many useful non-CJK features.

In this article, I will introduction:
1. How to install Fcite in Manjaro/Arch Linux.
2. How to Config Environmental variables

*** Install fcite packages in Manjaro

#+begin_src bash
yay -Syu adobe-source-han-sans-otc-fonts adobe-source-han-serif-otc-fonts
#+end_src

#+begin_src bash
yay -Syu fcitx fcitx-googlepinyin fcitx-im fcitx-configtool
#+end_src

*** Config Environmental variables

#+begin_src bash
vim ~/.profile
#+end_src

add:

#+begin_src file
export GTK_IM_MODULE=fcitx
export QT_IM_MODULE=fcitx
export XMODIFIERS=@im=fcitx

fcitx &
#+end_src

#+begin_src bash
source .profile
#+end_src


* Research :@Research:
** DONE Getting Started with Zotero :zotero:
CLOSED: [2021-12-07 Tue 12:46]
:PROPERTIES:
:EXPORT_FILE_NAME: zotero
:EXPORT_OPTIONS: author:nil
:ID:       2fbd7db1-7307-4837-ad23-93c57ce2c463
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :summary Set-up Zotero with Better Bibtex, zotfile and Scite.
:END:

*** Introduction

*** Why Use Zotero?
+ Be Organized: Keep all of your research and citations in one place
+ Save time: Format fewer citations by hand
+ Collaborate: Work with anyone in the world, anytime
+ It's Free: No cost even after you

*** Zotero Installation

#+begin_src bash
yay -S zotero
#+end_src

*** Launch Zotero :zotero-sync:

**** Create a Zotero Account
If you haven’t already created a Zotero account, please take a few moments to register now [[https://www.zotero.org/user/register][ *Here* ]]. It’s a free way to sync and access your library from anywhere, and it lets you join groups and back up all your attached files.

**** Set up Zotero syncing
You can now set up Zotero syncing to sync your data across multiple computers, access your library online, or collaborate in group libraries. Follow these steps to get started.

1. Open the Sync pane of the Zotero preferences
Goto "Edit" and click "Preferences"

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1618958146/zotero/Wed_Apr_21_08_33_25_AM_AEST_2021_yoifbp.png]]

2. Enter your username and password
Enter your username and password into the Sync preferences and click “Set Up Syncing”. Zotero will now automatically sync your data as you make changes.

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1618958273/zotero/Wed_Apr_21_08_37_31_AM_AEST_2021_on28w2.png]]

*** Zotero Connector
Go to this [[https://chrome.google.com/webstore/detail/zotero-connector/ekhagklcjbdpajgpjgmbionohlpdbjgc][link]].

*** Better Bibtex :better-bibtex:

**** Instaliion
Install by downloading the [[https://github.com/retorquere/zotero-better-bibtex/releases/tag/v5.4.29][latest release]] and save the XPI file, just clicking it and then in Zotero:

1. In the main menu go to Tools > Add-ons
2. Select ‘Extensions’
3. Click on the gear in the top-right corner and choose ‘Install Add-on From File…’
4. Choose .xpi that you’ve just downloaded, click ‘Install’
5. Restart Zotero

**** Settings
1. Go to Edit -> Preferences -> Better BibTex

***** citation key format
You can set key format gengeration the same format key with Google scholar

#+begin_src file
[auth:lower][year][veryshorttitle:lower]
#+end_src

Zotero:
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1631529034/zotero/citationKey_000_gnoo2l.png]]

Google Scholar:
#+begin_src file
@article{jordan2015machine,
  title={Machine learning: Trends, perspectives, and prospects},
  author={Jordan, Michael I and Mitchell, Tom M},
  journal={Science},
  volume={349},
  number={6245},
  pages={255--260},
  year={2015},
  publisher={American Association for the Advancement of Science}
}
#+end_src

**** Export
1. In the main menu go to File > Export Library
2. Format you can choose Better BibTex.
**Inportance Note**: Don't forget choose **keep updated**
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1631527429/zotero/export_ilg1il.png]]
3. Choose folder
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1631527595/zotero/exportFile_rtlngo.png]]
4.  There is a example:
   #+begin_src console
[yanboyang713@Boyang-PC ~]$ head reference.bib

@misc{ActionCommandsBasler,
  title = {Action {{Commands}} | {{Basler}}},
  howpublished = {https://docs.baslerweb.com/action-commands\#action-group-mask},
  file = {/home/yanboyang713/Zotero/storage/NKXFFZRJ/action-commands.html}
}

@misc{ActionCommandsBaslera,
  title = {Action {{Commands}} | {{Basler}}},
  howpublished = {https://docs.baslerweb.com/action-commands},
   #+end_src

*** Zotfile :zotfile:
Install by downloading the [[https://github.com/jlegewie/zotfile/releases/][latest release]] and save the XPI file, just clicking it – and then in Zotero:

1. In the main menu go to Tools > Add-ons
2. Select ‘Extensions’
3. Click on the gear in the top-right corner and choose ‘Install Add-on From File…’
4. Choose .xpi that you’ve just downloaded, click ‘Install’
5. Restart Zotero

**** Settings
1. In the main menu go to Tools -> ZotFile Preferences

***** Location of Files
1. Set **Custom Location**. For example: /home/yanboyang713/papers
2. Use subfolder defined by **/%a**, mean author name.

***** Renaming Rules
1. Set *Format for all Item Types except Patents*: {%b}
This will rename file same with your Citation Key.
2. Set *Maximum number of authors* choose 1
3. Uncheck *Add suffix when authors are omitted*

*** Scite :scite:
Install by downloading the [[https://github.com/scitedotai/scite-zotero-plugin/releases][latest release]]  and save the XPI file, just clicking it – and then in Zotero:

1. In the main menu go to Tools > Add-ons
2. Select ‘Extensions’
3. Click on the gear in the top-right corner and choose ‘Install Add-on From File…’
4. Choose .xpi that you’ve just downloaded, click ‘Install’
5. Restart Zotero

*** Reference List
1. http://zotfile.com/
2. https://github.com/scitedotai/scite-zotero-plugin
3. https://retorque.re/zotero-better-bibtex/


** DONE Building a Second Brain
CLOSED: [2021-12-07 Tue 10:18]
:PROPERTIES:
:EXPORT_FILE_NAME: SecondBrain
:EXPORT_OPTIONS: author:nil
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: subtitle: Capturing, Organizing, and Sharing Knowledge for Scholars and Researchers

:END:
*** Introduction
Building a Second Brain is about creating a reliable system – outside your physical skin-and-bone bodily boundaries – for storing, organising, digesting, and eventually transforming information into Good Creative Output.

Much like any well-integrated tool, I am currently using *Emacs Org Mode* as Second Brain *core* tool. This system also includes many other peripheral components that make up the complete *Second Brain* system.

I will follow the below overview diagram introducte each components one by one.

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1638851036/research/orgmode2_firjld.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1638851036/research/orgMode_asb3an.png]]

*** Org-mode modules
**** Planning
+ Task management (pomodoro method; time-blocking)
+ Time management (appointments; time-blocking)
***** TODO’s and tags
+ These are identifiers in an org-file as tasks or reminders.
+ The types of TODO’s can either be set globally in your init file, or they can be file/buffer specific.
+ They are created as a subtree (think ‘heading’), or in-line ('org-inlinetask).
+ You can assign deadlines, scheduled date and time, active timestamps, and inactive timestamps
***** Org-capture
These are customizable org-headings that you can create on-the-go.
They can be regular TODO’s or just notes.
***** Org-agenda
Populates all your TODO’s and appointments into a singular view.
Default is week-view.
Using org-super-agenda, I set up my agenda as a daily view with
appointments, deadlines, and a habit tracker.
***** Org-sidebar
Another way of accessing your TODO’s that are outside of your agenda. I
am using it to keep my project-specific TODO’s in the main project org
file.
**** Writing
***** org-roam
A note-taking package that replicates Roam Research which is based on
the Zettelkasten method. I use it to build my literature review and I use
org-roam-server to visualize my notes into a network.
It builds on the strength of org-mode’s hyperlinking properties.
***** Org-roam-bibtex
Utilizes a combination of org-ref, helm-bibtex, and
bibtex-completion to streamline note-taking workflow with references
within the org-roam ecosystem.
***** Org-noter
I use it to annotate PDFs and take notes within the same buffer.
+ Works extremely well with PDF-tools.
+ org-noter-create-skeleton

***** Org-transclusion
An effective way of “copy/pasting” text from one org file (let’s say
an org-roam note or a section of your thesis/dissertation) into your
main org file.
It will export all the transcluded text.
Sort of equivalent to “#+INCLUDE:”

**** Reference Management
   [[id:2fbd7db1-7307-4837-ad23-93c57ce2c463][Research/Getting Started with Zotero]]
