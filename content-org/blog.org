#+title: Boyang Yan's Blog
#+hugo_base_dir: ~/blog/
#+hugo_section: posts
#+hugo_front_matter_format: yaml

* Emacs :@Emacs:
** DONE Getting Started with Doom Emacs
CLOSED: [2021-10-05 Tue 03:44]
:PROPERTIES:
:EXPORT_FILE_NAME: doom
:EXPORT_OPTIONS: author:nil
:END:
*** Prerequisites
**** Instation
***** Arch Linux
#+begin_src bash
# required dependencies
pacman -S git emacs ripgrep
# optional dependencies
pacman -S fd
yay -S emacs-pdf-tools-git
```
#+end_src

With Emacs and Doom’s dependencies installed, next is to install Doom Emacs itself:
#+begin_src bash
git clone https://github.com/hlissner/doom-emacs ~/.emacs.d
~/.emacs.d/bin/doom install
#+end_src

*** Sart-up
https://zzamboni.org/post/my-doom-emacs-configuration-with-commentary/

Install the icons to avoid having weird symbols.
*M-x all-the-icons-install-fonts*

*Note*: M-x is SPC :

Open a file

**SPC f f** OR *SPC .*

*** The bin/doom utility
This utility is your new best friend. It won’t spot you a beer, but it’ll shoulder much of the work associated with managing and maintaining your Doom Emacs configuration, and then some. Not least of which is installation of and updating Doom and your installed packages.

It exposes a variety of commands. *bin/doom* help will list them all, but here is a summary of the most important ones:

+ *doom sync*: This synchronizes your config with Doom Emacs. It ensures that needed packages are installed, orphaned packages are removed and necessary metadata correctly generated. Run this whenever you modify your doom! block or packages.el file. You’ll need doom sync -u if you override the recipe of package installed by another module.
+ *doom upgrade*: Updates Doom Emacs (if available) and all its packages.
doom env: (Re)generates an “envvar file”, which is a snapshot of your shell environment that Doom loads at startup. If your app launcher or OS launches Emacs in the wrong environment you will need this. **This is required for GUI Emacs users on MacOS.**
+ *doom doctor*: If Doom misbehaves, the doc will diagnose common issues with your installation, system and environment.
+ *doom purge*: Over time, the repositories for Doom’s plugins will accumulate. Run this command from time to time to delete old, orphaned packages, and with the -g switch to compact existing package repos.
Use doom help to see an overview of the available commands that doom provides, and doom help COMMAND to display documentation for a particular COMMAND.

I recommend you add a couple of *alias* to your ZSH configuration.
#+begin_example
# Doom Emacs
alias doomsync="$HOME/.emacs.d/bin/doom sync"
alias doomupgrade="$HOME/.emacs.d/bin/doom upgrade"
alias doomdoctor="$HOME/.emacs.d/bin/doom doctor"
alias doompurge="$HOME/.emacs.d/bin/doom purge"
alias doomclean="$HOME/.emacs.d/bin/doom clean"
alias doombuild="$HOME/.emacs.d/bin/doom build"
#+end_example

*** Doom config file overview
We already know how to open a file and how to use doom utility, so let's we take a overview for Doom configuration.

Doom Emacs at the least uses three config files.

+ *init.el* defines which of the existing Doom modules are loaded. A Doom module is a bundle of packages, configuration and commands, organized into a unit that can be toggled easily from this file. You also can design your own *Module*.
+ *packages.el* defines which packages should be installed, beyond those that are installed and loaded as part of the enabled modules.
+ *config.el* contains all custom configuration and code. when you have get so many configuration contains, you may need seperate for each category. There is a example. Click [[https://github.com/yanboyang713/doom.git][Here]]

There are other files that can be loaded, but theses are the main ones. The load order of different files is defined depending on the type of session being started.

**** Config file headers
We start by simply defining the standard headers used by the three files. These headers come from the initial files generated by doom install, and contain either some Emacs-LISP relevant indicators like lexical-binding, or instructions about the contents of the file.

+ init.el
+ packages.el
+ config.el

#+begin_example
;;; ../../dotfiles/doom/+research.el -*- lexical-binding: t; -*-
#+end_example

**** Customized variables
Doom [[https://github.com/hlissner/doom-emacs/blob/develop/docs/getting_started.org#configure][does not recommend the Emacs customize mechanism]] :

*Note*: do not use M-x customize or the customize API in general. Doom is designed to be configured programmatically from your config.el, which can conflict with Customize’s way of modifying variables.

All necessary settings are therefore set by hand as part of this configuration file. The only exceptions are “safe variable” and “safe theme” settings, which are automatically saved by Emacs in custom.el, but this is OK as they don’t conflict with anything else from the config.

*** General configuration
My user information.
#+begin_src emacs-lisp
(setq user-full-name "Boyang Yan"
      user-mail-address "yanboyang713@gamil.com")
#+end_src

*** Projects with Projectile, File Explorer with Treemacs & EShell
**** Projectile
Doom Emacs have used package [[https://github.com/bbatsov/projectile][Projectile]] to management our project.

*SPC p p* - Switch to project.
*SPC SPC* - Find a File in a project

#+begin_src emacs-lisp
projectile-project-search-path '("~/Project/" "~/dotfiles/" "~/blog/content-org/")
#+end_src

**** Treemacs
SPC o p - Open
When Treemacs is opened, you can type *q* to close.


** TODO Getting Started with send and receive Email with Doom Emacs
:PROPERTIES:
:EXPORT_FILE_NAME: sendAndReceiveEmailWithdoom
:EXPORT_OPTIONS: author:nil
:END:
*** Receive Email
#+begin_src console
yay -S mu mbsync-git
#+end_src

https://devanswers.co/create-application-specific-password-gmail/

#+begin_src console
gpg2 -c xxxxxxx
#+end_src

#+begin_src console
time mu init --maildir=~/MailDir --my-address='yanboyang713@gmail.com'
mu index

time mbsync -c ~/.config/mu4e/mbsyncrc -a

#+end_src

[yanboyang713@Boyang-PC] ➜ ~ time mu init --maildir=~/MailDir --my-address='yanboyang713@gmail.com'
error: failed to open store @ /home/yanboyang713/.cache/mu/xapian: Unable to get write lock on /home/yanboyang713/.cache/mu/xapian: already locked
mu init --maildir=~/MailDir --my-address='yanboyang713@gmail.com'  0.00s user 0.00s system 66% cpu 0.006 total
[yanboyang713@Boyang-PC] ➜ ~ mu index
error: failed to open store @ /home/yanboyang713/.cache/mu/xapian: Unable to get write lock on /home/yanboyang713/.cache/mu/xapian: already locked


pkill -2 -u $UID mu
sleep 1
mu index



Using GPG for mbsync passwords
The basic idea is that every time a password is needed, an particular file is decrypted and loaded. The key for the decryption can be prompted for and be stashed by gpg-agent. The first step is to create a GPG key, which is covered very well elsewhere. The standard authentication mechanism for gnus and smtpmail can be reused to store login information for mbsync. For any one account, the password for IMAP access and the password for sending email (usually the same) can be added to ~/.authinfo.gpg:

machine imap.gmail.com login MyAccountName@gmail.com password MYPASSWORD machine smtp.gmail.com login MyAccountName@gmail.com password MYPASSWORD The first line is used by mbsync and the second by smtpmail. The line: PassCmd “gpg2 -q –for-your-eyes-only –no-tty -d ~/.authinfo.gpg | awk ’machine imap.gmail.com login MyAccountName@gmail.com {print $NF}’”

*** Send Email

#+begin_src console
yay -S msmtp msmtp-mta s-nail
#+end_src
echo "hello there username." | msmtp -a default username@domain.com

~/.mailrc
set mta=/usr/bin/msmtp


*** Doom Emacs Set-up
(package! mu4e)

*** Usage
https://cheatography.com/ddoherty03/cheat-sheets/mu4e-with-gmail-hints/

* Computer Vision :@ComputerVision:

** DONE Getting started with FFmpeg
CLOSED: [2021-11-01 Mon 15:03]
:PROPERTIES:
:EXPORT_FILE_NAME: ffmpeg
:EXPORT_OPTIONS: author:nil
:END:
*** Overview
FFmpeg is a free software project and is the leading software for everything related to multimedia like video encoding, streaming and muxing.

FFmpeg - "FF" mean "Fast Forward", "mpeg" mean "Moving Picture Expers Group"

*** Installation
https://github.com/jrottenberg/ffmpeg

https://www.whoishostingthis.com/compare/ffmpeg/resources/

alias ffmpeg='docker run -v=`pwd`:/tmp/ffmpeg opencoconut/ffmpeg'

#+begin_src bash
yay -S ffmpeg
#+end_src

*** Using Linux Terminal to Install VLC in Ubuntu
sudo snap install vlc

*** Invert the video stream to a virtual video camera
If your video stream is inverted, you can make a new virtual video camera which inverts the inverted video. You need to install v4l-utils and also v4l2loopback-dkms.

#+begin_src bash
yay -S v4l-utils v4l2loopback-dkms
#+end_src

*** Create the virtual video camera:
#+begin_src bash
modprobe v4l2loopback
#+end_src
https://askubuntu.com/questions/881305/is-there-any-way-ffmpeg-send-video-to-dev-video0-on-ubuntu

Check the name of the newly created camera:

#+begin_src console
[yanboyang713@boyang ~]$ v4l2-ctl --list-devices
Dummy video device (0x0000) (platform:v4l2loopback-000):
	/dev/video0
#+end_src

*** Image to virtual camera
ffmpeg -re -loop 1 -i input.jpg -vf format=yuv420p -f v4l2 /dev/video0


Then you can run ffmpeg to read from your actual webcam (here /dev/video0) and invert it and feed it to the virtual camera:

$ ffmpeg -f v4l2 -i /dev/video0 -vf "vflip" -f v4l2 /dev/video1
You can use the "Dummy" camera in your applications instead of the "Integrated" camera.

Bad image quality
If you experience images being too bright, too dark, too exposed or any other, you can install v4l2ucpAUR to tweak your image output.


** TODO Image Compression Based on Principal Component Analysis (PCA)
:PROPERTIES:
:EXPORT_FILE_NAME: PCAforImage
:EXPORT_OPTIONS: author:nil
:END:
*** Introduction
Principal Component Analysis (PCA) is a linear dimensionality reduction technique (algorithm) that transform a set of correlated variables (p) into a smaller k (k<p) number of uncorrelated variables called principal components while keeping as much of the variability in the original data as possible.

One of the use cases of PCA is that it can be used for image compression — a technique that minimizes the size in bytes of an image while keeping as much of the quality of the image as possible.



** DONE Beginning Explore artificial intelligence and computer vision
CLOSED: [2021-10-07 Thu 19:17]
:PROPERTIES:
:EXPORT_FILE_NAME: firstExploreAIandComputerVision
:EXPORT_OPTIONS: author:nil
:END:

*** What is artificial intelligence?
**** Explore into artificial intelligence
For the definition of artificial intelligence, academic research area always have different understandings. The widely accepted definition is:

+ *Artificial intelligence is the use of machines to simulate human cognitive abilities technology*.

Artificial intelligence involves a wide range of insights, learning, reasoning and decision-making.

From the perspective of industry application, the core ability of artificial intelligence ability is to make judgments or predictions based on given input.

The rise of deep learning and the three booms of AI.
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633594626/cv/MLThreeBooms_xnexn3.png]]

The Turing test, the cornerstone of artificial intelligence
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633607665/cv/turingTest_000_wuxoka.png]]


**** Three core elements of artificial intelligence
Three core elements of AI: data, algorithm and compute resource.
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633609084/cv/threeCoreElements_pj0xlg.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633609181/cv/threeCoreElementsOne_vn5zm9.png]]

***** Data
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633607997/cv/data_yoauah.png]]

***** Algorith
When you give a computer a task, you tell it not only what to do, but how to do it and a set of instructions about how to do it is called an algorithm.

+ Traditional algorithms -- traversal
+ Smarter algorithms -- gradient descent
+ More complex algorithms -- machine learning

***** Compute Resource/Power
 Breakthrough in computing power -- traditional CPU and new computing acceleration technology.

 [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633608323/cv/cpu_tkdhfn.png]]

 [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633608393/cv/fpga_bsknu0.png]]

 [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633608486/cv/compare_ou1gus.png]]

 smart chip
 [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633608633/cv/smartChip_ev498y.png]]


**** Artificial intelligence technonly relationship
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633609345/cv/relationship_nugk48.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673844/cv/AIrelationship_myqcj6.png]]

+ *Machine learning*: a way to achieve artificial intelligence

It is a multi-field interdisciplinary subject, involving probability theory, statistics, approximation theory, convex analysis, algorithm complexity theory and other subjects. Machine learning is the core of artificial intelligence, the fundamental way to make computers intelligent, and its applications are widespread
In all fields of artificial intelligence, it mainly uses induction and synthesis rather than deduction.

+ *Deep learning*: a technology that implements machine learning.

It uses a deep neural network to process the model more complex, so that the model has a deeper understanding of the data. It is a method of machine learning based on data representation learning. The motivation is to establish and simulate the human brain to analyzing the learning neural network, it imitates the mechanism of the human brain to interpret data, such as images, sounds and texts. The essence of deep learning is to learn more by building a machine learning model with many hidden layers and massive training data. Use the features to ultimately improve the accuracy of classification or prediction.

+ *Artificial neural network*: a machine learning algorithm

Neural networks generally have input layer -> hidden layer -> output layer. Generally speaking, a neural network with more than two hidden layers is called a deep neural network. Deep learning is a machine that uses a deep architecture like a deep neural network. Learn method.

***** What is machine Learning
*Artificial intelligence is a technology that uses machines to simulate human cognitive abilities*.

+ Traditional artificial intelligence methods: logical reasoning, expert systems (answering questions based on manually defined rules), etc.;

+ Contemporary artificial intelligence generally acquires the ability to make predictions and judgments through learning-machine learning

#+begin_example
Normal cat: round head, short face, five fingers on the forelimbs, four toes on the hind limbs, with sharp and curved claws at the ends of the toes,
The claws can stretch. Nocturnal. ---Baidu Encyclopedia
#+end_example

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633610156/cv/ml1_zrobow.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633610156/cv/ml2_lb82ew.png]]

***** Typical machine learning process
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633610289/cv/ml3_tzhdmh.png]]

***** What is Neural Network
****** How do people think? --Biological Neural Network

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633610519/cv/neuron_lhvbvb.png]]

sensor:
1. External stimulation passes through nerve endings and turns converted into electrical signals, transduced to nerve cells (Also called neuron)
2. Numerous neurons form the nerve center
3. The nerve center integrates various signals to do judgement.
4. According to the instructions of the nerve center, the human body respond to external stimuli.

****** How does the machine think? --Artificial neural networks

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633672744/cv/arNeuron_awz0bq.png]]

Artificial neuron

Input: x1,x2,x3
Output: output
Simplified model: It is agreed that each input has only two possible 1 or 0

All inputs are 1, which means that various conditions are met, and the output is 1;

All inputs are 0, which means that the condition is not true, and the output is 0

#+begin_example
Is watermelon good or bad?
Color: green; root: curled up; knock: voiced thoughts. ---Good melon
#+end_example

#+begin_example
Family Spring Outing?
Price: high and low; weather: good or bad; family: can you travel
#+end_example

****** The logical architecture of the neural network

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673214/cv/architectureNeuralNetworkOne_sijbou.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673216/cv/architectureNeuralNetworkTwo_l4dsh1.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673225/cv/architectureNeuralNetworkThree_cuepcr.png]]



***** What is Deep Learning
Deep neural network & deep learning

+ The traditional neural network has developed to a situation with multiple hidden layers,

+ Neural networks with multiple hidden layers are called deep neural networks, and machine learning research based on deep neural networks is called deep learning.

  [[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633673700/cv/DeepLearning_tq9n1e.png]]

**** The foreseeable future of artificial intelligence
***** Computer vision

+ Typical technology:
Face detection, tracking, recognition and attribute analysis, pedestrian and vehicle detection, tracking, recognition and attribute analysis, text detection and recognition, object detection and recognition

+ Typical application:
Face authentication, intelligent transportation, robot vision (such as drones), image search engine, image and video understanding, image and video beautification

***** Speech Recognition

+ Typical technology:
Voice recognition, voiceprint recognition, multi-microphone array system

+ Typical application:
Voice input, voice control, intelligent assistant, machine translation, robot hearing

***** natural language

+ Typical technology:
Words and sentences embedded, semantic modeling

+ Typical application:
Chatbot, smart assistant, smart customer service, video Frequency understanding, machine translation

*** Computer vision (CV)
**** What is CV
Several more rigorous definitions:

+ "Construct a clear and meaningful description of the objective objects in the image" (Ballard & Brown, 1982)

+ "Calculate the characteristics of the three-dimensional world from one or more digital images" (Trucco & Verri, 1998)

+ "Based on perceptual images to make useful decisions for objective objects and scenes" (Sockman & Shapiro, 2001)

Overview in one sentence:

It means that the computer has the ability to see, know, and think. It can be said that the computer has vision, that is, computer vision.

**** Deep learning and CV
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633674756/cv/computerVisionOne_kjgmyf.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633674755/cv/computerVisionTwo_l2tvqa.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633674756/cv/computerVisionThree_rdqpta.png]]

**** Application of CV
***** Image Classification

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678185/cv/classificationOne_lqnjcd.png]]

Image Classification - Neural Neural Network (CNN)

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678184/cv/classificationTwo_pu6kuc.png]]

Linear rectifier layer--RELU

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678183/cv/classificationThree_zhnzgk.png]]

Pooling layer-pool

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678183/cv/classificationFour_k9lyy9.png]]

***** Target Detection

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678492/cv/detactionOne_rsz53f.png]]

R-CNN

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678492/cv/detactionTwo_lnellb.png]]

***** Target Tracking
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678587/cv/tracking_rjkb2z.png]]

***** Semantic Image Segmentation
[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678681/cv/SegmentationOne_zfgix9.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678681/cv/SegmentationTwo_orkdal.png]]

***** Instance Segmentation

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678767/cv/instanceOne_duswl8.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633678767/cv/instanceTwo_osehjy.png]]

**** CV skills tree construction

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633680105/cv/treeOne_kbmiwg.png]]

[[https://res.cloudinary.com/dkvj6mo4c/image/upload/v1633680106/cv/treeTwo_wchdqb.png]]


** DONE Gettting Started SRS(Simple Realtime Server)
CLOSED: [2021-11-13 Sat 11:43   ]
:PROPERTIES:
:EXPORT_FILE_NAME: srs
:EXPORT_OPTIONS: author:nil
:END:

*** Overview
SRS is a simple, high efficiency and realtime video server, supports RTMP/WebRTC/HLS/HTTP-FLV/SRT.


* Cluster :@Cluster:
** DONE Getting Started ROS and Openwrt with Proxmox
CLOSED: [2021-11-05 Fri 19:52]
:PROPERTIES:
:EXPORT_FILE_NAME: ROSandOpenwrtProxmox
:EXPORT_OPTIONS: author:nil
:END:

*** Install Open vSwitch
Update the package index and then install the Open vSwitch packages by executing:

#+begin_src console
 apt update
 apt install ifupdown2
 apt install openvswitch-switch
#+end_src

root@pve-home:~# cat /etc/network/interfaces
https://karneliuk.com/2021/08/infrastructure-1-building-virtualized-environment-with-debian-linux-and-proxmox-on-hp-and-supermicro/

ifreload -a
ifup vmbr0

#+begin_src file
auto lo
iface lo inet loopback

auto enp6s0
iface enp6s0 inet manual

auto enp1s0
iface enp1s0 inet manual

auto enp2s0
iface enp2s0 inet manual

auto enp3s0
iface enp3s0 inet manual

auto enp5s0
iface enp5s0 inet manual

auto ens9
iface ens9 inet manual

auto vlan1
iface vlan1 inet static
        address 192.168.1.2/24
        gateway 192.168.1.1
        ovs_type OVSIntPort
        ovs_bridge vmbr0
        ovs_options vlan_mode=access
        ovs_extra set interface ${IFACE} external-ids:iface-id=$(hostname -s)-${IFACE}-vif
        dns-nameservers 192.168.1.1 8.8.8.8 8.8.4.4

auto bond0
iface bond0 inet manual
        ovs_bonds enp1s0 enp2s0 enp3s0 ens9 enp5s0
        ovs_type OVSBond
        ovs_bridge vmbr0
        ovs_options vlan_mode=native-untagged bond_mode=balance-slb

auto vmbr0
iface vmbr0 inet manual
        ovs_type OVSBridge
        ovs_ports bond0 vlan1
#+end_src


*** mikrotik
https://mikrotik.com/software


*** OpenWrt
https://openwrt.org/downloads
https://downloads.openwrt.org/releases/21.02.1/targets/x86/
https://downloads.openwrt.org/releases/21.02.1/targets/x86/64/
*** Migration of servers to Proxmox VE
https://pve.proxmox.com/wiki/Migration_of_servers_to_Proxmox_VE
*** VLAN
https://engineerworkshop.com/blog/configuring-vlans-on-proxmox-an-introductory-guide/


** DONE Getting Started Configuring VLANs on Proxmox
CLOSED: [2021-11-05 Fri 19:52]
:PROPERTIES:
:EXPORT_FILE_NAME: vlan
:EXPORT_OPTIONS: author:nil
:END:
*** Introduction
A virtualization server allows you to run multiple machines, virtual machines (VMs), on one physical device, also known as the host. There could be many different VMs each for different tasks. In this guide, we will discuss configuring your Proxmox virtualization server to use VLANs so that you can group related VMs onto their own subnet.
*** Motivation
For security, as well as organizational purposes, physical machines are often separated on the network from each other by VLANs. By logically separating devices based on their functionality with these VLANs, we can make sure that our family's personal devices aren't sitting out in the open on the same subnet exposed to our internet-facing web servers. This is fairly easy on your regular network setup because the devices are physically separate from each other and so each ethernet port physically connected to a device can be assigned an individual VLAN.

However, this system starts to break down when faced with virtualization servers. This is because diverse virtual machines are all sitting on the same physical host, forcing each VM to share the same physical connection. With a standard bridge between the individual VM and the host's NIC, we necessarily end up with each VM on the same subnet as the Proxmox host itself. Additionally, we end up with each VM on the same subnet as every other VM on that host. Not ideal.
*** Solution
Thankfully there's a way around this. In Proxmox, you can make your virtual bridge VLAN-aware so you can pass multiple VLANs through to your Proxmox server using only a single physical port. The individual VMs can then be configured to use whichever VLAN you choose.


** DONE Create Proxmox cloud-init template
CLOSED: [2021-12-03 Fri 18:27]
:PROPERTIES:
:EXPORT_FILE_NAME: clouldInit
:EXPORT_OPTIONS: author:nil
:ID:       632ee4dd-5bdd-4103-bab1-b3c1110aeac6
:END:
*** Overview
In this article, I'll demonstrate how to create a cloud-init enabled Ubuntu 20.04 LTS base image to use on Proxmox VE.

*** Cloud Native Image
The tradition packer builder to build a base image from an ISO file. Modern Linux distributions are increasingly moving away from this install method and preseed files. Rather, disk images are provided with the OS pre-installed, and configuration is performed via cloud-init. We will create a Proxmox KVM base image using Ubuntu's KVM cloud image.

*** Proxmox Script
The Proxmox API doesn't appear to offer the full functionality provided by the native shell commands to create a template, so we will run a script via SSH or Proxmox node's GUI shell.

The Script you can found on my GitHub. There is the Link.

The below sections, I will explain this Script step by steps.

*** Step 1: Download the image
We are downloading the kvm disk image.

*Note*: This is a qcow2 image format with an extension of .img, Promxox doesn't like this so we rename the disk image to .qcow2

#+begin_src bash
SRC_IMG="https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64-disk-kvm.img"
IMG_NAME="focal-server-cloudimg-amd64-disk-kvm.qcow2"
wget -O $IMG_NAME $SRC_IMG
#+end_src

*** Step 2: Add QEMU Guest Agent
The Ubuntu 20.04 image we are going to use does not include the *qemu-guest-agent* package which is needed for the *Guest VM* to report its IP details back to Proxmox. This is required for Packer to communicate with the VM after cloning. The template. *libguestfs-tools* will allow us to embed qemu-guest-agent into the image. You can also add any additional packages you'd like in your base image. Personally, I prefer to customize this base image later with packer so that the packages can live in source control.

#+begin_src bash
apt update
apt install -y libguestfs-tools
virt-customize --install qemu-guest-agent -a $IMG_NAME
#+end_src

*** Step 3: Create a VM in Proxmox with required settings and convert to template
For best performance, virtio "hardware" should be used. Additionally, cloud-init requires a serial console and cloudinit IDE (CDROM) drive. We will set the network config to DHCP so that we get an IP address. Lastly, we will expand the template disk image size so we have space to install items later. It appears packer doesn't support doing this later.

#+begin_src bash
TEMPL_NAME="ubuntu2004-cloud"
VMID="9000"
MEM="512"
DISK_SIZE="32G"
DISK_STOR="local-lvm"
NET_BRIDGE="vmbr0"
qm create $VMID --name $TEMPL_NAME --memory $MEM --net0 virtio,bridge=$NET_BRIDGE
qm importdisk $VMID $IMG_NAME $DISK_STOR
qm set $VMID --scsihw virtio-scsi-pci --scsi0 $DISK_STOR:vm-$VMID-disk-0
qm set $VMID --ide2 $DISK_STOR:cloudinit
qm set $VMID --boot c --bootdisk scsi0
qm set $VMID --serial0 socket --vga serial0
qm set $VMID --ipconfig0 ip=dhcp
qm resize $VMID scsi0 $DISK_SIZE
qm template $VMID
# Remove downloaded image
rm $IMG_NAME
#+end_src

*** Step 4: Packer template
Now that we have our cloud-init enabled image on Proxmox, we can use Packer to create a template based off of this template.
Ensure to set the scsi_controller="virtio-scsi-pci" and qemu_agent=true.

I'd recommend adding the Proxmox variables to a var file.

#+begin_src bash
packer build --var-file=./proxmox.pkvars.hcl --var "proxox_template_name=test-output-template" --var "proxmox_source_template=ubuntu2004-cloud" base.pkr.hcl
#+end_src

*** Final
Now that you've created a template using packer from the base template, you can use Terraform to deploy that VM!

*** References
1. https://gist.github.com/chriswayg/43fbea910e024cbe608d7dcb12cb8466
2. https://whattheserver.com/proxmox-cloud-init-os-template-creation/
3. https://norocketscience.at/deploy-proxmox-virtual-machines-using-cloud-init/
4. https://pve.proxmox.com/wiki/Cloud-Init_Support
5. https://blog.dustinrue.com/2020/05/going-deeper-with-proxmox-cloud-init/
6. https://gist.github.com/mike1237/cce83a74f898b11c2cec911204568cf9



** DONE Build a Kubernetes cluster on Proxmox via Ansible and Terraform
CLOSED: [2021-12-01 Wed 20:27]
:PROPERTIES:
:EXPORT_FILE_NAME: k8sOnProxmox
:EXPORT_OPTIONS: author:nil
:END:

[[https://miro.medium.com/max/1400/1*jL6SE1nSaPQb4EOWGnbZpw.jpeg]]


*** Overview
Proxmox is an open-source hypervisor that have enterprise capabilities and a large community behind it.

For Terraform and Ansible, i always like the idea of infrastructure as code (iac) and Terraform and Ansible just make it easy to accomplish.

The idea here was to be able to spin up a k3s cluster with minimum effort so i can spin it up and down for ever project that i would like to run.
*** Prerequires
1. read [[id:ee79f2a9-445b-4756-9853-e0819fda588c][DevOps/Terraform Beginner's Guide]]
2. read [[id:5012520e-c7d0-4b8e-8575-6ecf70e819b6][DevOps/Ansible Beginner's Guide]]
3. read [[id:632ee4dd-5bdd-4103-bab1-b3c1110aeac6][Cluster/Create Proxmox cloud-init template]]
*** System requirements
+ The deployment environment must have [[https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html][Ansible]] 2.4.0+
+ [[https://learn.hashicorp.com/tutorials/terraform/install-cli][Terraform]] installed
+ [[https://www.proxmox.com/en/proxmox-ve][Proxmox]] server

*** Proxmox setup


*** References
1. https://medium.com/@ssnetanel/build-a-kubernetes-cluster-using-k3s-on-proxmox-via-ansible-and-terraform-c97c7974d4a5



* DevOps :@DevOps:
** DONE DevOps Beginner's Guide :DevOps:
CLOSED: [2021-12-02 Thu 11:18]
:PROPERTIES:
:EXPORT_FILE_NAME: DevOps
:EXPORT_OPTIONS: author:nil
:ID:       807c90ce-9dfd-44be-861b-b2893282ed5f
:END:

*** Overview
In this blog, I discussed *what is DevOps*, and why it has *gained* so much *traction* in the IT industry lately.

*** What Is DevOps?
It is a combination of practices that *streamline* the *automation* and *integration* of processes between the *software development* and *IT teams*. This will help them to *build*, *test*, and *release software* in a faster and more reliable way.

**** Purpose
The term was formed by combining the words *"development"* and *"operations"* and signifies a cultural shift that *helps bridge the gap between the development and operation teams*.
**** Goal
 The *goal* of DevOps is to change and improve the relationship by advocating better communication and collaboration between these two business units.

*** DevOps Model For Teams
Teams using the DevOps model are able to evolve and improve their products at a higher rate over the organizations that use traditional processes. *Collaboration*, *Communication*, and *Integration* are the key elements of incorporating DevOps into any development and delivery setting.

This speed enables the teams (and in turn their organizations) to better serve their customers and compete more effectively in the market.

[[https://d1.awsstatic.com/product-marketing/DevOps/DevOps_feedback-diagram.ff668bfc299abada00b2dcbdc9ce2389bd3dce3f.png]]

*** DevOps Advantages
Improvement of collaboration between all stakeholders from planning through delivery and automation of the delivery process in order to:

+ Increase deployment frequency
+ Achieve faster time to market
+ Decrease the failure rate of new releases
+ Shorten the lead time between fixes
+ Improve mean time to recovery

According to the State of DevOps Report, "high-performing IT organizations deploy 30x more frequently with 200x shorter lead times; they have 60x fewer failures and recover 168x faster."

*** DevOps Principles
The phrase “The Three Ways” is used to describe the underlying principles of the DevOps movement.

**** The First Way: Principles of Flow
The First Way states the following, about the flow of work:

+ Work should only flow in one direction
+ No known defect should be passed downstream
+ Always seek to increase the flow

**** The Second Way: Principles of Feedback
The Second Way describes the feedback process as the following:

+ Establish an upstream feedback loop
+ Shorten the feedback loop
+ Amplify the feedback loop
  [[https://blog-assets.freshworks.com/freshservice/wp-content/uploads/2019/01/23142830/2.png]]

**** The Third Way: Principles of Continuous Learning
The Third Way describes the environment and culture, as the following practices

+ Promote experimentation
+ Learn from success and failure
+ Constant improvement
+ Seek to achieve mastery through practice

[[https://blog-assets.freshworks.com/freshservice/wp-content/uploads/2019/01/23142856/3.png]]


** DONE Terraform Beginner's Guide
CLOSED: [2021-12-02 Thu 11:09]
:PROPERTIES:
:EXPORT_FILE_NAME: terraform
:EXPORT_OPTIONS: author:nil
:ID:       ee79f2a9-445b-4756-9853-e0819fda588c
:END:
*** Overview
In this blog post, I am going to cover a brief introduction of *Infrastructure as Code (IaC)*, *Terraform*, its *lifecycle*, and all the core concepts that every beginner should know. I have tried to cover all the topics in this beginner’s guide that will give you a quick start for using Terraform.

*** Prerequires
1.[[id:807c90ce-9dfd-44be-861b-b2893282ed5f][DevOps/DevOps Beginner's Guide]]

*** What Is Infrastructure as Code (IaC)?
*Infrastructure as Code (IaC)* is a widespread terminology among DevOps professionals and a key DevOps practice in the industry. It is the process of managing and provisioning the complete IT infrastructure (comprises both physical and virtual machines) using machine-readable definition files. It helps in automating the complete data center by using programming scripts.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/Explanation-of-how-IaC-works.jpg]]

*** Popular IaC Tools:
1. *Terraform*: An open-source declarative tool that offers pre-written modules to build and manage an infrastructure.
2. *Chef*: A configuration management tool that uses cookbooks and recipes to deploy the desired environment. Best used for Deploying and configuring applications using a pull-based approach.
3. *Puppet*: Popular tool for configuration management that follows a Client-Server Model. Puppet needs agents to be deployed on the target machines before the puppet can start managing them.
4. *Ansible*: Ansible is used for building infrastructure as well as deploying and configuring applications on top of them. Best used for Ad hoc analysis.
5. *Packer*: Unique tool that generates VM images (not running VMs) based on steps you provide. Best used for Baking compute images.
6. *Vagrant*: Builds VMs using a workflow. Best used for Creating pre-configured developer VMs within VirtualBox.

*** What Is Terraform?
*Terraform* is one of the most popular *Infrastructure-as-code (IaC) tool*, used by DevOps teams to automate infrastructure tasks. It is used to automate the provisioning of your cloud resources. Terraform is an open-source, cloud-agnostic provisioning tool developed by HashiCorp and written in GO language.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/logo-hashicorp-e1605707253653.png]]

Benefits of using Terraform:

+ Does orchestration, not just configuration management
+ Supports multiple providers such as AWS, Azure, Oracle, GCP, and many more
+ Provide immutable infrastructure where configuration changes smoothly
+ Uses easy to understand language, HCL (HashiCorp configuration language)
+ Easily portable to any other provider

*** Terraform Lifecycle
Terraform lifecycle consists of – *init*, *plan*, *apply*, and *destroy*.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/terraform-lifecycle.png]]

1. *Terraform init* initializes the (local) Terraform environment. Usually executed only once per session.
2. *Terraform plan* compares the Terraform state with the as-is state in the cloud, build and display an execution plan. This does not change the deployment (read-only).
3. *Terraform apply* executes the plan. This potentially changes the deployment.
4. *Terraform destroy* deletes all resources that are governed by this specific terraform environment.

*** Terraform Core Concepts
1. *Variables*: Terraform has input and output variables, it is a key-value pair. Input variables are used as parameters to input values at run time to customize our deployments. Output variables are return values of a terraform module that can be used by other configurations.

   Please, read article on [[https://k21academy.com/terraform-iac/variables-in-terraform/][Terraform Variables]]

2. *Provider*: Terraform users provision their infrastructure on the major cloud providers such as AWS, Azure, OCI, and others. A provider is a plugin that interacts with the various APIs required to create, update, and delete various resources.

   Please, read article to know more about [[https://k21academy.com/terraform-iac/terraform-providers-overview/][Terraform Providers]]

3. *Module*: Any set of Terraform configuration files in a folder is a module. Every Terraform configuration has at least one module, known as its *root module*.

4. *State*: Terraform records information about what infrastructure is created in a Terraform state file. With the state file, Terraform is able to find the resources it created previously, supposed to manage and update them accordingly.

5. *Resources*: Cloud Providers provides various services in their offerings, they are referenced as Resources in Terraform. Terraform resources can be anything from compute instances, virtual networks to higher-level components such as DNS records. Each resource has its own attributes to define that resource.

6. *Data Source*: Data source performs a read-only operation. It allows data to be fetched or computed from resources/entities that are not defined or managed by Terraform or the current Terraform configuration.

7. *Plan*: It is one of the stages in the Terraform lifecycle where it determines what needs to be created, updated, or destroyed to move from the real/current state of the infrastructure to the desired state.

8. *Apply*: It is one of the stages in the Terraform lifecycle where it applies the changes real/current state of the infrastructure in order to achieve the desired state.

Check Out: Our previous blog post on [[https://k21academy.com/terraform-iac/terraform-cheat-sheet/][Terraform Cheat Sheet]].

*** Terraform Installation
Terraform Installation
Before you start working, make sure you have Terraform installed on your machine, it can be installed on any OS, say Windows, macOS, Linux, or others. Terraform installation is an easy process and can be done in a few minutes.

Read our blog to know how to [[https://k21academy.com/terraform-iac/terraform-installation-overview/][install Terraform]] in Linux, Mac, Windows

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/Terraform-installation.jpg]]

*** Terraform Providers
A provider is responsible for understanding API interactions and exposing resources. It is an executable plug-in that contains code necessary to interact with the API of the service. Terraform configurations must declare which providers they require so that Terraform can install and use them.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/Terraform-provider-api-call.png]]

Terraform has over a hundred providers for different technologies, and each provider then gives terraform user access to its resources. So through AWS provider, for example, you have access to hundreds of AWS resources like EC2 instances, the AWS users, etc.

*** Terraform Configuration Files
Configuration files are a set of files used to describe infrastructure in Terraform and have the file extensions .tf and .tf.json. Terraform uses a declarative model for defining infrastructure. Configuration files let you write a configuration that declares your desired state. Configuration files are made up of resources with settings and values representing the desired state of your infrastructure.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/terraform-config-files-e1605834689106.png]]

A Terraform configuration is made up of one or more files in a directory, provider binaries, plan files, and state files once Terraform has run the configuration.

1. *Configuration file (*.tf files)*: Here we declare the provider and resources to be deployed along with the type of resource and all resources specific settings

2. *Variable declaration file (variables.tf or variables.tf.json)*: Here we declare the input variables required to provision resources

3. *Variable definition files (terraform.tfvars)*: Here we assign values to the input variables

4. *State file (terraform.tfstate)*: a state file is created once after Terraform is run. It stores state about our managed infrastructure.

*** Getting started using Terraform
To get started building infrastructure resources using Terraform, there are few things that you should take care of. The general steps to deploy a resource(s) in the cloud are:

1. Set up a Cloud Account on any cloud provider (AWS, Azure, OCI)
2. Install Terraform
3. Add a provider – AWS, Azure, OCI, GCP, or others
4. Write configuration files
5. Initialize Terraform Providers
6. PLAN (DRY RUN) using terraform plan
7. APPLY (Create a Resource) using terraform apply
8. DESTROY (Delete a Resource) using terraform destroy

*** Import Existing Infrastructure
Terraform is one of the great IaC tools with which, you can deploy all your infrastructure’s resources. In addition to that, you can manage infrastructures from different cloud providers, such as AWS, Google Cloud, etc. But what if you have already created your infrastructure manually?

Terraform has a really nice feature for importing existing resources, which makes the migration of existing infrastructure into Terraform a lot easier.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2020/11/terraform-import-workflow-diagram.png]]

Currently, Terraform can only import resources into the state. It does not generate a configuration for them. Because of this, prior to running terraform import it is necessary to write manually a resource configuration block for the resource, to which the imported object will be mapped. For example:

#+begin_src file
resource "aws_instance" "import_example" {
  # ...instance configuration...
}
#+end_src

Now terraform import can be run to attach an existing instance to this resource configuration:

#+begin_src console
$ terraform import aws_instance.import_example i-03efafa258104165f
#+end_src

This command locates the AWS instance with ID i-03efafa258104165f (which has been created outside Terraform) and attaches it to the name aws_instance.import_example in the Terraform state.

*** Conclusion
I hope the above gives you an idea about how you can get started with Terraform.

*** Related/References


** DONE Ansible Beginner's Guide
CLOSED: [2021-12-02 Thu 17:58]
:PROPERTIES:
:EXPORT_FILE_NAME: ansible
:EXPORT_OPTIONS: author:nil
:ID:       5012520e-c7d0-4b8e-8575-6ecf70e819b6
:END:
*** Overview
This article covers all the aspects of Ansible, a tool used in DevOps for the Management, Deployment, and Orchestration of IT Infrastructure.

*** What Is Ansible?
Ansible is a simple configuration management and IT automation engine for multi-tier deployments. It automates both cloud and on-premise provisioning & configuration. It automates cloud provisioning. Rather than managing one system at a time, Ansible uses a model that inter-relates the entire IT infrastructure and enables you to manage everything using something called an Infrastructure as Code (IAC) approach. Ansible is secure and agentless. It relies on OpenSSH and the code written in YAML format. Ansible nodes are run on Unix systems but they can be used to configure changes across Unix as well as Windows systems.

*** Who should learn Ansible?
Ansible is a part of the DevOps stack. Ansible means automation. Ansible seamlessly connects workflow orchestration with configuration management and provisioning deployment. Ansible has various use cases in Provisioning, Configuration Management, Application Deployment, Continuous Deployment, Automation, and Orchestration. So, if you are looking forward to a career in DevOps, IT automation, and managing cloud infrastructure then Ansible is a must-have.

*** Why Use Ansible?
+ *No Agent*: As long as the box can be ssh’d into and it has python, it can be configured with Ansible.
+ *Idempotent*: Ansible’s whole architecture is structured around the concept of idempotency. The core idea here is that you only do things if they are needed and that things are repeatable without side effects.
+ *Declarative Not Procedural*: Other configuration tools tend to be procedural do this and then do that and so on. Ansible works by you writing a description of the state of the machine that you want and then it takes steps to fulfill that description.
+ *Tiny Learning Curve*: Ansible is quite easy to learn. It doesn’t require any extra knowledge.

*** Ansible Use Cases
+ *Provisioning*: Provisioning is creating new infrastructure. Ansible allows for application management, deployment, orchestration, and configuration management.
+ *Continuous Delivery*: Ansible provides a simpler way to automatically deploy applications. All required services for a deployment can be configured from a single system. Continuous Integration (CI) tool can be used to run Ansible playbook which can be used to test and automatically deploy the application to production if tests are passed.
+ *Application Deployment*: Ansible provides a simpler way to deploy applications across the infrastructure. Deployment of multi-tier applications can be simplified and the infrastructure can be easily changed over time.
+ *Ansible for Cloud Computing*: Ansible makes it easy to provision instances across all cloud providers. Ansible contains multiple modules and allows to manage of large cloud infrastructure across the public-private and hybrid cloud.
+ *Ansible for Security and Compliance*: You can define security policies in Ansible which will automate security policy across all machines in the network. Security roles once configured in an Ansible node will be embedded across all machines in the network automatically.

*** Ansible Architecture Diagram
[[https://miro.medium.com/max/564/1*eaY6sN1T9VJiVOrMQMNdRQ.png]]
[[https://miro.medium.com/max/625/0*K9Kqdh4ZLT-fHJeP.png]]
[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/06/Ansible_Diagram2-16-1024x461.png]]

+ *Modules*: Modules are script-like programs written to specify the desired state of the system. These are typically written in a code editor. Modules are written by the developer and executed via SSH. Modules are part of a larger program called Playbook. Ansible module is a standalone script that can be used inside an Ansible Playbook.
+ *Plugins*: Plugins are pieces of code that enhance the core functionality of Ansible. Plugins execute on the control node.
+ *Inventory*: Ansible reads information about the machines you manage from the inventory. Inventory is listed in the file which contains IP addresses, databases, and servers.
+ *Playbook*: Playbooks are files written in YAML. Playbooks describe the tasks to be done by declaring configurations in order to bring a managed node into the desired state.

*** Ansible Playbook
+ Plain-text YAML files that describe the desired state of something
+ Human and Machine-readable
+ Can be used to build the entire application environment

[[https://miro.medium.com/max/463/0*t2iCHi_buMmtKGmw]]

*** What Are Inventories In Ansible?
+ Static lines of servers
+ Dynamic list of servers: AWS, Azure, GCP, etc.
+ Ranges
+ Other custom things

  [[https://miro.medium.com/max/201/1*mLdHcg8SvBvXRDceZIdKeA.png]]
  [[https://miro.medium.com/max/1006/0*E_bhUEFXGoQCOV_K.jpg]]

*** Ansible Modules
+ Over 1000 modules provided by Ansible to automate
+ Modules are like plugins that do the actual work in Ansible, they are what gets executed in each playbook task.
+ Each module is mostly standalone and can be written in a standard scripting language (such as Python, Perl, Ruby, Bash, etc.)

[[https://miro.medium.com/max/793/1*UDC-1_SR4Z26APTYRWDP3w.png]]

*** Ansible Tower
Ansible Tower is a GUI and REST interface for Ansible that supercharges it by adding RBAC, centralized logging, auto-scaling/provisioning call-backs, graphical inventory editing, and more.

*Capabilities*:

This command-line tool sends commands to the Tower API. It is capable of retrieving, creating, modifying, and deleting most resources within the Tower.

+ A few potential uses include:
+ Launching playbook runs (for instance, from Jenkins, TeamCity, Bamboo, etc.)
+ Checking on job statuses
+ Rapidly creating objects like organizations, users, teams, and more.

[[https://www.ansible.com/products/tower]]

*** Ansible Roles
1. Roles are a way to group tasks together into one container. We could have a role for setting up MySQL, another one for configuring ip tables.
2. Roles make it easy to configure hosts. Any role can be performed on any host or group of hosts such as:
    + hosts: all
    + roles:
    + role_1
    + role_2

*** Ansible Variables
There are many different ways to source variables:
+ Playbooks
+ Files
+ Inventories (group vars, host vars)
+ Command-line Discovered Variables
+ Ansible Tower

*** How To Run The Ansible Commands?
*Ad-Hoc*: Ansible <inventory> -m
[[https://miro.medium.com/max/520/1*W8ndyJq6S37tdAPBEHvUbQ.png]]
*Playbooks*: Ansible-playbook
[[https://miro.medium.com/max/519/1*SfmrmCzzcKmf4GO7TApVmg.png]]

*** AD-HOC Commands Examples
*Transferring file to many servers/machines*

#+begin_src console
$ Ansible Abc -m copy -a "src = /etc/yum.conf dest = /tmp/yum.conf"
#+end_src

*Creating a new directory*

#+begin_src console
$ Ansible ABC -m file -a "dest = /path/user1/new mode = 777 owner = user1 group = user1 state = directory"
#+end_src

*Deleting whole directory and files*

#+begin_src console
$ Ansible ABC -m file -a "dest = /path/user1/new state = absent"
#+end_src


** DONE The comparison and introduction between Terraform and Ansible
CLOSED: [2021-12-02 Thu 10:53]
:PROPERTIES:
:EXPORT_FILE_NAME: terraformAndAnsible
:EXPORT_OPTIONS: author:nil
:END:
*** Overview
The Terraform vs. Ansible battle continues to escalate as the DevOps environment focuses more on automation and orchestration. These two tools help in automating configurations and deploying infrastructure. Terraform offers to deploy Infrastructure as a Code, helps in readability and lift and shift deployments. Ansible is a configuration management tool for automating system configuration and management.

*** Terraform

**** What is Terraform?

Terraform is an open-source tool for building, changing, and versioning infrastructure securely and effectively. It is an Infrastructure as a Code tool that is very straightforward to use. It helps to develop and scale Cloud services and manage the state of the network. Its primary use is in data centers and software-defined networking environments. It does not install and manage software on existing devices; instead, it creates, modifies, and destroys servers and various other cloud services. Slack, Uber, Starbucks, Twitch, all big brands are using Terraform. We can also integrate Terraform with Microsoft Azure, Heroku, and Google Compute Engine, etc.

Now, we will discuss the working of terraform.

**** How does Terraform work?
There are two main working components of terraform.

+ Terraform Core
+ Providers

Terraform is of *declarative nature*. It directly describes the end state of the system without defining the steps to reach there. It works at a high level of abstraction to describe what services and resources should be created and defined.

Terraform core takes two input sources to do its job. The first input source is a *terraform configuration* that is configured by its users. Users define what needs to be provisioned and created. The second input source is a state that holds information about the infrastructure.

So terraform core takes the input and figures out various plans for what steps to follow to get the desired output.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/06/TerraformCore_Diagram-08-1024x421.png]]

The second principal component is providers, such as cloud providers like AWS, GCP, Azure, or other Infrastructure as service platforms. It helps to create infrastructure on different levels. Let’s take an example where users create an AWS infrastructure, deploy Kubernetes on top of it, and then create services inside the cluster of Kubernetes. Terraform has multiple providers for various technologies; users can access resources from these providers through terraform. This is the basic working terminology of terraform that helps to provision and cover the complete application set up from infrastructure to fully developed application.

**** Features of Terraform
As we have discussed the working of Terraform, now we will look at the features of Terraform.

+ Terraform follows a *declarative approach* which makes deployments fast and easy.
+ It is a convenient tool to display the resulting model in a *graphical form*.
+ Terraform also manages *external service providers* such as cloud networks and in-house solutions.
+ It is one of the rare tools to offer *building infrastructure* from scratch, whether public, private or multi-cloud.
+ It helps *manage parallel environments*, making it a good choice for testing, validating bug fixes, and formal acceptance.
+ Modular code helps in achieving *consistency*, *reusability*, and *collaboration*.
+ Terraform can *manage multiple clouds* to increase fault tolerance.


*** Ansible

**** What is Ansible?
Ansible is the most significant way to automate and configure apps and IT infrastructure.  Ansible is an *open-source configuration management tool* mainly designed for provisioning and deploying applications using IaaC.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/06/Ansible-Official-Logo-Black-299x300.png]]

It has its own language to describe system configuration. Ansible is *agentless*, making it manage large deployments across enterprises using Windows Power Shell or SSH to perform its tasks. Ansible is not completely declarative; it is a hybrid of procedural and declarative. It can integrate with Amazon EC2, Docker, and Kubernetes. Companies like Zalando, Revolt, and 9gaga are using Ansible.

**** How does Ansible work?
Ansible is agentless and doesn’t run on target nodes. It makes *connections using SSH* or other authentication methods. It installs various *Python modules* on the target using JSON. These modules are simple instructions that run on the target. These modules are executed and removed once their job is done. This strategy ensures that there is no misuse of resources on target. Python is mandatory to be installed on both the controlling and the target nodes.

[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/06/Ansible_Diagram-07-1024x564.png]]

Ansible *management node* acts as a controlling node that controls the entire execution of the playbook. This node is the place to run the installations. There is an *inventory file* that provides the host list where the modules need to be run. The management node makes SSH connections to execute the modules on the host machine and installs the product. Modules are removed once they are installed in the system. This is the simple working process of Ansible.

**** Features of Ansible

Now we will discuss various features Ansible provides to benefit its users.

+ Ansible is used for *configuration management* and follows a procedural approach.
+ Ansible deals with *infrastructure platforms* such as bare metal, cloud networks, and virtualized devices like hypervisors.
+ Ansible follows *idempotent behavior* that makes it to place node in the same state every time.
+ It uses *Infrastructure as a Code system configuration* across the infrastructure.
+ It offers *rapid and easy deployment* of multi-tier apps with being agentless.
+ If the code is *interrupted*, it allows entering the code again without any conflicts with other invocations.

*** Difference between Terraform and Ansible Provisioning
Let’s see how Terraform vs. Ansible battle differentiates from each other:

| Terraform                                                                                        | Ansible                                                                                                     |
|--------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------|
| Terraform is a provisioning tool.                                                                | Ansible is a configuration management tool.                                                                 |
| It follows a declarative Infrastructure as a Code approach.                                      | It follows a procedural approach.                                                                           |
| It is the best fit for orchestrating cloud services and setup cloud infrastructure from scratch. | It is mainly used for configuring servers with the right software and updates already configured resources. |
| Terraform does not support bare metal provisioning by default.                                   | Ansible supports the provisioning of bare metal servers.                                                    |
| It does not provide better support in terms of packaging and templating.                         | It provides full support for packaging and templating.                                                      |
| It highly depends on lifecycle or state management.                                              | It does not have lifecycle management at all.                                                               |

*** Configuration Management vs. Orchestration
Terraform and Ansible have so many similarities and differences at the same time. The difference comes when we look at two significant concepts of DevOps: *Orchestration* and *configuration management*.

*Configuration management* tools solve the issues locally rather than replacing the system entirely. Ansible helps to configure each action and instrument and ensures smooth functioning without any damage or error. In addition, Ansible comes up with hybrid capabilities to perform both orchestration and replace infrastructure.

*Orchestration tools* ensure that an environment is in its desired state continuously. Terraform is explicitly designed to store the state of the domain. Whenever there is any glitch in the system, terraform automatically restores and computes the entire process in the system after reloading. It is the best fit in situations where a constant and invariable state is needed. *Terraform Apply* helps to resolve all anomalies effectively.

Let’s have a look at the *Procedural* and *Declarative* nature of Terraform and Ansible.

*** Procedural vs Declarative
There are two main categories of DevOps tools: *Procedural* vs. *Declarative*. These two categories tell the action of tools.

Terraform follows the *declarative approach*, ensuring that if your defined environment suffers changes, it rectifies those changes. This tool attempts to reach the desired end state described by the sysadmin. Puppet also follows the declarative approach. With terraform, we can describe the desired state and figure out how to move from one state to the next automatically.

Ansible is of hybrid nature. It follows both declarative and *procedural style* configuration.  It performs ad-hoc commands to implement procedural-style configurations. Please read the documentation of Ansible very carefully to get in-depth knowledge of its behavior. It’s important to know whether you need to add or subtract resources to get the desired result or need to indicate the resources required explicitly.

*** Terraform vs Ansible Provisioning
[[https://eadn-wc03-4064062.nxedge.io/cdn/wp-content/uploads/2021/07/TerraformVsAnsible-400x224.png]]

*Terraform* deals with *infrastructure automation*. Its current declarative model lacks some features which arise complexity. Using Terraform, the elements of required environments are separately described, including their relationships. It assesses the model, creates a plan based on dependencies, and gives optimized commands to Infrastructure as a Service. If there is no change in the environment or strategy, repeated runs will do nothing. If there is any update in the plan or environment, it will *synchronize* the cloud infrastructure.

*Ansible* follows a *procedural approach*. Various users create playbooks that are evaluated through top to bottom approach and executed in sequence. *Playbooks* are responsible for the configuration of network devices that contributes towards a procedural approach.  Of course, Ansible provisions the cloud infrastructure as well. But its procedural approach limits it to large infrastructure deployments.

*** Which one to choose: Terraform or Ansible?
*Terraform* vs. *Ansible*: Every tool has its unique characteristics and limitations. Let’s check out which one to go with.

*Terraform* comes with good *scheduling capabilities* and is very *user-friendly*. It integrates with docker well, as docker handles the configuration management slightly better than Terraform. But there is no clear evidence of how the target devices are brought to their final state, and sometimes, the final configuration is unnecessary.

*Ansible* comes with better *security* and *ACL functionality*. It is considered a mature tool because it adjusts comfortably with traditional automation frameworks. It offers simple operations and helps to code quickly. But, on the other hand, it is not good at services like logical dependencies, orchestration services, and interconnected applications.

You can now choose between these two, according to the requirement of the situation and the job. For example, if the containerized solution is used to provision software within the cloud, then Terraform is preferable. On the other hand, if you want to gain reasonable control of your devices and find other ways to deploy underlying services, Ansible is more suitable. These tools will provide more comprehensive solutions in the future.

*** Conclusion
It is essential to know which tool is used for which job among Terraform vs. Ansible. Terraform is mainly known for provisioning infrastructure across various clouds. It supports more than 200 providers and a great tool to manage cloud services below the server. In comparison, Ansible is optimized to perform both provisioning and configuration management. Therefore, we can say that both Terraform and Ansible can work hand in hand as standalone tools or work together but always pick up the right tool as per the job requirement.

*** References


* Linux :@Linux:
** DONE Getting Started Dynamic Window Manager (DWM)
CLOSED: [2021-11-30 Tue 17:50]
:PROPERTIES:
:EXPORT_FILE_NAME: dwm
:EXPORT_OPTIONS: author:nil
:END:

*** Introduction


*** Installing
**** Install Xorg

#+begin_src console
pacman -S xorg-server xorg-xinit xorg-xrandr xorg-xsetroot
#+end_src
**** Install DWM
#+begin_src bash
git clone git://git.suckless.org/dwm ~/.config/dwm
git clone git://git.suckless.org/st ~/.config/st
git clone git://git.suckless.org/dmenu ~/.config/dmenu
#+end_src

#+begin_src bash
cd ~/.config/dwm && sudo make install
cd ~/.config/st && sudo make install
cd ~/.config/dmenu && sudo make install
#+end_src
**** Installing a Display Manager (DM)
#+begin_src bash
pacman -S lightdm

pacman -S lightdm-gtk-greeter

pacman -S lightdm-gtk-greeter-settings
#+end_src
**** Enable lightdm service
#+begin_src bash
sudo systemctl enable lightdm
#+end_src
**** Adding an entry for DWM in the DM
Create this file and add the following.
#+begin_src bash
mkdir /usr/share/xsessions

sudo vim /usr/share/xsessions/dwm.desktop
#+end_src

#+begin_src file
[Desktop Entry]
Encoding=UTF-8
Name=Dwm
Comment=the dynamic window manager
Exec=dwm
Icon=dwm
Type=XSession
#+end_src

*** Multi-monitor setup
If configured to use Xinerama libraries in config.mk, dwm can automatically detect configured screen outputs (monitor, overhead projector, etc.) and their resolutions and draw the windows in the output area accordingly.

One of the easiest ways to configure screen outputs is via the RandR X server extension using the xrandr tool. Without arguments it will list the current configuration of screen outputs.

#+begin_src bash
xrandr
#+end_src

For each connected output the supported resolution modes will be printed.

This is a example for set-up xrandr. You can put below content into ~/.xprofile, when system run X windows will set-up montors automatically.
#+begin_src bash
#!/bin/bash

###############################
# Set Monitor                 #
###############################
xrandr --output DP-1 --primary --mode 1920x1080 --pos 0x0 --rotate left --output HDMI-1 --mode 2560x1440 --pos 1080x0 --rotate normal --output DVI-D-1 --off
#+end_src


*** Basic Commands
+ Moving between windows: *[Alt]+[j]* or *[Alt]+[k]*
+ To move a terminal to another tag: *[Shift]+[Alt]+[<TAG_NUMBER>]*
+ To focus on another tag: *[Alt]+[tag number]*
+ To change the amount of windows in the master area: *[Alt]+[d]* (Decrease) or *[Alt]+[i]* (Increase)
+ To toggle a window between the master and stack area: *[Alt]+[Return]*
+ To kill a window: *[Shift]+[Alt]+[c]*
+ Click another tag with the right mouse button to bring those windows into your current focus.
*** Layouts
*Note*: By default dwm is in tiled layout mode.

+ Tiled: *[Alt]+[t]*
+ Floating: *[Alt]+[f]*
+ Monocle: *[Alt]+[m]*

Further layout modes can be included through patches.

*** Floating
To resize the floating window: *[Alt]+[right mouse button]*

To move it around *[Alt]+[left mouse button]*

Floating in Tiled Layout

+ Toggle floating mode on the active window: *[Alt]+[Shift]+[space]*
+ Resize the window: *[Alt]+[right mouse button]*
+ toggle it in being floating *[Alt]+[middle mouse button]*

If you want to set some type of window to be always floating, look at the config.def.h and the rules array, where the last but one element defines this behaviour.

*** Quitting
To quit dwm cleanly: *[Shift]+[Alt]+[q]*
