[{"categories":null,"content":"ESXi Installation ","date":"2021-04-09","objectID":"/esxi/:1:0","tags":null,"title":"ESXi","uri":"/esxi/"},{"categories":null,"content":"Prerequisites Linux machine with superuser access to it USB flash drive that can be detected by the Linux machine The ESXi ISO image, VMware-VMvisor-Installer-version_number-build_number.x86_64.iso, which includes the isolinux.cfg file Syslinux package ","date":"2021-04-09","objectID":"/esxi/:1:1","tags":null,"title":"ESXi","uri":"/esxi/"},{"categories":null,"content":"Procedure Determine USB flash drive (1). list disk using fdisk [yanboyang713@boyang ~]$ sudo fdisk -l [sudo] password for yanboyang713: Disk /dev/nvme0n1: 465.76 GiB, 500107862016 bytes, 976773168 sectors Disk model: CT500P1SSD8 Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: 0B01F91F-D62C-6A4F-BDED-D81DB5D8A3F6 Device Start End Sectors Size Type /dev/nvme0n1p1 4096 618495 614400 300M EFI System /dev/nvme0n1p2 618496 940843216 940224721 448.3G Linux filesystem /dev/nvme0n1p3 940843217 976768064 35924848 17.1G Linux swap (2). Plug in your USB flash drive. (3). list disk again [yanboyang713@boyang ~]$ sudo fdisk -l Disk /dev/nvme0n1: 465.76 GiB, 500107862016 bytes, 976773168 sectors Disk model: CT500P1SSD8 Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: 0B01F91F-D62C-6A4F-BDED-D81DB5D8A3F6 Device Start End Sectors Size Type /dev/nvme0n1p1 4096 618495 614400 300M EFI System /dev/nvme0n1p2 618496 940843216 940224721 448.3G Linux filesystem /dev/nvme0n1p3 940843217 976768064 35924848 17.1G Linux swap Disk /dev/sda: 3.8 GiB, 4081057792 bytes, 7970816 sectors Disk model: USB Flash Disk Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x673666e2 Device Boot Start End Sectors Size Id Type /dev/sda1 2048 7970815 7968768 3.8G b W95 FAT32 NOTE: As you can see your disk is /dev/sda. Create a partition table on the USB flash device. [yanboyang713@boyang ~]$ sudo fdisk /dev/sda Welcome to fdisk (util-linux 2.36.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): a. Enter d to delete partitions until they are all deleted. b. Enter n to create a primary partition 1 that extends over the entire disk. c. Enter t to set the type to an appropriate setting for the FAT32 file system, such as c. d. Enter a to set the active flag on partition 1. e. Enter p to print the partition table. f. Enter w to write the partition table and exit the program. [yanboyang713@boyang ~]$ sudo fdisk /dev/sda Welcome to fdisk (util-linux 2.36.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): d Selected partition 1 Partition 1 has been deleted. Command (m for help): p Disk /dev/sda: 3.8 GiB, 4081057792 bytes, 7970816 sectors Disk model: USB Flash Disk Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x673666e2 Command (m for help): n Partition type p primary (0 primary, 0 extended, 4 free) e extended (container for logical partitions) Select (default p): p Partition number (1-4, default 1): First sector (2048-7970815, default 2048): Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-7970815, default 7970815): Created a new partition 1 of type 'Linux' and of size 3.8 GiB. Partition #1 contains a vfat signature. Do you want to remove the signature? [Y]es/[N]o: Y The signature will be removed by a write command. Command (m for help): p Disk /dev/sda: 3.8 GiB, 4081057792 bytes, 7970816 sectors Disk model: USB Flash Disk Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x673666e2 Device Boot Start End Sectors Size Id Type /dev/sda1 2048 7970815 7968768 3.8G 83 Linux Filesystem/RAID signature on partition 1 will be wiped. Command (m for help): t Selected partition 1 Hex code or alias (type L to list all): c Changed type of partition 'Linux' to 'W95 FAT32 (LBA)'. Comma","date":"2021-04-09","objectID":"/esxi/:1:2","tags":null,"title":"ESXi","uri":"/esxi/"},{"categories":null,"content":"https://flink.apache.org/downloads.html ","date":"2021-04-05","objectID":"/flink/:0:0","tags":["Flink"],"title":"Getting Started with Flink","uri":"/flink/"},{"categories":["emacs"],"content":"installation yay -S emacs yay -S adobe-source-code-pro-fonts clone spacemacs repo and use develop branch git clone https://github.com/syl20bnr/spacemacs.git ~/.emacs.d -b develop clone zilongshanren layer and checkout develop branch git clone https://github.com/yanboyang713/spacemacs-private.git ~/.spacemacs.d/ [yanboyang713@boyang ~]$ gpg --homedir ~/.emacs.d/elpa/gnupg --receive-keys 066DAFCB81E42C40 gpg: key 066DAFCB81E42C40: \"GNU ELPA Signing Agent (2019) \u003celpasign@elpa.gnu.org\u003e\" not changed gpg: Total number processed: 1 gpg: unchanged: 1 ","date":"2021-04-04","objectID":"/spacemacs/:1:0","tags":["spacemacs","emacs"],"title":"Getting Started with Spacemacs","uri":"/spacemacs/"},{"categories":null,"content":"Greek Letters Symbol Script $$\\begin{equation}\\alpha\\end{equation}$$ \\alpha $A$ A $\\beta$ \\beta $B$ B $\\gamma$ \\gammma $\\Gamma$ \\Gamma $\\pi$ \\pi $\\Pi$ \\Pi $\\phi$ \\phi $\\Phi$ \\Phi $\\varphi$ \\varphi $\\theta$ \\theta ","date":"2021-03-09","objectID":"/writingmathematicfomular/:1:0","tags":null,"title":"Writing Mathematic Fomular","uri":"/writingmathematicfomular/"},{"categories":null,"content":"Operators Symbol Script $\\cos$ \\cos $\\sin$ \\sin $\\lim$ \\lim $\\exp$ \\exp $\\to$ \\to $\\infty$ \\infty $\\equiv$ \\equiv $\\bmod$ \\bmod $\\times$ \\times ","date":"2021-03-09","objectID":"/writingmathematicfomular/:2:0","tags":null,"title":"Writing Mathematic Fomular","uri":"/writingmathematicfomular/"},{"categories":null,"content":"Power and Indices Symbol Script $k_{n+1}$ k_{n+1} $n^2$ n^2 $k_n^2$ k_n^2 ","date":"2021-03-09","objectID":"/writingmathematicfomular/:3:0","tags":null,"title":"Writing Mathematic Fomular","uri":"/writingmathematicfomular/"},{"categories":null,"content":"Fractions and Binomials Symbol Script $\\frac{n!}{k!(n-k)!}$ \\frac{n!}{k!(n-k)!} $\\binom{n}{k}$ \\binom{n}{k} $\\frac{\\frac{x}{1}}{x - y}$ \\frac{\\frac{x}{1}}{x - y} $^3/_7$ ^3/_7 ","date":"2021-03-09","objectID":"/writingmathematicfomular/:4:0","tags":null,"title":"Writing Mathematic Fomular","uri":"/writingmathematicfomular/"},{"categories":null,"content":"Roots Symbol Script $\\sqrt{k}$ \\sqrt{k} $\\sqrt[n]{k}$ \\sqrt[n]{k} ","date":"2021-03-09","objectID":"/writingmathematicfomular/:5:0","tags":null,"title":"Writing Mathematic Fomular","uri":"/writingmathematicfomular/"},{"categories":null,"content":"Sums and Integrals Symbol Script $\\sum_{i=1}^{10} t_i$ \\sum_{i=1}^{10} t_i $\\int_0^\\infty \\mathrm{e}^{-x},\\mathrm{d}x$ \\int_0^\\infty \\mathrm{e}^{-x},\\mathrm{d}x $\\sum$ \\sum $\\prod$ \\prod $\\coprod$ \\coprod $\\bigoplus$ \\bigoplus $\\bigotimes$ \\bigotimes $\\bigodot$ \\bigodot $\\bigcup$ \\bigcup $\\bigcap$ \\bigcap $\\biguplus$ \\biguplus $\\bigsqcup$ \\bigsqcup $\\bigvee$ \\bigvee $\\bigwedge$ \\bigwedge $\\int$ \\int $\\oint$ \\oint $\\iint$ \\iint $\\iiint$ \\iiint $\\idotsint$ \\idotsint $\\sum_{\\substack{0\u003ci\u003cm\\0\u003cj\u003cn}} P(i, j)$ \\sum_{\\substack{0\u003ci\u003cm\\0\u003cj\u003cn}} P(i, j) $\\int\\limits_a^b$ \\int\\limits_a^b Symbol Script $a’$ $a^{\\prime}$ a` a^{\\prime} $a’’$ a’’ $\\hat{a}$ hat{a} $\\bar{a}$ \\bar{a} $\\grave{a}$ \\grave{a} $\\acute{a}$ \\acute{a} $\\dot{a} \\dot{a} $\\ddot{a}$ \\ddot{a} $\\not{a}$ \\not{a} $\\mathring{a}$ \\mathring{a} $\\overrightarrow{AB}$ \\overrightarrow{AB} $\\overleftarrow{AB}$ \\overleftarrow{AB} $a’’’$ a’’’ $\\overline{aaa}$ \\overline{aaa} $\\check{a}$ \\check{a} $\\vec{a}$ \\vec{a} $\\underline{a}$ \\underline{a} $\\color{red}x$ \\color{red}x $\\pm$ \\pm $\\mp$ \\mp $\\int y \\mathrm{d}x$ \\int y \\mathrm{d}x $,$ , $:$ : $;$ ; $!$ ! $\\int y, \\mathrm{d}x$ \\int y, \\mathrm{d}x $\\dots$ \\dots $\\ldots$ \\ldots $\\cdots$ \\cdots $\\vdots$ \\vdots $\\ddots$ \\ddots ","date":"2021-03-09","objectID":"/writingmathematicfomular/:6:0","tags":null,"title":"Writing Mathematic Fomular","uri":"/writingmathematicfomular/"},{"categories":null,"content":"Brackets etc Symbol Script $(a)$ (a) $[a]$ [a] ${a}$ {a} $\\langle f \\rangle$ \\langle f \\rangle $\\lfloor f \\rfloor$ \\lfloor f \\rfloor $\\lceil f \\rceil$ \\lceil f \\rceil $\\ulcorner f \\urcorner$ \\ulcorner f \\urcorner https://github.com/cben/mathdown ","date":"2021-03-09","objectID":"/writingmathematicfomular/:7:0","tags":null,"title":"Writing Mathematic Fomular","uri":"/writingmathematicfomular/"},{"categories":null,"content":"Personal Evaluation I see myself advancing with a strong ability and interesting in new technology and academic research. I successfully participated two international academic conferences, one published paper and one patent. In the University study, I got undergraduate scholarships and the best prize for final project. The final project is related to swipe card system. My insteresting research area on computer graphic, softerware testing and machine learning. Working Experience 2020.3 - 2021.6 Support Engineer Microsoft, China Azure Machine Learning CSS Team 2018.12 - 2020.3 Algorithm Engineer Peng Chen Laboratory, China Network Communication Research Center 2018.1 - 2018.11 Research Assistant (Computer Science) University of Wollongong, Australia Metamorphic Testing - Software Testing Education 2021.3 - now Master of Statistics and Operations Research Royal Melbourne Institute of Technology (RMIT), Australia 2019.3 - 2020 Netmath University of Illinois (Urbana–Champaign) Major in Mathematics 2014.3 - 2017.12 Bachelor of Computer Science University of Wollongong, Australia Major in Software Engineering Main Achievement: The best final project prize (Human Research Management System base on Web) First Three Semester had: Undergraduate Excellence Scholarship 2013.3 - 2014.3 English for Tertiary Studies University of Wollongong College, Australia Main subjects: Academic English 2011.9 - 2013.3 Bachelor of Business Management China University Of Mining And Technology Average mark of 91% . Most of courses got High Distinction Publications Boyang Yan, Brian Yecies, Zhi Quan Zhou: Metamorphic Relations for Data Validation: A Case Study of Translated Text Messages. IEEE/ACM 4th International Workshop on Metamorphic Testing (MET ‘19), in conjunction with the 41st International Conference on Software Engineering (ICSE ‘19), Montreal, Canada; 05/2019 Patents June 2, 2020 An automated text difference analysis and verification system/method China Patent Office Application ID: 202010489931.8 Academy Congress and Conference May 26, 2019 Paper Presentation - Title: Metamorphic Relations for Data Validation: A Case Study of Translated Text Messages ICSE, Montreal, Canada August 11, 2017 showing my final year project about Human Resource Management System based on Web (whole swap card subsystem and database are finish independently) IEEE Sections Congress (SC2017), Sydney, Australia Certifications 2019 First Aid CPR AED American Heart Association 2017 Amateur Radio Operator’s certificate of proficiency (Standard) The Wireless Institute of Australia (WIA) 2016 Ross a. Hull memorial Vhf-Uhf contest The Wireless Institute of Australia (WIA) Twelfth Place in Section A (Analog Modes, Best 7 Days) and Twelfth Place in Section C (Analog Modes, Best 2 Days) 2014 Amateur Radio Operator’s certificate of proficiency (Foundation) The Wireless Institute of Australia (WIA) 2010 The Second Place Prize Awarded, Tianjin School Sports Competition Award “92” National Games Tianjin Tianjin Basketball Tryouts, China High school men’s Group B. Primary and Secondary (Vocational) School Basketball Games. Have got the second grading certificate and title ","date":"2021-03-05","objectID":"/about/:0:0","tags":null,"title":"About Me","uri":"/about/"},{"categories":["MATH2349"],"content":"Module 1 - Data Preprocessing: From Raw Data to Ready to Analyse ","date":"2021-03-05","objectID":"/datapreprocessing/:1:0","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Summary Module 1 will set the background for the entire course. Data preprocessing will be defined and the importance of data preprocessing inside the data analysis workflow will be explored. The module will define 5 major tasks for data preprocessing and will provide a quick overview of these tasks. Then, in the following modules of this course, we will unwrap each data preprocessing task by providing details of operations related to that task. ","date":"2021-03-05","objectID":"/datapreprocessing/:1:1","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Learning Objectives The learning objectives of this module are as follows: Define data preprocessing. Identify steps for data analysis, understand the place of data preprocessing inside the data analysis workflow. Understand the reasons why data preprocessing is important. Identify major tasks in data preprocessing. Understand main benefits of using R statistical programming language in data preprocessing. ","date":"2021-03-05","objectID":"/datapreprocessing/:1:2","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Data “Data! Data! Data! I can’t make bricks without clay!” - Sir Arthur Conan Doyle. Data. Our world has turned out to be increasingly dependent upon this resource. Sir Conan Doyle’s famous detective, Sherlock Holmes, wouldn’t shape any theories or draw any conclusions unless he had adequate data. Data is the fundamental building piece of all that we do in analytics such as the analyses we perform, the reports we build, and the decisions we made. “In God we trust. All others must bring data.” – W. Edwards Deming. This quote by [W. Edwards Deming] (https://en.wikipedia.org/wiki/W._Edwards_Deming) (statistician, professor, author, lecturer, and consultant), emphasizes the significance of data-driven decisions. Obviously, everyone has better bring (also understand and interpret) data to back up their claims. ","date":"2021-03-05","objectID":"/datapreprocessing/:1:3","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"The rise of the Data Analyst Today’s organizations have access to more data than ever before, but more data isn’t better data unless you know what to do with it. Organizations are struggling to find people who can turn their data into insights and value, which in turn has created a high demand across the world for data analysts. During an interview in 2009, Google’s Chief Economist Dr. Hal R. Varian stated, “The ability to take data - to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it - that’s going to be a hugely important skill in the next decades.” Read the full article here ","date":"2021-03-05","objectID":"/datapreprocessing/:1:4","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Data Analysis Steps The statistical approach to data analysis is much broader than just analysing data. Data analysis process starts with defining problem statement, continues with planning and collecting data, preprocessing data, exploring data using descriptive statistics and data visualizations, analysing/modelling data, and finalizes with interpreting and reporting findings. This process is depicted in the following illustration. Defining Problem Statement: This is the first step of data analysis. In this step, the problem statement is identified by the organizations/researchers. The data analyst should thoroughly understand the problem and the domain of the problem. Planning and Collecting Data: In this step, the appropriate tools for data collection related to the problem statement will be identified. This step may include designing a survey for data collection, scraping data from web or accessing an Excel/a database file. Data Preprocessing: The objective of this step is to make the data ready for the further statistical analysis. This step is one of the important phases in data analysis. The accuracy of the statistical analysis depends on the quality of the data gained in this step. Several operations such as importing data, reshaping data from long to wide format, filtering data, cleaning data, identifying outliers, and transforming variables can be applied to the data to make ready for the statistical analysis. Exploring Data via Descriptive Statistics/Visualizations: The objective of this step is to understand the main characteristics of the data. Exploratory analyses are generally done using descriptive statistics (i.e. mean, median, standard deviation, frequencies, percentages etc.) and visualization tools (i.e. scatter plots, box plots, histograms, interactive data visualizations etc.). Exploratory analysis will show you the things that you didn’t expect or raise new questions about the data. Analysing/Modelling Data: The statistical analysis/modelling step can include a broad range of techniques like statistical hypothesis testing, statistical modelling, and machine learning algorithms. Generally, the type of the variables in the data set and the purpose of the investigation will determine the appropriate analysis technique. Interpretation and Reporting: The last step of the data analysis is the reporting and the interpretation of the results. This step is also critical as if you cannot understand and communicate your results to others, it doesn’t matter how well you conducted your analysis. ","date":"2021-03-05","objectID":"/datapreprocessing/:1:5","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"What is Data Preprocessing? Most statistical theory concentrates on data modelling, prediction, and statistical inference while it is usually assumed that data are in the correct state for the data analysis. However, in practice, a data analyst spends most of his/her time (usually 50%-80% of an analyst time) on making ready the data before doing any statistical operation (Dasu and Johnson (2003)). Despite the amount of time it takes, there has been surprisingly very little emphasis on how to preprocess data well (Wickham and others (2014)). Real-world data are commonly incomplete, noisy, inconsistent, and don’t have all the correct labels and codes that are required for the analysis. Data Preprocessing, which is also commonly referred to as data wrangling, data manipulation, data cleaning, etc., is a process and the collection of operations needed to prepare all forms of untidy data (incomplete, noisy and inconsistent data) for statistical analysis. ","date":"2021-03-05","objectID":"/datapreprocessing/:1:6","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Why Data Preprocessing is important Data preprocessing may significantly influence the statistical conclusions based on the data. “Garbage in, garbage out (GIGO)” is a famous saying that is used to emphasis “the quality of the statistical analyses (output) always depends on the quality of the data (input)”. By preprocessing data, we can minimise the garbage that gets into our analysis so that we can minimise the amount of garbage that our analyses/models result. ","date":"2021-03-05","objectID":"/datapreprocessing/:1:7","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Why do you learn Data Preprocessing? The road to becoming an expert in data analysis can be challenging and in fact, obtaining expertise in the broad range of data analysis is a career-long process. In this course, you will take a step closer to fluency in the early stages; namely in the data preprocessing step, as you need to be able to import, manage, manipulate and transform your data before performing any kind of data analysis. ","date":"2021-03-05","objectID":"/datapreprocessing/:1:8","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Major Tasks in Data Preprocessing We will define 5 major tasks for data preprocessing framework, namely: Get, Understand, Tidy \u0026 Manipulate, Scan and Transform. A typical data preprocessing process usually (but not necessarily) follows the following order of tasks given below: Get: A data set can be stored in a computer or can be online in different file formats. We need to get the data set into R by importing it from other data sources (i.e., .txt, .xls, .csv files and databases) or scraping from web. R provides many useful commands to import (and export) data sets in different file formats. Understand: We cannot perform any type of data preprocessing without understanding what we have in hand. In this step, we will check the data volume (i.e., the dimensions of the data) and structure, understand the variables/attributes in the data set, and understand the meaning of each level/value for the variables. Tidy \u0026 Manipulate: In this step, we will apply several important tasks to tidy up messy data sets. We will follow Hadley Wickham’s “Tidy Data” principles: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. We may also need to manipulate, i.e. filter, arrange, select, subset/split data, or generate new variables from the data set. Scan: This step will include checking for plausibility of values, cleaning data for obvious errors, identifying and handling outliers, and dealing with missing values. Transform: Some statistical analysis methods are sensitive to the scale of the variables and it may be necessary to apply transformations before using them. In this step we will introduce well-known data transformations, data scaling, centering, standardising and normalising methods. There are also other steps related with preprocessing special types of data including dates, time and characters/strings. The last module of this course will introduce the special operations used for date, time and text preprocessing. Tidy Data Datasets can be created from a diverse ranges of sources including manually created spreadsheets, datasets scraped from the internet, previously collected or historical data, or complex databases and data warehouses. The Bicycle dataset above is an example of a previously collected dataset downloaded from an Open Access data repository. Regardless of a dataset’s origin, all must abide by the Tidy Data rules (Wickham 2014). As Wickham explains, tidy data allows easy manipulation, analysis and visualisation for data analysis purposes. The basic structure of a dataset includes rows and columns. The three tidy dataset rules are as follows: Each variable forms a column Each observation forms a row Each type of observational unit forms a table We will use a sample dataset, expain this three rules at the below. The sample dataset you can download from here Each variable forms a column The Bicycle data contains 23 columns. With the exception of Unique_ID, the other columns refer to variables. For example, the NB_YEAR column is a time variable that tells us the year an observation was recorded. If we select only the CT_VOLUME… variables, we can how the Bicycle dataset abides by Rule #1. Each observation forms a row The first row of the dataset, or the header row, includes the name of each column/variable. These names cannot contain as special characters or spaces, and their length should be as short as possible. This will reduce the amount of typing when you code! An observation refers to the units of sampling. For example, this might be a person, a date, or a machine that comprise a “population”. The sampling unit for the bicyle data refers to daily bike traffic volume at different locations. Location is one unit of sampling, day is the second unit of sampling. This is an example of a nested dataset, where daily traffic volumes can be nested within different locations. As you can see, the 55,967th observation relates to data recorded from Scotchman’s Creek Trail","date":"2021-03-05","objectID":"/datapreprocessing/:1:9","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Module 2 - Get: importing, Scraping and Exporting Data with R ","date":"2021-03-05","objectID":"/datapreprocessing/:2:0","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Summary All statistical work begins with data, and most data are stuck inside files and databases. Data are arriving from multiple sources at an alarming rate and analysts and organizations are seeking ways to leverage these new sources of information. Consequently, analysts need to understand how to get data from these sources. Module 2 will cover the process of importing data, scraping data from the web, and exporting data. First we will cover the basics of importing tabular and spreadsheet data (i.e., .txt, .xls, .csv files). Then, we will cover how to acquire data sets from other statistical software (i.e., Stata, SPSS, or SAS) and databases. As the modern data analysis techniques often include scraping data files stored online, we will also cover the fundamentals of web scraping using R. Lastly, the equally important process of getting data out of R, in other words, exporting data will be covered. ","date":"2021-03-05","objectID":"/datapreprocessing/:2:1","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Learning objectives The learning objectives of this module are as follows: Understand how to get data from tabular and spreadsheet files. Understand how to get data from statistical software and databases. Learn how to scrape data files stored online. Learn how to export to tabular and spreadsheet files. Learn how to save R objects. ","date":"2021-03-05","objectID":"/datapreprocessing/:2:2","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Purpose By completing the essential module reading, skill builders, and reviewing the recorded lecture, you will be able to work on worksheet questions and your assessments. ","date":"2021-03-05","objectID":"/datapreprocessing/:2:3","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Reading/Importing Data The first step in any data preprocessing task is to “GET” the data. Data can come from many resources but two of the most common formats of the data sources include text and Excel files. In addition to text and Excel files, there are other ways that data can be stored and exchanged. Commercial statistical software such as SPSS, SAS, Stata, and Minitab often have the option to store data in a specific format for that software. In addition, analysts commonly use databases to store large quantities of data. R has good support to work with these additional options. In this section, we will cover how to import data into R by reading data from text files, Excel spreadsheets, commercial statistical software data files and databases. Moreover, we will cover how to load data from saved R object files for holding or transferring data that has been processed in R. In addition to the commonly used base R functions to perform data importing, we will also cover functions from the popular readr, readxl and foreign packages. Reading Data from Text Files Text files are a popular way to hold and exchange tabular data as almost any data application supports exporting data to the CSV (or other text file) formats. Text file formats use delimiters to separate the different elements in a line, and each line of data is in its own line in the text file. Therefore, importing different kinds of text files can follow a consistent process once you have identified the delimiter. There are two main groups of functions that we can use to read in text files: Base R functions: The Base R functions are the built-in functions that are already available when you download R and RStudio. Therefore, in order to use Base R functions, you do not need to install or load any packages before using them. readr package functions: Compared to the equivalent base functions, readr functions are around 10× faster. In order to use readr package functions, you need to install and load the readr package using the following commands: ##install.packages(\"readr\") library(readr) Base R functions read.table() is a multi-purpose function in base R for importing data. The functions read.csv() and read.delim() are special cases of read.table() in which the defaults have been adjusted for efficiency. To illustrate these functions let’s work with a CSV (.csv comma separated values) file called iris.csv which you can download from here. Before running any command note that we need to save this data set into our working directory, or we need to explicitly define the location of this data set. In the first example, let’s assume that we have already downloaded iris.csv data and saved it in our working directory. Then, the following command will read iris.csv data and store it in the iris1 object in R as a data frame: ## The following command assumes that the iris.csv file is in the working directory iris1 \u003c- read.csv(\"iris.csv\") Note that the iris1 object has appeared in your Environment pane (probably located on the top-right of your RStudio window). If you click the arrow next to it, it will expand to show you the variables it contains, and clicking on it will open it to view, the same as using the View() function: View(iris1) Now you can also observe the initial few rows of the iris1 object using head() function as follows: head(iris1) ### X Sepal.Length Sepal.Width Petal.Length Petal.Width Species ### 1 1 5.1 3.5 1.4 0.2 setosa ### 2 2 4.9 3.0 1.4 0.2 setosa ### 3 3 4.7 3.2 1.3 0.2 setosa ### 4 4 4.6 3.1 1.5 0.2 setosa ### 5 5 5.0 3.6 1.4 0.2 setosa ### 6 6 5.4 3.9 1.7 0.4 setosa If you wish to observe more rows, you can use the n argument within the head() function: head(iris1, n = 15) ### X Sepal.Length Sepal.Width Petal.Length Petal.Width Species ### 1 1 5.1 3.5 1.4 0.2 setosa ### 2 2 4.9 3.0 1.4 0.2 setosa ### 3 3 4.7 3.2 1.3 0.2 setosa ### 4 4 4.6 3.1 1.5 0.2 setosa ### 5 5 5.0 3.6 1.4 0.2 setosa ### 6 6 5.4 3.9 1.7 0.4 setosa ### 7 7 4.6 3.4 1.4 0.3 setosa ### ","date":"2021-03-05","objectID":"/datapreprocessing/:2:4","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Additional Resources and Further Reading R data import/export manual https://cran.r-project.org/doc/manuals/R-data.html (R Team (2000)) is a comprehensive source for all types of data importing and exporting tasks in R. Also, RStudio’s “Data Import Cheatsheet” is a compact resource for all importing functions available in the readr package. ","date":"2021-03-05","objectID":"/datapreprocessing/:2:5","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"References Adler, Joseph. 2010. R in a Nutshell: A Desktop Quick Reference. \" O’Reilly Media, Inc.\". Deborah, Nolan, and TL Ducan. 2014. “XML and Web Technologies for Data Sciences with R.” N. Deborah, \u0026 TL Ducan, XML and Web Technologies for Data Sciences with R, 581–618. Munzert, Simon, Christian Rubba, Peter Meißner, and Dominic Nyhuis. 2014. Automated Data Collection with R: A Practical Guide to Web Scraping and Text Mining. John Wiley \u0026 Sons. Team, R Core. 2000. “R Data Import/Export.” ","date":"2021-03-05","objectID":"/datapreprocessing/:2:6","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Model 3 - Understand: Understanding Data and Data Structures ","date":"2021-03-05","objectID":"/datapreprocessing/:3:0","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Summary Importing data successfully doesn’t mean that we have all the information about our data. Understanding data structures and variable types in the data set are also crucial for conducting data preprocessing. We shouldn’t be performing any type of data preprocessing without understanding what we have in hand. In this module, I will provide the basics of variable types and data structures. You will learn to check the types of the variables, dimensions and structure of the data, and levels/values for the variables. We will also cover how to manipulate the format of the data (i.e., data type conversions). Finally, the difference between wide and long formatted data will be explained. ","date":"2021-03-05","objectID":"/datapreprocessing/:3:1","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Learning objectives The learning objectives of this module are as follows: Understand R’s basic data types (i.e., character, numeric, integer, factor, and logical). Understand R’s basic data structures (i.e., vector, list, matrix, and data frame) and main differences between them. Learn to check attributes (i.e., name, dimension, class, levels etc.) of R objects. Learn how to convert between data types/structures. Understand the difference between wide vs. long formatted data. ","date":"2021-03-05","objectID":"/datapreprocessing/:3:2","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Purpose By completing the essential module reading, skill builders and reviewing the recorded lecture, you will be able to finalise your Assignment 1. ","date":"2021-03-05","objectID":"/datapreprocessing/:3:3","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Types of variables A data set is a collection of measurements or records which are often called as variables and there are two major types of variables that can be stored in a data set: qualitative and quantitative. The qualitative variable is often called as categorical and they have a non-numeric structure such as gender, hair colour, type of a disease, etc. The qualitative variable can be nominal or ordinal. Nominal variable: They have a scale in which the numbers or letters assigned to objects serve as labels for identification or classification. Examples of this variable include binary variables (e.g., yes/no, male/female) and multinomial variables (e.g. religious affiliation, eye colour, ethnicity, suburb). Ordinal variable: They have a scale that arranges objects or alternatives according to their ranking. Examples include the exam grades (i.e., HD, DI, Credit, Pass, Fail etc.) and the disease severity (i.e., severe, moderate, mild). The second type of variable is called the quantitative variable. These variables are the numerical data that we can either measure or count. The quantitative variables can be either discrete or continuous. Continuous quantitative variable: They arise from a measurement process. Continuous variables are measured on a continuum or scale. They can have almost any numeric value and can be meaningfully subdivided into finer and finer increments, depending upon the precision of the measurement system. For example: time, temperature, wind speed may be considered as continuous quantitative variables. Discrete quantitative variable: They arise from a counting process. Examples include the number of text messages you sent this past week and the number of faults in a manufacturing process. The following short video by Nicola Petty provides a great overview on the variable types. Note that, in some statistical sources, the “type of the data” and the “type of the variables” are used synonymously. In the following video, the term “types of data” are used to refer the “types of variables”. {% youtube hZxnzfnt5v8 %} ","date":"2021-03-05","objectID":"/datapreprocessing/:3:4","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Data Structures in R In the previous section, we defined the types of variables in a general sense. However, as R is a programming language it has own definitions of data types and structures. Technically, R classifies all the different types of data into four classes: Logical: This class consists of TRUE or FALSE (binary) values. A logical value is often created via comparison between variables. x \u003c- 10 y \u003c- (x \u003e 0) y ### [1] TRUE We can use class() function to check the class of an object. ## check the class of y class(y) ### [1] \"logical\" Numeric (integer or double): Quantitative values are called as numeric in R. It is the default computational data type. Numeric class can be integer or double. Integer types can be seen as discrete values (e.g., 2) whereas, double class will have floating point numbers (e.g., 2.16). Here is an example of a double numeric variable: ## create a double-precision numeric variable dbl_var \u003c- c(4, 7.5, 14.5) ## check the class of dbl_var class(dbl_var) ### [1] \"numeric\" To check whether a numeric object is integer or double, you can also use typeof(). ## check the type of dbl_var object typeof(dbl_var) ### [1] \"double\" In order to create an integer variable, we must place an L directly after each number. Here is an example: ## create an integer (numeric) variable int_var \u003c- c(4L, 7L, 14L) ## check the class of int_var class(int_var) ### [1] \"integer\" Character: A character class is used to represent string values in R. The most basic way to generate a character object is to use quotation marks \" \" and assign a string/text to an object. ## create a character variable using \" \" and check its class char_var \u003c- c(\"debit\", \"credit\", \"Paypal\") class(char_var) ### [1] \"character\" Factor: Factor class is used to represent qualitative data in R. Factors can be ordered or unordered. Factors store the nominal values as a vector of integers in the range [1…k] (where k is the number of unique values in the nominal variable), and an internal vector of character strings (the original values) mapped to these integers. Factor objects can be created with the factor() function: ## create a factor variable using factor() fac_var1 \u003c- factor( c(\"Male\", \"Female\", \"Male\", \"Male\") ) fac_var1 ### [1] Male Female Male Male ### Levels: Female Male ## check its class class(fac_var1) ### [1] \"factor\" To see the levels of a factor object levels() function will be used: ## check the factor levels levels(fac_var1) ### [1] \"Female\" \"Male\" By default, the levels of the factors will be ordered alphabetically. Using the levels() argument, we can control the ordering of the levels while creating a factor: ## create a factor variable using factor() and order the levels using levels() argument fac_var2 \u003c- factor( c(\"Male\", \"Female\", \"Male\", \"Male\"), levels = c(\"Male\", \"Female\") ) fac_var2 ### [1] Male Female Male Male ### Levels: Male Female ## check its levels levels(fac_var2) ### [1] \"Male\" \"Female\" We can also create ordinal factors in a specific order using the ordered = TRUE argument: ## create a ordered factor variable using factor() and order the levels using levels() argument ordered_fac \u003c-factor( c(\"DI\", \"HD\", \"PA\", \"NN\", \"CR\", \"DI\", \"HD\", \"PA\"), levels = c(\"NN\", \"PA\", \"CR\", \"DI\", \"HD\"), ordered=TRUE ) ordered_fac ### [1] DI HD PA NN CR DI HD PA ### Levels: NN \u003c PA \u003c CR \u003c DI \u003c HD The ordering will be reflected as NN \u003c PA \u003c CR \u003c DI \u003c HD in the output. Factors are also created during the data import. Many import functions like read.csv(), read_cvs(), read.table() etc. have stringsAsFactors option that determines how the character data is read in R. For BaseR read.csv() function, the default setting is stringsAsFactors = False. However, if we set it to TRUE, then all columns that are detected to be character/strings are converted to factor variables. Let’s read the VIC_pet.csv dataset available under data repository using read.csv(): pets \u003c- read.csv(\"data/VIC_pet.csv\",stringsAsFactors = TRUE) head(pets) ### id State Region Anim","date":"2021-03-05","objectID":"/datapreprocessing/:3:5","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Vectors A vector is the basic structure in R, which consists of one-dimensional sequence of data elements of the same basic type (i.e., integer, double, logical, or character). Vectors are created by combining multiple elements into one dimensional array using the c() function. The one-dimensional examples illustrated previously are considered vectors. ## a double numeric vector dbl_var \u003c- c(4, 7.5, 14.5) ## an integer vector int_var \u003c- c(4L, 7L, 14L) ## a logical vector log_var \u003c- c(T, F, T, T) ## a character vector char_var \u003c- c(\"debit\", \"credit\", \"Paypal\") All elements of a vector must be the same type, if you attempt to combine different types of elements, they will be coerced to the most flexible type possible. Here are some examples: ## vector of characters and numerics will be coerced to a character vector ex1 \u003c- c(\"a\", \"b\", \"c\", 1, 2, 3) ## check the class of ex1 class(ex1) ### [1] \"character\" ## vector of numerics and logical will be coerced to a numeric vector ex2 \u003c- c(1, 2, 3, TRUE, FALSE) ## check the class of ex2 class(ex2) ### [1] \"numeric\" ## vector of logical and characters will be coerced to a character vector ex3 \u003c- c(TRUE, FALSE, \"a\", \"b\", \"c\") ## check the class of ex3 class(ex3) ### [1] \"character\" In order to add additional elements to a vector we can use c() function. ## add two elements (4 and 6) to the ex2 vector ex4 \u003c- c(ex2, 4, 6) ex4 ### [1] 1 2 3 1 0 4 6 To subset a vector, we can use square brackets [ ] with positive/negative integers, logical values or names. Here are some examples: ## take the third element in ex4 vector ex4[3] ### [1] 3 ## take the first three elements in ex4 vector ex4[1:3] ### [1] 1 2 3 ## take the first, third, and fifth element ex4[c(1,3,5)] ### [1] 1 3 0 ## take all elements except first ex4[-1] ### [1] 2 3 1 0 4 6 ## take all elements less than 3 ex4[ ex4 \u003c 3 ] ### [1] 1 2 1 0 ","date":"2021-03-05","objectID":"/datapreprocessing/:3:6","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Lists A list is an R structure that allows you to combine elements of different types and lengths. In order to create a list we can use the list() function. ## create a list using list() function list1 \u003c- list(1:3, \"a\", c(TRUE, FALSE, TRUE), c(2.5, 4.2)) ## check the class of list1 class(list1) ### [1] \"list\" To see the detailed structure within an object we can use the structure function str(), which provides a compact display of the internal structure of an R object. ","date":"2021-03-05","objectID":"/datapreprocessing/:3:7","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"check the structure of the list1 object str(list1) ```console ### List of 4 ### $ : int [1:3] 1 2 3 ### $ : chr \"a\" ### $ : logi [1:3] TRUE FALSE TRUE ### $ : num [1:2] 2.5 4.2 Note how each of the four list items above are of different classes (integer, character, logical, and numeric) and different lengths. In order to add on to lists we can use the append() function. Let’s add a fifth element to the list1 and store it as list2: ## add another list c(\"credit\", \"debit\", \"Paypal\") on list1 list2 \u003c- append(list1, list(c(\"credit\", \"debit\", \"Paypal\"))) ## check the structure of the list2 object str(list2) ### List of 5 ### $ : int [1:3] 1 2 3 ### $ : chr \"a\" ### $ : logi [1:3] TRUE FALSE TRUE ### $ : num [1:2] 2.5 4.2 ### $ : chr [1:3] \"credit\" \"debit\" \"Paypal\" R objects can also have attributes, which are like metadata for the object. These metadata can be very useful in that they help to describe the object. Some examples of R object attributes are: names, dimnames dimensions (e.g. matrices, arrays) class (e.g. integer, numeric) length other user-defined attributes/metadata Attributes of an object (if any) can be accessed using the attributes() function. Let’s check if list2 has any attributes. attributes(list2) ### NULL We can add names to lists using names() function. ## add names to a pre-existing list names(list2) \u003c- c (\"item1\", \"item2\", \"item3\", \"item4\", \"item5\") str(list2) ### List of 5 ### $ item1: int [1:3] 1 2 3 ### $ item2: chr \"a\" ### $ item3: logi [1:3] TRUE FALSE TRUE ### $ item4: num [1:2] 2.5 4.2 ### $ item5: chr [1:3] \"credit\" \"debit\" \"Paypal\" Now, you can see that each element has a name and the names are displayed after a dollar $ sign. In order to subset lists, we can use dollar $ sign or square brackets [ ]. Here are some examples: ## take the first list item in list2 list2[1] ### $item1 ### [1] 1 2 3 ## take the first list item in list2 using $ list2$item1 ### [1] 1 2 3 ## take the third element out of fifth list item list2$item5[3] ### [1] \"Paypal\" ## take multiple list items list2[c(1,3)] ### $item1 ### [1] 1 2 3 ### ### $item3 ### [1] TRUE FALSE TRUE ","date":"2021-03-05","objectID":"/datapreprocessing/:3:8","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Matrices A matrix is a collection of data elements arranged in a two-dimensional rectangular layout. In R, the elements of a matrix must be of same class (i.e. all elements must be numeric, or character, etc.) and all columns of a matrix must be of same length. We can create a matrix using the matrix() function using nrow and ncol arguments. ## create a 2x3 numeric matrix m1 \u003c- matrix(1:6, nrow = 2, ncol = 3) m1 ### [,1] [,2] [,3] ### [1,] 1 3 5 ### [2,] 2 4 6 The underlying structure of this matrix can be seen using str() and attributes() functions as follows: str(m1) ### int [1:2, 1:3] 1 2 3 4 5 6 attributes(m1) ### $dim ### [1] 2 3 Matrices can also be created using the column-bind cbind() and row-bind rbind() functions. However, note that the vectors that are being binded must be of equal length and mode. ## create two vectors v1 \u003c- c(1, 4, 5) v2 \u003c- c(6, 8, 10) ## create a matrix using column-bind m2 \u003c- cbind(v1, v2) m2 ### v1 v2 ### [1,] 1 6 ### [2,] 4 8 ### [3,] 5 10 ## create a matrix using row-bind m3 \u003c- rbind(v1, v2) m3 ### [,1] [,2] [,3] ### v1 1 4 5 ### v2 6 8 10 We can also use cbind() and rbind() functions to add onto matrices. v3 \u003c- c(9, 8, 7) m4 \u003c- rbind(m3, v3) m4 ### [,1] [,2] [,3] ### v1 1 4 5 ### v2 6 8 10 ### v3 9 8 7 We can add names to the rows and columns of a matrix using rownames and colnames. Let’s add row names as subject1, subject2, and subject3 and column names as var1, var2, and var3 for m4: ## add row names to m4 rownames(m4) \u003c- c(\"subject1\", \"subject2\", \"subject3\") ## add column names to m4 colnames(m4) \u003c- c(\"var1\", \"var2\", \"var3\") ## check attributes attributes(m4) ### $dim ### [1] 3 3 ### ### $dimnames ### $dimnames[[1]] ### [1] \"subject1\" \"subject2\" \"subject3\" ### ### $dimnames[[2]] ### [1] \"var1\" \"var2\" \"var3\" In order to subset matrices we use the [ operator. As matrices have two dimensions, we need to incorporate subsetting arguments for both row and column dimensions. A generic form of matrix subsetting looks like: matrix [rows, columns]. We can illustrate it using matrix m4: m4 ### var1 var2 var3 ### subject1 1 4 5 ### subject2 6 8 10 ### subject3 9 8 7 ## take the value in the first row and second column m4[1,2] ### [1] 4 ## subset for rows 1 and 2 but keep all columns m4[1:2, ] ### var1 var2 var3 ### subject1 1 4 5 ### subject2 6 8 10 ## subset for columns 1 and 3 but keep all rows m4[ , c(1, 3)] ### var1 var3 ### subject1 1 5 ### subject2 6 10 ### subject3 9 7 ## subset for both rows and columns m4[1:2, c(1, 3)] ### var1 var3 ### subject1 1 5 ### subject2 6 10 ## use column names to subset m4[ , \"var1\"] ### subject1 subject2 subject3 ### 1 6 9 ## use row names to subset m4[\"subject1\" , ] ### var1 var2 var3 ### 1 4 5 ","date":"2021-03-05","objectID":"/datapreprocessing/:3:9","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Data Frames A data frame is the most common way of storing data in R and, generally, is the data structure most often used for data analyses. A data frame is a list of equal-length vectors and they can store different classes of objects in each column (i.e., numeric, character, factor). Data frames are usually created by importing/reading in a data set using the functions covered in Module 2. However, data frames can also be created explicitly with the data.frame() function or they can be coerced from other types of objects like lists. In the following example, we will create a simple data frame df1 and assess its basic structure: ## create a data frame using data.frame() df1 \u003c- data.frame (col1 = 1:3, col2 = c (\"credit\", \"debit\", \"Paypal\"), col3 = c (TRUE, FALSE, TRUE), col4 = c (25.5, 44.2, 54.9)) ## inspect its structure str(df1) ### 'data.frame': 3 obs. of 4 variables: ### $ col1: int 1 2 3 ### $ col2: chr \"credit\" \"debit\" \"Paypal\" ### $ col3: logi TRUE FALSE TRUE ### $ col4: num 25.5 44.2 54.9 In the example above, col2 is character. If we set stringsAsFactors = TRUE, then this variable will be converted to a column of factors. ## use stringsAsFactors = TRUE df1 \u003c- data.frame (col1 = 1:3, col2 = c (\"credit\", \"debit\", \"Paypal\"), col3 = c (TRUE, FALSE, TRUE), col4 = c (25.5, 44.2, 54.9), stringsAsFactors = TRUE) ## inspect its structure str(df1) ### 'data.frame': 3 obs. of 4 variables: ### $ col1: int 1 2 3 ### $ col2: Factor w/ 3 levels \"credit\",\"debit\",..: 1 2 3 ### $ col3: logi TRUE FALSE TRUE ### $ col4: num 25.5 44.2 54.9 We can add columns (variables) and rows (items) on to a data frame using cbind() and rbind() functions. Here are some examples: ## create a new vector v4 \u003c- c(\"VIC\", \"NSW\", \"TAS\") ## add a column (variable) to df1 df2 \u003c- cbind(df1, v4) Adding attributes to data frames is very similar to what we have done in matrices. We can use rownames() and colnames() functions to add the row and column names, respectively. ## add row names rownames(df2) \u003c- c(\"subj1\", \"subj2\", \"subj3\") ## add column names colnames(df2) \u003c- c(\"number\", \"card_type\", \"fraud\", \"transaction\", \"state\") ## check the structure and the attributes str(df2) ### 'data.frame': 3 obs. of 5 variables: ### $ number : int 1 2 3 ### $ card_type : Factor w/ 3 levels \"credit\",\"debit\",..: 1 2 3 ### $ fraud : logi TRUE FALSE TRUE ### $ transaction: num 25.5 44.2 54.9 ### $ state : chr \"VIC\" \"NSW\" \"TAS\" attributes(df2) ### $names ### [1] \"number\" \"card_type\" \"fraud\" \"transaction\" \"state\" ### ### $class ### [1] \"data.frame\" ### ### $row.names ### [1] \"subj1\" \"subj2\" \"subj3\" Data frames possess the characteristics of both lists and matrices. Therefore, they are subsetted with a single vector, they behave like lists and will return the selected columns with all rows and if you subset with two vectors, they behave like matrices and can be subset by row and column. Here are some examples: df2 ### number card_type fraud transaction state ### subj1 1 credit TRUE 25.5 VIC ### subj2 2 debit FALSE 44.2 NSW ### subj3 3 Paypal TRUE 54.9 TAS ## subset by row numbers, take second and third rows only df2[2:3, ] ### number card_type fraud transaction state ### subj2 2 debit FALSE 44.2 NSW ### subj3 3 Paypal TRUE 54.9 TAS ## same as above but uses row names df2[c(\"subj2\", \"subj3\"), ] ### number card_type fraud transaction state ### subj2 2 debit FALSE 44.2 NSW ### subj3 3 Paypal TRUE 54.9 TAS ## subset by column numbers, take first and forth columns only df2[, c(1,4)] ### number transaction ### subj1 1 25.5 ### subj2 2 44.2 ### subj3 3 54.9 ## same as above but uses column names df2[, c(\"number\", \"transaction\")] ### number transaction ### subj1 1 25.5 ### subj2 2 44.2 ### subj3 3 54.9 ## subset by row and column numbers df2[2:3, c(1, 4)] ### number transaction ### subj2 2 44.2 ### subj3 3 54.9 ## same as above but uses row and column names df2[c(\"subj2\", \"subj3\"), c(\"number\", \"transaction\")] ### number transaction ### subj2 2 44.2 ### subj3 3 54.9 ## subset using $: take t","date":"2021-03-05","objectID":"/datapreprocessing/:3:10","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Converting Data Types/Structures In traditional programming languages, you need to specify the type of data a given variable can contain i.e. either integer, character, string or decimal. In R, this is not necessary. R is smart enough to “guess/create” the data type based on the values provided for a variable. However, R is not that smart (thanks to that! Otherwise why we need analysts!) to guess the correct data type within the context of analysis. To illustrate this point, let’s import the banksim.csv data set containing the following variables: id: Customer ID number age: Numerical variable marital: Categorical variable with three levels (married, single, divorced where widowed counted as divorced) education: Categorical variable with three levels (primary, secondary, tertiary) job: Categorical variable containing type of jobs balance: Numerical variable, balance in the bank account day: Numerical variable, last contacted month of the day month: Categorical variable, last contacted month duration: Numerical variable, duration of the contact time library(readr) bank \u003c- read_csv(\"data/banksim.csv\") str(bank) ### tibble [15 x 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ### $ id : num [1:15] 1 2 3 4 5 6 7 8 9 10 ... ### $ age : chr [1:15] \"44\" \"88\" \"36\" \"25\u003c=\" ... ### $ marital : chr [1:15] \"married\" \"married\" \"divorced\" \"single\" ... ### $ education: chr [1:15] \"secondary\" \"secondary\" \"secondary\" \"secondary\" ... ### $ job : chr [1:15] \"blue-collar\" \"admin.\" \"blue-collar\" \"technician\" ... ### $ balance : chr [1:15] \"16178\" \"330\" \"853\" \"616\" ... ### $ day : num [1:15] 21 2 20 28 12 16 15 5 26 14 ... ### $ month : chr [1:15] \"nov\" \"dec\" \"jun\" \"jul\" ... ### $ duration : num [1:15] 297 357 15 117 54 -268 129 156 168 216 ... ### - attr(*, \"spec\")= ### .. cols( ### .. id = col_double(), ### .. age = col_character(), ### .. marital = col_character(), ### .. education = col_character(), ### .. job = col_character(), ### .. balance = col_character(), ### .. day = col_double(), ### .. month = col_character(), ### .. duration = col_double() ### .. ) The str() output reveals how R guesses the data types of each variable. Accordingly, id, day and duration are read as numeric values, and the rest are read as characters. However, according to the variable definitions given above, the correct data type for age and balance variables should be numeric (or integer). Now’s lets investigate the reason for the incorrect data types. bank ### # A tibble: 15 x 9 ### id age marital education job balance day month duration ### \u003cdbl\u003e \u003cchr\u003e \u003cchr\u003e \u003cchr\u003e \u003cchr\u003e \u003cchr\u003e \u003cdbl\u003e \u003cchr\u003e \u003cdbl\u003e ### 1 1 44 married secondary blue-collar 16178 21 nov 297 ### 2 2 88 married secondary admin. 330 2 dec 357 ### 3 3 36 divorced secondary blue-collar 853 20 jun 15 ### 4 4 25\u003c= single secondary technician 616 28 jul 117 ### 5 5 33 single secondary services 310 12 m 54 ### 6 6 37 married tertiary management 0 16 jul -268 ### 7 7 42 married tertiary management 1205 15 mar 129 ### 8 8 43 married secondary blue-collar 130 5 may 156 ### 9 9 58 married primary u 99999 26 aug 168 ### 10 10 41 married secondary admin. 3634 14 may 216 ### 11 11 0 married primary management 92 2 feb 447 ### 12 12 34 single secondary services 528D 2 sep 121 ### 13 13 28 single secondary admin. 350 19 may 5 ### 14 14 58 widowed tertiary management 136 8 jul 199 ### 15 15 34 married unknown blue-collar 41 6 may 34 As seen from the output, row 4 of age column has “\u003c=” and row 12 of balance column is “528D”, therefore these characters forced columns to be read as characters even if they have a numeric nature. A good practice is always to check the definitions of variables, understand their types within the context, and then to apply proper type conversions if they are not in the correct data type. Data type and structure conversions can be done easily using as. functions. Essentially, as. functions will convert the object to a given type (whenever possible) and is. functions will test for the given data type and","date":"2021-03-05","objectID":"/datapreprocessing/:3:11","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Long vs. wide format data A single data set can be rearranged in many ways. One of the ways is called “long format (a.k.a long layout)”. In this layout, the data set is arranged in such a way that a single subject’s information is stored in multiple rows. In the wide format (a.k.a wide layout), a single subject’s information is stored in multiple columns. The main difference between a wide layout and a long layout is that the wide layout contains all the measured information in different columns. An illustration of the same data set stored in wide vs. long format is given below: Fig1. The same data set presented in wide vs. long format In Module 4, we will see how we can convert a long format to a wide one and vice versa using R. ","date":"2021-03-05","objectID":"/datapreprocessing/:3:12","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Additional Resources and Further Reading Data Wrangling with R by Boehmke (2016) is a comprehensive source for all data types and structures in R. This book is also one the recommended texts in our course. It is available through RMIT Library. Base R cheatsheet on http://github.com/rstudio/cheatsheets/raw/master/base-r.pdf is useful for remembering commonly used functions and arguments for data types and structures in R. ","date":"2021-03-05","objectID":"/datapreprocessing/:3:13","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"References Boehmke, Bradley C. 2016. Data Wrangling with R. Springer. Wickham, Hadley. 2014. Advanced R. CRC Press. ","date":"2021-03-05","objectID":"/datapreprocessing/:3:14","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Module 4 - Tidy and Manipulate: Tidy Data Principles and Manipulating Data ","date":"2021-03-05","objectID":"/datapreprocessing/:4:0","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Overview Summary In this module, I will present Hadley Wickham’s “Tidy Data” principles (Hadley Wickham and others (2014)) and discuss the main benefits of following these principles. We will identify most common problems with messy data sets and explore the powerful tidyr package to tidy messy data sets. Lastly, we will cover the “Grammar of Data Manipulation” - the powerful dplyr package using examples. In preparation of this section, I heavily used our recommended textbooks (Boehmke (2016) and Wickham and Grolemund (2016)), R Studio’s Data wrangling with R and RStudio webinar, tidyr and dplyr reference manuals (H Wickham (2014), Wickham et al. (2017)). learning objectives The learning objectives of this module are as follows: Identify and understand the underlying tidy data principles. Identify common problems with messy data sets. Learn how to get your data into a tidy form using tidyr package tools. Learn data manipulation tasks (i.e., select, filter, arrange, join, merge) using the powerful dplyr package functions. ","date":"2021-03-05","objectID":"/datapreprocessing/:4:1","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Tidy Data Principles “Happy families are all alike; every unhappy family is unhappy in its own way.” —Leo Tolstoy “Tidy datasets are all alike, but every messy dataset is messy in its own way.” —Hadley Wickham Hadley Wickham wrote a stellar article called “Tidy Data” in Journal of Statistical Software to provide a standard way to organise data values within a dataset. In his paper, Wickham developed the framework of “Tidy Data Principles” to provide a standard and consistent way of storing data that makes transformation, visualization, and modelling easier. Along with the tidy data principles, he also developed the tidyr package, which provides a bunch of tools to help tidy up the messy data sets. In this section, I will give you a practical introduction to tidy data and the accompanying tools in the tidyr package. If you’d like to learn more about the underlying theory, you might enjoy the Tidy Data paper published in the Journal of Statistical Software. Once you’ve imported and understand the structure of your data, it is a good idea to tidy it. Tidying your data means storing it in a consistent form that matches the semantics of the data set with the way it is stored. In brief, there are three interrelated rules which make a dataset tidy (Wickham and Grolemund (2016)). In tidy data: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. The following illustration taken from Wickham and Grolemund (2016) shows these three rules visually: Fig1. Tidy data rules: variables are in columns, observations are in rows, and values are in cells (taken from Wickham and Grolemund (2016)) To demonstrate these rules, we will use a simple data set: Student Name Math English Anna 86 90 John 43 75 Catherine 80 82 In this simple data, there are in fact three variables illustrated in the following table: First variable is “Student Name”, the second is “Subject” that represents whether the subject is Maths or English, and the third one is the “Grade” information inside the data matrix. When we arrange each variable in columns and each student in a row then we will get the tidy version of the same data as follows: No Student Name Subject Grade 1 Anna Math 86 2 John Math 43 3 Catherine Math 80 4 Anna English 90 5 John English 75 6 Catherine English 82 You can see that in this format, each variable forms a column and each student forms a row: The main advantage of using tidy principles is it allows R’s vectorized nature to shine. One can extract variables in a simple, standard way. Have a look at the following illustration. Which would you rather work with? Tidy data is important because the consistent structure lets you focus on questions about the data, not fighting to get the data into the right form for different functions. ","date":"2021-03-05","objectID":"/datapreprocessing/:4:2","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Common problems with messy data sets Real data sets can, and often do, violate the three principles of tidy data. This section describes most common problems with messy datasets: Column headers are values, not variable names: A common problem is a dataset where some (or all) of the column names are not names of variables, but values of a variable. Here is an illustration of this problem: In the example above, the column names 2011, 2012, and 2013 represent values of the year variable, and each row represents three observations, not one. Multiple variables are stored in rows: The opposite of the first problem can also occur when the variables are stored in rows. In such cases, cells include the actual variables, not the observations. Here is an example: Multiple variables are stored in one column: Sometimes, one column stores the information of two or more variables. Therefore, multiple variables can be extracted from one column. Here is an illustration of this problem: In the example above, date variable actually stores three new variable information, namely; year, month, and day. Multiple columns forms a variable: You may need to combine multiple columns into a single column to form a new variable. Here is an illustration of this problem: In this example, the year, month, and day variables are given separately in the original data but assume that we need to combine these three columns into a single variable called date for the time series analysis. ","date":"2021-03-05","objectID":"/datapreprocessing/:4:3","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"The tidyr package Most messy datasets can be tidied with a small set of tools. The tidyr package is a very useful package that reshapes the layout of data sets. In the next section you will be introduced the tidyr package and its functions with examples. We will use the subset of the data contained in the World Health Organization Global Tuberculosis Report (also given in tidyr package documentation) to illustrate the functions in the tidyr package. Before loading this dataset, we need to install and load the package using: ## install the tidyr package ##install.packages(\"tidyr\") ## load the tidyr package library(tidyr) The following example shows the same data organized in four different ways (table1, table2, table3, table4a, table4b). Each dataset shows the same values of four variables, country, year, population, and cases, but each dataset organizes the values in a different way as follows: ## load the example data organized in four different ways table1 ### # A tibble: 6 x 4 ### country year cases population ### \u003cchr\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e ### 1 Afghanistan 1999 745 19987071 ### 2 Afghanistan 2000 2666 20595360 ### 3 Brazil 1999 37737 172006362 ### 4 Brazil 2000 80488 174504898 ### 5 China 1999 212258 1272915272 ### 6 China 2000 213766 1280428583 table2 ### # A tibble: 12 x 4 ### country year type count ### \u003cchr\u003e \u003cint\u003e \u003cchr\u003e \u003cint\u003e ### 1 Afghanistan 1999 cases 745 ### 2 Afghanistan 1999 population 19987071 ### 3 Afghanistan 2000 cases 2666 ### 4 Afghanistan 2000 population 20595360 ### 5 Brazil 1999 cases 37737 ### 6 Brazil 1999 population 172006362 ### 7 Brazil 2000 cases 80488 ### 8 Brazil 2000 population 174504898 ### 9 China 1999 cases 212258 ### 10 China 1999 population 1272915272 ### 11 China 2000 cases 213766 ### 12 China 2000 population 1280428583 table3 ### # A tibble: 6 x 3 ### country year rate ### * \u003cchr\u003e \u003cint\u003e \u003cchr\u003e ### 1 Afghanistan 1999 745/19987071 ### 2 Afghanistan 2000 2666/20595360 ### 3 Brazil 1999 37737/172006362 ### 4 Brazil 2000 80488/174504898 ### 5 China 1999 212258/1272915272 ### 6 China 2000 213766/1280428583 table4a ### # A tibble: 3 x 3 ### country `1999` `2000` ### * \u003cchr\u003e \u003cint\u003e \u003cint\u003e ### 1 Afghanistan 745 2666 ### 2 Brazil 37737 80488 ### 3 China 212258 213766 table4b ### # A tibble: 3 x 3 ### country `1999` `2000` ### * \u003cchr\u003e \u003cint\u003e \u003cint\u003e ### 1 Afghanistan 19987071 20595360 ### 2 Brazil 172006362 174504898 ### 3 China 1272915272 1280428583 pivot_longer() function The function pivot_longer() is an updated approach to gather() function. When column names are values instead of variables, we need to gather or in other words, we need to transform data from wide to long format. Fig2. gather() – tidyr by RStudio To illustrate gather() function, let’s have a look at the data given in table4a: table4a ### # A tibble: 3 x 3 ### country `1999` `2000` ### * \u003cchr\u003e \u003cint\u003e \u003cint\u003e ### 1 Afghanistan 745 2666 ### 2 Brazil 37737 80488 ### 3 China 212258 213766 To tidy a dataset like this, we need to gather those columns into a new pair of variables using gather() function. To describe that operation, we need three parameters: The set of columns that represent values, not variables. In this example, those are the columns 1999 and 2000. The name of the variable whose values form the column names. The argument name key stands for that variable. For this example, the key argument is year. The name of the variable whose values are spread over the cells. The argument name value stands for that, in this example value argument is the number of cases. table4a %\u003e% gather(`1999`, `2000`, key = \"year\", value = \"cases\") ### # A tibble: 6 x 3 ### country year cases ### \u003cchr\u003e \u003cchr\u003e \u003cint\u003e ### 1 Afghanistan 1999 745 ### 2 Brazil 1999 37737 ### 3 China 1999 212258 ### 4 Afghanistan 2000 2666 ### 5 Brazil 2000 80488 ### 6 China 2000 213766 OR using the pivot_longer function: table4a %\u003e% pivot_longer(names_to = \"year\", values_to = \"cases\", cols = 2:3) ### # A tibble: 6 x 3 ### country year cases ### \u003cchr\u003e \u003cchr\u003e \u003cint\u003e ### 1 Afghanistan 1","date":"2021-03-05","objectID":"/datapreprocessing/:4:4","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"The dplyr package Although there are many data manipulation packages/functions in R, most of them lack consistent coding and the ability to easily flow together. This leads to difficult-to-read nested functions and/or choppy code. Hadley Wickham developed the very popular dplyr package to make these data processing tasks more efficient along with a syntax that is consistent and easier to remember and read. The dplyr package is regarded as the “Grammar of Data Manipulation” in R and it originates from the popular plyr package, also developed by Hadley Wickham. The plyr package covers data manipulation for a range of data structures (i.e., data frames, lists, arrays) whereas dplyr is focused on data frames. In this section, I will focus on dplyr. We will cover primary functions inside dplyr for data manipulation. The full list of capabilities can be found in the dplyr reference manual. I highly recommend going through it as there are many great functions provided by dplyr that I will not cover here. I will use the nycflights13 package and the data sets to explore the basic data manipulation verbs of dplyr. First, we need to install and load the dplyr and nycflights13 packages using: ## install the dplyr package ##install.packages(\"dplyr\") ## load the dplyr package library(dplyr) ## install the nycflights13 package for the data set ##install.packages(\"nycflights13\") ## load the nycflights13 package library(nycflights13) The nycflights13 package includes five data frames containing information on airlines, airports, flights, weather, and planes that departed from New York City in 2013. The data comes from the US Bureau of Transportation Statistics. Let’s look at the nycflights13::flights data set: ## View the flights data set under the nycflights13 package nycflights13::flights ### # A tibble: 336,776 x 19 ### year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ### \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cdbl\u003e \u003cint\u003e \u003cint\u003e ### 1 2013 1 1 517 515 2 830 819 ### 2 2013 1 1 533 529 4 850 830 ### 3 2013 1 1 542 540 2 923 850 ### 4 2013 1 1 544 545 -1 1004 1022 ### 5 2013 1 1 554 600 -6 812 837 ### 6 2013 1 1 554 558 -4 740 728 ### 7 2013 1 1 555 600 -5 913 854 ### 8 2013 1 1 557 600 -3 709 723 ### 9 2013 1 1 557 600 -3 838 846 ### 10 2013 1 1 558 600 -2 753 745 ### # ... with 336,766 more rows, and 11 more variables: arr_delay \u003cdbl\u003e, ### # carrier \u003cchr\u003e, flight \u003cint\u003e, tailnum \u003cchr\u003e, origin \u003cchr\u003e, dest \u003cchr\u003e, ### # air_time \u003cdbl\u003e, distance \u003cdbl\u003e, hour \u003cdbl\u003e, minute \u003cdbl\u003e, time_hour \u003cdttm\u003e You might notice that this data frame prints differently from other data frames you might have used in the past: it only shows the first few rows and all the columns that fit on one screen. It prints differently because it’s a tibble. Tibbles are a modern take on data frames. They are slightly tweaked to work better with tidyr and dplyr (and many others). For now, you don’t need to worry about the differences (you may refer to here to learn more on tibbles. select() function When working with a large data frame, often we want to only assess specific variables. The select() function allows us to select and/or rename variables. In addition to the existing functions like : and c(), there are a number of special functions that can work inside select. Some of them are given in the following table. Functions Usage - Select everything but : Select range contains() Select columns whose name contains a character string ends_with() Select columns whose name ends with a string everything() Select every column matches() Select columns whose name matches a regular expression num_range() Select columns named x1, x2, x3, x4, x5 one_of() Select columns whose names are in a group of names starts_with() Select columns whose name starts with a character string To illustrate we will use the flights data. Let’s select year, month and day columns using: ## Select columns: year, month and day select(flights, year, month, day) ### # A tibble: 336,776 x 3 ### year mo","date":"2021-03-05","objectID":"/datapreprocessing/:4:5","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Additional Resources and Further Reading You can refer to the tidyr package manual (H Wickham (2014)) and the Tidy Data paper for a detailed information on tidy data principles and tidyr package. Our recommended textbooks (Boehmke (2016) and Wickham and Grolemund (2016)), R Studio’s Data wrangling with R and RStudio webinar, and dplyr reference manual (H Wickham (2014), Wickham et al. (2017)) are great resources to excel your knowledge in Data Manipulation with dplyr. If you are interested in improving your coding style, I would recommend using the tidyverse style guide. Note that a good coding style is like correct punctuation, you can manage without it, but a good coding style makes things easier to read for you and for the others who are gonna use your codes! ","date":"2021-03-05","objectID":"/datapreprocessing/:4:6","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"References Boehmke, Bradley C. 2016. Data Wrangling with R. Springer. Wickham, H. 2014. “Tidyr: Easily Tidy Data with Spread () and Gather () Functions. R Package.” Version 0.2. 0. Available at Http://CRAN. R-Project. Org/Package= Tidyr [Verified 7 June 2016]. Wickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O’Reilly Media, Inc.\". Wickham, Hadley, and others. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. Wickham, H, R Francois, L Henry, and K Müller. 2017. “Dplyr: A Grammar of Data Manipulation. R Package Version 0.7. 0.” URL https://CRAN. R-project. org/package= dplyr. ","date":"2021-03-05","objectID":"/datapreprocessing/:4:7","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Module 5 - Scan: Missing Values ","date":"2021-03-05","objectID":"/datapreprocessing/:5:0","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Introduction Dealing missing values is an unavoidable task in the data preprocessing. For almost every data set, we will encounter some missing values. So, it is important to know how R handles missing values and how they are represented. In this module first, you will learn how the missing values and special values are represented in R. Then, you will learn how to identify, recode and exclude missing values. Moreover, we will cover missing value imputation techniques briefly. Note that the missing value analysis and the missing value imputation are broader concepts that would be a stand-alone topic of another course. Interested readers may refer to the books and resources in the additional resources and the further reading section for further details. The analysts may also need to check and correct the obvious errors and/or inconsistencies in a data set. In this module, I also briefly introduced the deductive and deducorrect packages (in fact deducorrect is a former version of deductive package), and useful functions to correct the obvious errors and inconsistencies in a given data set. However, this part of module left as an optional reading for students. ","date":"2021-03-05","objectID":"/datapreprocessing/:5:1","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"learning objectives The learning objectives of this module are as follows: Learn how missing and special values are represented in the data set. Identify missing values in the data set. Learn how to recode missing values. Learn the functions for removing missing values. Learn commonly used approaches to impute/replace missing value(s). (Optional) Check and correct obvious inconsistencies and errors in the data set. ","date":"2021-03-05","objectID":"/datapreprocessing/:5:2","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Missing Data In R, a numeric missing value is represented by NA (NA stands for “not available”), while character missing values are represented by . In addition to NA and , some other values may represent missing values (i.e. 99, ., .., just space, or NULL) depending on the software (i.e., Excel, SPSS etc.) that you import in your data. Let’s have a look at the pet1.csv data: library(readr) pet1 \u003c- read_csv(\"https://raw.githubusercontent.com/yanboyang713/RMIT-Data-Repository/main/pet1.csv\") ### ### -- Column specification -------------------------------------------------------- ### cols( ### id = col_double(), ### State = col_character(), ### Region = col_character(), ### Reference = col_double(), ### Animal_Name = col_character(), ### Colour_primary = col_character() ### ) head(pet1) ### # A tibble: 6 x 6 ### id State Region Reference Animal_Name Colour_primary ### \u003cdbl\u003e \u003cchr\u003e \u003cchr\u003e \u003cdbl\u003e \u003cchr\u003e \u003cchr\u003e ### 1 118269 Victoria Ballarat NA Jack Wilson Brown ### 2 106347 Victoria Ballarat NA Eva Black And White ### 3 156347 Victoria Wyndham NA \u003cNA\u003e TRI ### 4 63947 Victoria Geelong NA Archie White/Brown ### 5 79724 Victoria Ballarat NA Susie Brown ### 6 43442 Victoria Geelong NA Pearl Tri Colour Note that, as we read this data from a .csv file, missing values are represented as NA for the integer reference variable where else for the character Animal_Name variable. However, let’s look at another example SPSS data file named population_NA.sav: library(foreign) population_NA \u003c- read.spss(\"https://raw.githubusercontent.com/yanboyang713/RMIT-Data-Repository/main/population_NA.sav\", to.data.frame = TRUE) ### re-encoding from UTF-8 population_NA ### Region X.2013 X.2014 X.2015 X.2016 ### 1 ISL 3.21 3.25 3.28 3.32 ### 2 CAN 3.87 3.91 3.94 3.99 ### 3 RUS 7.83 7.85 7.87 .. ### 4 COL 41.27 41.74 NA .. ### 5 ZAF 43.53 44.22 NA .. ### 6 LTU 47.42 46.96 46.63 46.11 ### 7 MEX 60.43 61.10 61.76 62.41 ### 8 IND 394.85 NA NA .. ### 9 NLD 497.64 499.59 501.68 504.01 ### 10 KOR 504.92 506.97 508.91 510.77 As you see in the data frame, there are two different representations for the missing values: one is NA, the other is .. . Therefore, we need to be careful about different representations of the missing values while importing the data from other software. ","date":"2021-03-05","objectID":"/datapreprocessing/:5:3","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Identifying Missing Data To identify missing values we will use is.na() function which returns a logical vector with TRUE in the element locations that contain missing values represented by NA. is.na() will work on vectors, lists, matrices, and data frames. Here are some examples of is.na() function: ## create a vector with missing data x \u003c- c(1:4, NA, 6:7, NA) x ### [1] 1 2 3 4 NA 6 7 NA is.na(x) ### [1] FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE ## create a data frame with missing data df \u003c- data.frame(col1 = c(1:3, NA), col2 = c(\"this\", NA,\"is\", \"text\"), col3 = c(TRUE, FALSE, TRUE, TRUE), col4 = c(2.5, 4.2, 3.2, NA)) ## identify NAs in full data frame is.na(df) ### col1 col2 col3 col4 ### [1,] FALSE FALSE FALSE FALSE ### [2,] FALSE TRUE FALSE FALSE ### [3,] FALSE FALSE FALSE FALSE ### [4,] TRUE FALSE FALSE TRUE ## identify NAs in specific data frame column is.na(df$col4) ### [1] FALSE FALSE FALSE TRUE To identify the location or the number of NAs we can use the which() and sum() functions: ## identify location of NAs in vector which(is.na(x)) ### [1] 5 8 ## identify count of NAs in data frame sum(is.na(df)) ### [1] 3 More convenient way to compute the total missing values in each column is to use colSums(): colSums(is.na(df)) ### col1 col2 col3 col4 ### 1 1 0 1 ","date":"2021-03-05","objectID":"/datapreprocessing/:5:4","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Recode Missing Data We can use normal subsetting and assignment operations in order to recode missing values; or recode specific indicators that represent missing values. For instance, we can recode missing values in vector x with the mean values in x. To do this, first we need to subset the vector to identify NAs and then assign these elements a value. Here is an example: ## create vector with missing data x \u003c- c(1:4, NA, 6:7, NA) x ### [1] 1 2 3 4 NA 6 7 NA ## recode missing values with the mean (also see \"Missing Value Imputation Techniques\" section) x[is.na(x)] \u003c- mean(x, na.rm = TRUE) x ### [1] 1.000000 2.000000 3.000000 4.000000 3.833333 6.000000 7.000000 3.833333 Similarly, if missing values are represented by another value (i.e. ..) we can simply subset the data for the elements that contain that value and then assign a desired value to those elements. Remember that population_NA data frame has missing values represented by “..” in the X.2016 column. Now let’s change “..” values to NA’s. ## population_NA data frame has missing values represented by \"..\" in the X.2016 column. population_NA$X.2016 ### [1] \"3.32 \" \"3.99 \" \".. \" \".. \" \".. \" \"46.11 \" \"62.41 \" ### [8] \".. \" \"504.01 \" \"510.77 \" ## Note the white spaces after ..'s and change \".. \" values to NAs population_NA[population_NA == \".. \" ] \u003c- NA population_NA Or, population_NA[which(population_NA == \".. \" )] \u003c- NA ### Region X.2013 X.2014 X.2015 X.2016 ### 1 ISL 3.21 3.25 3.28 3.32 ### 2 CAN 3.87 3.91 3.94 3.99 ### 3 RUS 7.83 7.85 7.87 \u003cNA\u003e ### 4 COL 41.27 41.74 NA \u003cNA\u003e ### 5 ZAF 43.53 44.22 NA \u003cNA\u003e ### 6 LTU 47.42 46.96 46.63 46.11 ### 7 MEX 60.43 61.10 61.76 62.41 ### 8 IND 394.85 NA NA \u003cNA\u003e ### 9 NLD 497.64 499.59 501.68 504.01 ### 10 KOR 504.92 506.97 508.91 510.77 If we want to recode missing values in a single data frame variable, we can subset for the missing value in that specific variable of interest and then assign it the replacement value. For example, in the following example, we will recode the missing value in col4 with the mean value of col4. ## data frame with missing data df \u003c- data.frame(col1 = c(1:3, NA), col2 = c(\"this\", NA,\"is\", \"text\"), col3 = c(TRUE, FALSE, TRUE, TRUE), col4 = c(2.5, 4.2, 3.2, NA)) ## recode the missing value in col4 with the mean value of col4 df$col4[is.na(df$col4)] \u003c- mean(df$col4, na.rm = TRUE) df ### col1 col2 col3 col4 ### 1 1 this TRUE 2.5 ### 2 2 \u003cNA\u003e FALSE 4.2 ### 3 3 is TRUE 3.2 ### 4 NA text TRUE 3.3 Note that, replace_na() function from tidyr package can also be used to replace NA values. For more information and examples see here. ## Replace NAs in a data frame df \u003c- tibble(x = c(1, 2, NA), y = c(\"a\", NA, \"b\")) df %\u003e% replace_na(list(x = 0, y = \"unknown\")) ##\u003e # A tibble: 3 x 2 ##\u003e x y ##\u003e \u003cdbl\u003e \u003cchr\u003e ##\u003e 1 1 a ##\u003e 2 2 unknown ##\u003e 3 0 b ## Replace NAs in a vector df %\u003e% dplyr::mutate(x = replace_na(x, 0)) ##\u003e # A tibble: 3 x 2 ##\u003e x y ##\u003e \u003cdbl\u003e \u003cchr\u003e ##\u003e 1 1 a ##\u003e 2 2 NA ##\u003e 3 0 b ## OR df$x %\u003e% replace_na(0) ##\u003e [1] 1 2 0 df$y %\u003e% replace_na(\"unknown\") ##\u003e [1] \"a\" \"unknown\" \"b\" ## Replace NULLs in a list: NULLs are the list-col equivalent of NAs df_list \u003c- tibble(z = list(1:5, NULL, 10:20)) df_list %\u003e% replace_na(list(z = list(5))) ##\u003e # A tibble: 3 x 1 ##\u003e z ##\u003e \u003clist\u003e ##\u003e 1 \u003cint [5]\u003e ##\u003e 2 \u003cdbl [1]\u003e ##\u003e 3 \u003cint [11]\u003e ","date":"2021-03-05","objectID":"/datapreprocessing/:5:5","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Excluding Missing Data A common method of handling missing values is simply to omit the records or fields with missing values from the analysis. However, this may be dangerous, since the pattern of missing values may in fact be systematic, and simply deleting records with missing values would lead to a biased subset of the data. Some authors recommend that if the amount of missing data is very small relatively to the size of the data set (up to 5%), then leaving out the few values with missing features would be the best strategy in order not to bias the analysis. When this is the case, we can exclude missing values in a couple different ways. If we want to exclude missing values from mathematical operations, we can use the na.rm = TRUE argument. If you do not exclude these values, most functions will return an NA. Here are some examples: ## create a vector with missing values x \u003c- c(1:4, NA, 6:7, NA) ## including NA values will produce an NA output when used with mathematical operations mean(x) ### [1] NA ## excluding NA values will calculate the mathematical operation for all non-missing values mean(x, na.rm = TRUE) ### [1] 3.833333 We may also want to subset our data to obtain complete observations (those observations in our data that contain no missing data). We can do this a few different ways. ## data frame with missing values df \u003c- data.frame(col1 = c(1:3, NA), col2 = c(\"this\", NA,\"is\", \"text\"), col3 = c(TRUE, FALSE, TRUE, TRUE), col4 = c(2.5, 4.2, 3.2, NA)) df ### col1 col2 col3 col4 ### 1 1 this TRUE 2.5 ### 2 2 \u003cNA\u003e FALSE 4.2 ### 3 3 is TRUE 3.2 ### 4 NA text TRUE NA First, to find complete cases we can leverage the complete.cases() function which returns a logical vector identifying rows which are complete cases. So, in the following case rows 1 and 3 are complete cases. We can use this information to subset our data frame which will return the rows which complete.cases() found to be TRUE. complete.cases(df) ### [1] TRUE FALSE TRUE FALSE ## subset data frame with complete.cases to get only complete cases df[complete.cases(df), ] ### col1 col2 col3 col4 ### 1 1 this TRUE 2.5 ### 3 3 is TRUE 3.2 ## or subset with `!` operator to get incomplete cases df[!complete.cases(df), ] ### col1 col2 col3 col4 ### 2 2 \u003cNA\u003e FALSE 4.2 ### 4 NA text TRUE NA A shorthand alternative approach is to simply use na.omit() to omit all rows containing missing values. ## or use na.omit() to get same as above na.omit(df) ### col1 col2 col3 col4 ### 1 1 this TRUE 2.5 ### 3 3 is TRUE 3.2 However, it seems like a waste to omit the information in all the other fields just because one field value is missing. Therefore, data analysts should carefully approach to excluding missing values especially when the amount of missing data is very large. Another recommended approach is to replace the missing value with a value substituted according to various criteria. These approaches will be given in the next section. ","date":"2021-03-05","objectID":"/datapreprocessing/:5:6","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Basic Missing Value Imputation Techniques Imputation is the process of estimating or deriving values for fields where data is missing. There is a vast body of literature on imputation methods and it goes beyond the scope of this course to discuss all of them. In this section I will provide basic missing value imputation techniques only. Replace the missing value(s) with some constant, specified by the analyst In some cases, a missing value can be determined because the observed values combined with their constraints force a unique solution. As an example, consider the following data frame listing the costs for staff, cleaning, housing and the total total for three months. df \u003c- data.frame(month = c(1:3), staff = c(15000 , 20000, 23000), cleaning = c(100, NA, 500), housing = c(300, 200, NA), total = c(NA, 20500, 24000)) df ### month staff cleaning housing total ### 1 1 15000 100 300 NA ### 2 2 20000 NA 200 20500 ### 3 3 23000 500 NA 24000 Now, assume that we have the following rules for the calculation of total cost: staff + cleaning + housing = total and all costs \u003e 0. Therefore, if one of the variables is missing, we can clearly derive the missing values by solving the rule. For this example, first month’s total cost can be found as 15000 + 100 + 300 = 15400. Other missing values can be found in a similar way. The deducorrect and validate packages have a number of functions available that can impute (and correct) the values according to the given rules automatically for a given data frame. ##install.packages(\"deductive\") ##install.packages(\"validate\") library(deductive) library(validate) ## Define the rules as a validator expression Rules \u003c- validator( staff + cleaning + housing == total, staff \u003e= 0, housing \u003e= 0, cleaning \u003e= 0) ## Use impute_lr function imputed_df \u003c- impute_lr(df,Rules) imputed_df ### month staff cleaning housing total ### 1 1 15000 100 300 15400 ### 2 2 20000 300 200 20500 ### 3 3 23000 500 500 24000 The deducorrect package together with validate provide a collection of powerful methods for automated data cleaning and imputing. For more information on these packages please refer to “Correction of Obvious Inconsistencies and Errors” section of the module notes and the deducorrect package manual and validate package manual. Replace the missing value(s) with the mean, median or mode Replacing the missing value with the mean, median (for numerical variables) or the mode (for categorical variables) is a crude way of treating missing values. The Hmisc package has a convenient wrapper function allowing you to specify what function is used to compute imputed values from the non-missing. Consider the following data frame with missing values: x \u003c- data.frame( no = c(1:6), x1 = c(15000 , 20000, 23000, NA, 18000, 21000), x2 = c(4, NA, 4, 5, 7, 8), x3 = factor(c(NA, \"False\", \"False\", \"False\", \"True\", \"True\"))) x ### no x1 x2 x3 ### 1 1 15000 4 \u003cNA\u003e ### 2 2 20000 NA False ### 3 3 23000 4 False ### 4 4 NA 5 False ### 5 5 18000 7 True ### 6 6 21000 8 True For this data frame, imputation of the mean, median and mode can be done using Hmisc package as follows: ##install.packages(\"Hmisc\") library(Hmisc) ## mean imputation (for numerical variables) x1 \u003c- impute(x$x1, fun = mean) x1 ### 1 2 3 4 5 6 ### 15000 20000 23000 19400* 18000 21000 ## median imputation (for numerical variables) x2 \u003c- impute(x$x2, fun = median) x2 ### 1 2 3 4 5 6 ### 4 5* 4 5 7 8 ## mode imputation (for categorical/factor variables) x3 \u003c- impute(x$x3, fun= mode) x3 ### 1 2 3 4 5 6 ### False* False False False True True A nice feature of the impute function is that the resulting vector remembers what values were imputed. This information may be requested with is.imputed function as in the example below. ## check which values are imputed is.imputed(x1) ### [1] FALSE FALSE FALSE TRUE FALSE FALSE is.imputed(x2) ### [1] FALSE TRUE FALSE FALSE FALSE FALSE is.imputed(x3) ### [1] TRUE FALSE FALSE FALSE FALSE FALSE More Complex Approaches to Missing Value Imputati","date":"2021-03-05","objectID":"/datapreprocessing/:5:7","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Special values In addition to missing values, there are a few special values that are used in R. These are -Inf, Inf and NaN. If a computation results in a number that is too big, R will return Inf (meaning positive infinity) for a positive number and -Inf for a negative number (meaning negative infinity). Here are some examples: 3 ^ 1024 ### [1] Inf -3 ^ 1024 ### [1] -Inf This is also the value returned when you divide by 0: 12 / 0 ### [1] Inf Sometimes, a computation will produce a result that makes little sense. In these cases, R will often return NaN (meaning “not a number”): Inf - Inf ### [1] NaN 0/0 ### [1] NaN ","date":"2021-03-05","objectID":"/datapreprocessing/:5:8","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Identifying Special Values Calculations involving special values often result in special values, thus it is important to handle special values prior to analysis. The is.finite, is.infinite, or is.nan functions will generate logical values (TRUE or FALSE) and they can be used to identify the special values in a data set. ## create a vector with special values m \u003c- c( 2, 0/0, NA, 1/0, -Inf, Inf, (Inf*2) ) m ### [1] 2 NaN NA Inf -Inf Inf Inf ## check finite values is.finite(m) ### [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## check infinite (-inf or +inf) values is.infinite(m) ### [1] FALSE FALSE FALSE TRUE TRUE TRUE TRUE ## check not a number (NaN) values is.nan(m) ### [1] FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## create a data frame containing special values df \u003c- data.frame(col1 = c( 2, 0/0, NA, 1/0, -Inf, Inf), col2 = c( NA, Inf/0, 2/0, NaN, -Inf, 4)) df ### col1 col2 ### 1 2 NA ### 2 NaN Inf ### 3 NA Inf ### 4 Inf NaN ### 5 -Inf -Inf ### 6 Inf 4 ## check whether dataframe has infinite (-inf or +inf) values is.infinite(df) ## Error in is.infinite(df) : default method not implemented for type 'list' These functions accept vectorial input, therefore you will receive an error when you try to use it with a data frame. In such cases, we can use apply family functions. Remark: Useful apply family functions The apply family functions will apply a specified function to a given data object (e.g. vectors, lists, matrices, data frames). Most common forms of apply functions are: apply() for matrices and data frames lapply() for lists (output as list) sapply() for lists (output simplified) tapply() for vectors … many others There is a very useful and comprehensive tutorial on apply family functions in DataCamp. Please read this tutorial for more information on the usage of apply family. You can also use swirl() to practice apply family functions. In swirl(), “R Programming Course” Lesson 10 and 11 cover the apply family functions. Among those apply functions, sapply and lapply functions can apply any function to a list. Remember from Module 3 that data frames possess the characteristics of both lists and matrices. Therefore, we can use sapply or lappy for data frames. Now for the previous example, let’s use sapply function: df ### col1 col2 ### 1 2 NA ### 2 NaN Inf ### 3 NA Inf ### 4 Inf NaN ### 5 -Inf -Inf ### 6 Inf 4 ## check whether dataframe has infinite (-inf or +inf) values using sapply sapply(df, is.infinite) ### col1 col2 ### [1,] FALSE FALSE ### [2,] FALSE TRUE ### [3,] FALSE TRUE ### [4,] TRUE FALSE ### [5,] TRUE TRUE ### [6,] TRUE FALSE By using sapply we could see the infinite values in the data frame. Now remember that is.infinite function doesn’t check for NaN numbers. Therefore, we also need to check the data frame for NaN values. In order to do that, we can write a simple function to check every numerical column in a data frame for infinite values or NaN’s. ## data frame df ### col1 col2 ### 1 2 NA ### 2 NaN Inf ### 3 NA Inf ### 4 Inf NaN ### 5 -Inf -Inf ### 6 Inf 4 ## Check every numerical column whether they have infinite or NaN values using a function called is.special is.special \u003c- function(x){ if (is.numeric(x)) (is.infinite(x) | is.nan(x)) } ## apply this function to the data frame. sapply(df, is.special) ### col1 col2 ### [1,] FALSE FALSE ### [2,] TRUE TRUE ### [3,] FALSE TRUE ### [4,] TRUE TRUE ### [5,] TRUE TRUE ### [6,] TRUE FALSE Here, the is.special function is applied to each column of df using sapply. is.special checks the data frame for numerical special values if the type is numeric. Using a similar approach you can also check for special values or NA’s at the same time using: ## Check every numerical column whether they have infinite or NaN or NA values using a function called is.specialorNA is.specialorNA \u003c- function(x){ if (is.numeric(x)) (is.infinite(x) | is.nan(x) | is.na(x)) } ## apply this function to the data frame. sapply(df, is.specialorNA) ### col1 col2 ### [1,] FALSE TRUE ### [2,] TRUE TRUE ","date":"2021-03-05","objectID":"/datapreprocessing/:5:9","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Optional Reading: Checking for Obvious Inconsistencies or Errors An obvious inconsistency occurs when a data record contains a value or combination of values that cannot correspond to a real-world situation. For example, a person’s age cannot be negative, a man cannot be pregnant, and an under-aged person cannot possess a drivers’ license. Such knowledge can be expressed as rules or constraints. In data preprocessing literature these rules are referred to as edit rules or edits, in short. Checking for obvious inconsistencies can be done straightforwardly in R using logical indices. For example, to check which elements of x obey the rule: “x must be non-negative” one can simply use the following. ## create a vector called x x \u003c- c( 0, -2, 1, 5) ## check the non-negative elements x_nonnegative \u003c- (x \u003e= 0) x_nonnegative ### [1] TRUE FALSE TRUE TRUE However, as the number of variables increases, the number of rules may increase, and it may be a good idea to manage the rules separate from the data. For such cases, the editrules package allows us to define rules on categorical, numerical or mixed-type data sets which each record must obey. Furthermore, editrules can check which rules are obeyed or not and allows one to find the minimal set of variables to adapt so that all rules can be obeyed. This package also implements several basic rule operations allowing users to test rule sets for contradictions and certain redundancies. To illustrate I will use a small data set (datawitherrors.csv) given below: datawitherrors \u003c- read.csv(\"https://raw.githubusercontent.com/yanboyang713/RMIT-Data-Repository/main/datawitherrors.csv\") datawitherrors ### ï..no age agegroup height status yearsmarried ### 1 1 21 adult 178 single -1 ### 2 2 2 child 147 married 0 ### 3 3 18 adult 167 married 20 ### 4 4 221 elderly 154 widowed 2 ### 5 5 34 child -174 married 3 As you noticed, there are many inconsistencies/errors in this small data set (i.e., age = 221, height = -174, years married = -1, etc.) . To begin with a simple case, let’s define a restriction on the age variable using editset functions. In order to use editset functions, we need to install and load the editrules package. ##install.packages(\"editrules\") library(editrules) In the first rule, we will define the restriction on the age variable as $ 0 age 150 $ using editset function. (Rule1 \u003c- editset(c(\"age \u003e= 0\", \"age \u003c= 150\"))) ### ### Edit set: ### num1 : 0 \u003c= age ### num2 : age \u003c= 150 The editset function parses the textual rules and stores them in an editset object. Each rule is assigned a name according to its type (numeric, categorical, or mixed) and a number. The data set can be checked against these rules using the violatedEdits function. violatedEdits(Rule1, datawitherrors) ### edit ### record num1 num2 ### 1 FALSE FALSE ### 2 FALSE FALSE ### 3 FALSE FALSE ### 4 FALSE TRUE ### 5 FALSE FALSE violatedEdits returns a logical array indicating for each row of the data, which rules are violated. From the output, it can be understood that the 4th record violates the second rule (age \u003c= 150). One can also read rules, directly from a text file using the editfile function. As an example, consider the contents of the following text file (also available here): 1 # numerical rules 2 age \u003e= 0 3 height \u003e 0 4 age \u003c= 150 5 age \u003e yearsmarried 6 7 # categorical rules 8 status %in% c(“married”,“single”,“widowed”) 9 agegroup %in% c(“child”,“adult”,“elderly”) 10 if ( status == “married” ) agegroup %in% c(“adult”,“elderly”) 11 12 # mixed rules 13 if ( status %in% c(“married”,“widowed”)) age - yearsmarried \u003e= 17 14 if ( age \u003c 18 ) agegroup == “child” 15 if ( age \u003e= 18 \u0026\u0026 age \u003c65 ) agegroup == “adult” 16 if ( age \u003e= 65 ) agegroup == “elderly” These rules are numerical, categorical and mixed (both data types). Comments are written behind the usual # character. The rule set can be read using editfile function as follows: Rules \u003c- editfile(\"data/editrules.txt\", type = \"all\") Rules ### ### Data model: ### dat6 : ag","date":"2021-03-05","objectID":"/datapreprocessing/:5:10","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Optional Reading: Correction of Obvious Inconsistencies or Errors When the data you are analysing is generated by people rather than machines or measurement devices, certain typical human-generated errors are likely to occur. Given that data must obey certain edit rules, the occurrence of such errors can sometimes be detected from raw data with (almost) certainty. Examples of errors that can be detected are typing errors in numbers, rounding errors in numbers, and sign errors. The deducorrect package has several functions available that can correct such errors. Consider the following data frame (datawitherrors2.csv): datawitherrors2 \u003c- read.csv(\"https://raw.githubusercontent.com/yanboyang713/RMIT-Data-Repository/main/datawitherrors2.csv\") datawitherrors2 ### no height unit ### 1 1 178.00 cm ### 2 2 1.47 m ### 3 3 70.00 inch ### 4 4 154.00 cm ### 5 5 5.92 ft The task here is to standardise the lengths and express all of them in meters. The deducorrect package can correct this inconsistency using correctionRules function. For example, to perform the above task, one first specifies a file with correction rules as follows. 1 # convert centimeters 2 if ( unit == “cm” ){ 3 height \u003c- height/100 4 } 5 # convert inches 6 if (unit == “inch” ){ 7 height \u003c- height/39.37 8 } 9 # convert feet 10 if (unit == “ft” ){ 11 height \u003c- height/3.28 12 } 13 # set all units to meter 14 unit \u003c- “m” With correctionRules we can read these rules from the txt file using .file argument. ##install.packages(\"deducorrect\") library(deducorrect) ## read rules from txt file using validate Rules2 \u003c- correctionRules(\"data/editrules2.txt\") Rules2 ### Object of class 'correctionRules' ### ## 1------- ### if (unit == \"cm\") height \u003c- height/100 ### ## 2------- ### if (unit == \"inch\") height \u003c- height/39.37 ### ## 3------- ### if (unit == \"ft\") height \u003c- height/3.28 ### ## 4------- ### unit \u003c- \"m\" Now, we can apply them to the data frame and obtain a log of all actual changes as follows: cor \u003c- correctWithRules(Rules2, datawitherrors2) cor ### $corrected ### no height unit ### 1 1 1.780000 m ### 2 2 1.470000 m ### 3 3 1.778004 m ### 4 4 1.540000 m ### 5 5 1.804878 m ### ### $corrections ### row variable old new how ### 1 1 height 178 1.78 if (unit == \"cm\") height \u003c- height/100 ### 2 1 unit cm m unit \u003c- \"m\" ### 3 3 height 70 1.778004 if (unit == \"inch\") height \u003c- height/39.37 ### 4 3 unit inch m unit \u003c- \"m\" ### 5 4 height 154 1.54 if (unit == \"cm\") height \u003c- height/100 ### 6 4 unit cm m unit \u003c- \"m\" ### 7 5 height 5.92 1.804878 if (unit == \"ft\") height \u003c- height/3.28 ### 8 5 unit ft m unit \u003c- \"m\" The returned value, cor$corrected will give a list containing the corrected data as follows: cor$corrected ### no height unit ### 1 1 1.780000 m ### 2 2 1.470000 m ### 3 3 1.778004 m ### 4 4 1.540000 m ### 5 5 1.804878 m ","date":"2021-03-05","objectID":"/datapreprocessing/:5:11","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Additional Resources and Further Reading As mentioned before, the missing value analysis and the missing value imputation are broader concepts that would be a standalone topic of another course. Interested readers may refer to the “Statistical analysis with missing data (Little and Rubin (2014))” and “Flexible imputation of missing data (Van Buuren (2012))” for the theory behind the missing value mechanism and analysis. There are many good R tutorials for handling missing data using R. “Missing Data: To impute or note to impute?” and “Data Science Live Book” are only two of them. Moreover, the missForest and mice packages’ manuals provide detailed information on the missing value imputation using random forest algorithm and multiple imputation techniques, respectively. For checking and correcting errors and inconsistencies in the data, users can refer to the deducorrect , deductive and validate packages’ manuals and “An introduction to data cleaning with R (De Jonge and Loo (2013))” discussion paper. ","date":"2021-03-05","objectID":"/datapreprocessing/:5:12","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"References De Jonge, Edwin, and Mark van der Loo. 2013. “An Introduction to Data Cleaning with R.” Heerlen: Statistics Netherlands. Little, Roderick JA, and Donald B Rubin. 2014. Statistical Analysis with Missing Data. Vol. 333. John Wiley \u0026 Sons. Van Buuren, Stef. 2012. Flexible Imputation of Missing Data. CRC press. ","date":"2021-03-05","objectID":"/datapreprocessing/:5:13","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Module 6 - Scan: Outliers ","date":"2021-03-05","objectID":"/datapreprocessing/:6:0","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Overview Summary In statistics, an outlier is defined as an observation which stands far away from the most of other observations. An outlier can be a result of a measurement error and the inclusion of that error would have a great impact on the analysis results. Therefore, every data set should be scanned for possible outliers before conducting any statistical analysis. Like missing values, detecting outliers and dealing with them are also crucial in the data preprocessing. In this module, first you will learn how to identify univariate and multivariate outliers using descriptive, graphical and distance-based methods. Then, you will learn different approaches to deal with these values using R. Learning Objectives The learning objectives of this module are as follows: Identify the outlier(s) in the data set. Apply descriptive methods to identify univariate outliers. Apply graphical approaches to scan for univariate or multivariate outliers. Apply distance-based metrics to identify univariate or multivariate outliers. Learn commonly used approaches to handle outliers. ","date":"2021-03-05","objectID":"/datapreprocessing/:6:1","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Outliers In statistics, an outlier is defined as an observation which stands far away from most of the other observations. An outlier deviates so much from other observations as to arouse suspicion that is was generated by a different mechanism (Hawkins (1980)). Let’s take an example. Assume that we are interested in customer profiling and we find out that the average annual income of our customers is 800K. But there are two customers having annual income of 3 and 3.2 million dollars. These two customers’ annual income is much higher than rest of the customers (see the boxplot below). These two observations will be seen as outliers. Types of Outliers Outlier can be univariate and multivariate. Univariate outliers can be found when looking at a distribution of values in a single variable. The example given above is an example of a univariate outlier as we only look at the distribution of income (i.e., one variable) among our customers. On the other hand, multivariate outliers can be found in a n-dimensional space (of n-variables). In order to find them, we need to look at distributions in multi-dimensions. To illustrate multivariate outliers, let’s assume that we are interested in understanding the relationship between height and weight. Below, we have univariate and bivariate distribution for Height and Weight. When we look at the univariate distributions of Height and Weight (i.e., using boxplots) separately, we don’t spot any abnormal cases (i.e. above and below the $$ 1.5 × IQR $$ fence). However, when we look at the bivariate (two dimensional) distribution of Height and Weight (using scatter plot), we can see that we have one observation whose weight is 45.19 kg and height is 185.09 (on the upper-left side of the scatter plot). This observation is far away from most of the other weight and height combinations thus, will be seen as a multivariate outlier. Most Common Causes of Outliers Often an outlier can be a result of data entry errors, measurement errors, experimental errors, intentional errors, data processing errors or due to the sampling (i.e., sampling error). The following are the most common causes of outliers (taken from: https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/) Data Entry Error: Outliers can arise because of the human errors during data collection, recording, or entry. Measurement Error: It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty. Experimental Error: Another cause of outliers is the experimental error. Experimental errors can arise during data extraction, experiment/survey planning and executing errors. Intentional Error: This type of outlier is commonly found in self-reported measures that involves sensitive data. For example, teens would typically under report the amount of alcohol that they consume. Only a fraction of them would report actual value. Here actual values might look like outliers because rest of the teens are under reporting the consumption. Data Processing Error: Often, due to the data sets are extracted from multiple sources, it is possible that some manipulation or extraction errors may lead to outliers in the data set. Sampling Error: Sometimes, outliers can arise due to the sampling (taking samples from population) process. Typically, this type of outliers can be seen when we take a few observations as a sample. ","date":"2021-03-05","objectID":"/datapreprocessing/:6:2","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Detecting Outliers Outliers can drastically change the results of the data analysis and statistical modelling. Some of the unfavourable impacts of outliers are as follows: They increase the error variance. They reduce the power of statistical tests. They can bias or influence the estimates of model parameters that may be of substantive interest. Therefore, one of the most important tasks in data preprocessing is to identify and properly handle the outliers. There are many methods developed for outlier detection. Majority of them deal with numerical data. This module will introduce the most basic ones with their application using R packages. Univariate Outlier Detection Methods One of the simplest methods for detecting univariate outliers is the use of boxplots. A boxplot is a graphical display for describing the distribution of the data using the median, the first (Q1) and third quartiles (Q3), and the inter-quartile range (IQR = Q3 − Q1). Below is an illustration of a typical boxplot (taken from [Dr. James Baglin’s Intro to Stats website] (https://astral-theory-157510.appspot.com/secured/MATH1324_Module_02.html#box_plots)). In the boxplot, the “Tukey’s method of outlier detection” is used to detect outliers. According to this method, outliers are defined as the values in the data set that fall beyond the range of $$ −1.5 × IQR $$ to $$ 1.5 × IQR $$. These $$ −1.5 × IQR $$ and $$ 1.5 × IQR $$ limits are called “outlier fences” and any values lying outside the outlier fences are depicted using an “o” or a similar symbol on the boxplot. Note that Tukey’s method is a nonparametric way of detecting outliers, therefore it is mainly used to test outliers in non-symmetric / non-normal data distributions. In order to illustrate the boxplot, we will use the Diamonds.csv data set available under the data repository. Diamonds \u003c- read.csv(\"https://raw.githubusercontent.com/yanboyang713/RMIT-Data-Repository/main/Diamonds.csv\") head(Diamonds) ## carat cut color clarity depth table price x y z ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.20 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 We can use boxplot() function (under Base graphics) to get the boxplot of the carat variable: Diamonds$carat %\u003e% boxplot(main=\"Boxplot of Diamond Carat\", ylab=\"Carat\", col = \"grey\") According to the Tukey’s method, the carat variable seems to have many outliers. In the next section, we will discuss different approaches to handle these outliers. There are also distance based methods to detect univariate outliers. One of them is to use the z- scores (i.e., normal scores) method. In this method, a standardised score (z-score) of all observations are calculated using the following equation: $$ z_i = \\frac{X_i - \\bar{X}}{S} $$ In the equation below, $ X_i $ denotes the values of observations, $ \\bar{X} $ and $ S $ are the sample mean and standard deviation, respectively. An observation is regarded as an outlier based on its z-score, if the absolute value of its z-score is greater than 3. Note that, z-score method is a parametric way of detecting outliers and assumes that the underlying data is normally distributed. Therefore, if the distribution is not approximately normal, this method shouldn’t be used. In order to illustrate the z-score approach, we will use the “outliers package”. The outliers package provides several useful functions to systematically extract outliers. Among those, the scores() function will calculate the z-scores (in addition to the t, chi-square, IQR, and Median absolute deviation scores) for the given data. Note that, there are many alternative functions in R for calculating z-scores. You may also use these functions and detect the outliers. Let’s investigate any outliers in the depth variable using z-score approach. First, we will make s","date":"2021-03-05","objectID":"/datapreprocessing/:6:3","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Approaches to Handling Outliers Most of the ways to deal with outliers are similar to the methods of missing values like deleting them or imputing some values (i.e., mean, median, mode) instead. There are also other approaches specific to dealing with outliers like capping, transforming, and binning them. Here, we will discuss the common techniques used to deal with outliers. Some of the methods mentioned in this Module (like transforming and binning) will be covered in the next module (Module 7: Transform), therefore I won’t go into the details of transforming and binning here. Excluding or Deleting Outliers Some authors recommend that if the outlier is due to data entry error, data processing error or outlier observations are very small in numbers, then leaving out or deleting the outliers would be used as a strategy. When this is the case, we can exclude/delete outliers in a couple different ways. To illustrate, let’s revisit the outliers in the carat variable. Remember that we already found the locations of the outliers in the carat variable using which() function. Intuitively, we can exclude these observations from the data using: Carat_clean\u003c- Diamonds$carat[ - which( abs(z.scores) \u003e3 )] Let’s see another example on the previous versicolor data: versicolor \u003c- iris %\u003e% filter(Species == \"versicolor\" ) %\u003e% dplyr::select(Sepal.Length, Sepal.Width,Petal.Length) head(versicolor) ## Sepal.Length Sepal.Width Petal.Length ## 1 7.0 3.2 4.7 ## 2 6.4 3.2 4.5 ## 3 6.9 3.1 4.9 ## 4 5.5 2.3 4.0 ## 5 6.5 2.8 4.6 ## 6 5.7 2.8 4.5 results \u003c- mvn(data = versicolor, multivariateOutlierMethod = \"quan\", showOutliers = TRUE) results$multivariateOutliers ## Observation Mahalanobis Distance Outlier ## 34 34 10.974 TRUE ## 49 49 9.580 TRUE The Mahalonobis distance method provided us the locations of outliers in the data set. In our example the 34st and 49th observations are the suggested outliers. Using the basic filtering and subsetting functions, we can easily exclude these two outliers: # Exclude 34st and 49th observations versicolor_clean \u003c- versicolor[ -c(34,49), ] # Check the dimension and see outliers are excluded dim(versicolor_clean) ## [1] 48 3 Note that, the mvn() function also has an argument called showNewData = TRUE to exclude the outliers. One can simply detect and remove outliers using the following argument: versicolor_clean2 \u003c- mvn(data = versicolor, multivariateOutlierMethod = \"quan\", showOutliers = TRUE, showNewData = TRUE) # Prints the data without outliers dim(versicolor_clean2$newData) ## [1] 48 3 ","date":"2021-03-05","objectID":"/datapreprocessing/:6:4","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Imputing Like imputation of missing values, we can also impute outliers. We can use mean (need to be used with caution!), median imputation or capping methods to replace outliers. Before imputing values, we should analyse the distribution carefully, and investigate whether the suggested outlier is a result of data entry/processing error. If the outlier is due to a data entry/processing error, we can go with imputing values. For the illustration purposes, let’s replace the two outlier values in carat variable with its mean by using Base R functions: Diamonds \u003c- read.csv(\"data/Diamonds.csv\") Diamonds$carat[ which( abs(z.scores) \u003e3 )] \u003c- mean(Diamonds$carat, na.rm = TRUE) Replacing outliers with the median or a user specified value can also be done using a similar approach. Note that, you may also prefer to write your own functions to deal with outliers. ","date":"2021-03-05","objectID":"/datapreprocessing/:6:5","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Capping (a.k.a Winsorising) Capping or winsorising involves replacing the outliers with the nearest neighbours that are not outliers. For example, for outliers that lie outside the outlier fences on a boxplot, we can cap it by replacing those observations outside the lower limit with the value of 5th percentile and those that lie above the upper limit, with the value of 95th percentile. In order to cap the outliers we can use a user-defined function as follows (taken from: Stackoverflow): # Define a function to cap the values outside the limits cap \u003c- function(x){ quantiles \u003c- quantile( x, c(.05, 0.25, 0.75, .95 ) ) x[ x \u003c quantiles[2] - 1.5*IQR(x) ] \u003c- quantiles[1] x[ x \u003e quantiles[3] + 1.5*IQR(x) ] \u003c- quantiles[4] x} To illustrate capping we will use the Diamond data set. In order to cap the outliers in the carat variable, we can simply apply our user-defined function to the carat variable as follows: Diamonds \u003c- read.csv(\"https://raw.githubusercontent.com/yanboyang713/RMIT-Data-Repository/main/Diamonds.csv\") carat_capped \u003c- Diamonds$carat %\u003e% cap() We can also apply this function to a data frame using sapply function. Here is an example of applying cap() function to a subset of the Diamonds data frame: # Take a subset of Diamonds data using quantitative variables Diamonds_sub \u003c- Diamonds %\u003e% dplyr::select(carat, depth, price) # See descriptive statistics summary(Diamonds_sub) ## carat depth price ## Min. :0.2000 Min. :43.00 Min. : 326 ## 1st Qu.:0.4000 1st Qu.:61.00 1st Qu.: 950 ## Median :0.7000 Median :61.80 Median : 2401 ## Mean :0.7979 Mean :61.75 Mean : 3933 ## 3rd Qu.:1.0400 3rd Qu.:62.50 3rd Qu.: 5324 ## Max. :5.0100 Max. :79.00 Max. :18823 # Apply a user defined function \"cap\" to a data frame Diamonds_capped \u003c- sapply(Diamonds_sub, FUN = cap) # Check summary statistics again summary(Diamonds_capped) ## carat depth price ## Min. :0.2000 Min. :58.80 Min. : 326 ## 1st Qu.:0.4000 1st Qu.:61.00 1st Qu.: 950 ## Median :0.7000 Median :61.80 Median : 2401 ## Mean :0.7821 Mean :61.75 Mean : 3812 ## 3rd Qu.:1.0400 3rd Qu.:62.50 3rd Qu.: 5324 ## Max. :2.0000 Max. :64.70 Max. :13107 Transforming and binning values Transforming variables can also eliminate outliers. Natural logarithm of a value reduces the variation caused by outliers. Binning is also a form of variable transformation. Transforming and binning will be covered in detail in the next module (Module 7: Transform). Outliers can also be valuable! The outlier detection methods (i.e. Tukey’s method, z-score method) provide us ‘suggested outliers’ in the data which tend to be far away from the rest of observations. Therefore, they serve as a reminder of possible anomalies in the data. For some applications, those anomalies would be problematic (especially for the statistical tests) and usually be handled using omitting, imputing, capping, binning or applying transformations …etc. as they can bias the statistical results. On the other hand, for some other applications like anomaly detection or fraud detection, these anomalies could be valuable and interesting. For such cases you may choose to leave (and investigate further) those values as they can tell you an interesting story about your data. To wrap-up: We don’t always remove, impute, cap or transform suggested outliers in the data, for some applications outliers can provide valuable information or insight therefore analysts may choose to keep those values for further investigation. ","date":"2021-03-05","objectID":"/datapreprocessing/:6:6","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Additional Resources and Further Reading As mentioned before, univariate and multivariate outlier analysis are broader concepts. Interested readers may refer to the “Outlier analysis, by Charu C. Aggarwal” for the theory behind the outlier detection methods (Aggarwal (2015)). Another useful resource is “R and Data Mining: Examples and Case Studies” by Yanchang Zhao (also available here). Chapter 7 of this book covers univariate and multivariate outlier detection methods, outlier detection using clustering and outlier detection in time series. The [outliers package manual] (https://cran.r-project.org/web/packages/outliers/outliers.pdf) includes useful functions for the commonly used outlier tests and the distance-based approaches. This package can be used to detect univariate outliers. I find the MVN package useful for detecting multivariate outliers as it provides many alternative visualizations in addition to the distance-based metrics. The MVN package also includes different types of univariate and multivariate tests for normality. For more information and capabilities of the MVN package please refer to the paper by Korkmaz, Goksuluk, and Zararsiz (2014), which is available here. There are also other R packages for outlier detection, and all might give different results. [This blog by Antony Unwin] (http://blog.revolutionanalytics.com/2018/03/outliers.html) compares different outlier detection methods available in R using the OutliersO3 package. References Aggarwal, Charu C. 2015. “Outlier Analysis.” In Data Mining, 237–63. Springer. Hawkins, Douglas M. 1980. Identification of Outliers. Vol. 11. Springer. Korkmaz, Selcuk, Dincer Goksuluk, and Gokmen Zararsiz. 2014. “MVN: An R Package for Assessing Multivariate Normality.” The R Journal 6 (2): 151–62. ","date":"2021-03-05","objectID":"/datapreprocessing/:6:7","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Cheatsheet ","date":"2021-03-05","objectID":"/datapreprocessing/:7:0","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Modele 2 Reading/Importing Data from CSV files Base R Import Iris1 \u003c- read.csv(file= “~/Desktop/data/iris.csv”) (.csv eliminates any spaces in variable names and fills it with ‘.’) Iris2 \u003c- read.csv(\"iris.csv\", stringsAsFactors = FALSE) NOTE: Set Working directory setwd(\"~/Desktop/data\") Check Structure str(iris1) Using table to read csv file. iris5 \u003c- read.table( \"iris.csv\" , sep=\",\" or \\t, header = TRUE, stringsAsFactors = FALSE) readr (10 times faster) ## (maintain full name, auto set stringasfactor=FALSE) iris9 \u003c- read_csv(\"iris.csv\") NOTE: Display 1st to 6th rows and 1st to 4th variables iris9[1:6,1:4] from Excel files xlsx ## Using sheet index or name: iris11\u003c- read.xlsx(\"iris.xlsx\", sheetName = \"iris\", startRow = 3) ## Row column index: iris12\u003c- read.xlsx(\"iris.xlsx\", sheetName = \"iris\", rowIndex = 3:5, colIndex = 1:4) (keepFormulas (use on which any platform, allows you no to external see the dependencies, text of any formulas) Readxl use on which any platform, no external dependencies, load date and times iris13\u003c- read_excel(\"iris.xlsx\", sheet = \"iris\",skip = 1, col_names = paste (\"Var\", Data 1:6)) gdata (to read excel, Base R cannot read excel) science_data \u003c- read.xls(url) from statistical software Foreign iris_spss \u003c- read.spss(\"iris.sav\", to.data.frame = TRUE) Scraping HTML Table Data rvest births \u003c- read_html(\"link here\") ## show select no. of tables: length(html_nodes(births, \"table\")) ## [1] 1 ## select the of second tables element of the html_nodes births_data\u003c- html_table(html_nodes(births, \"table\")[[1]]) Write data to text files Base R To csv: write.csv(df, file = \"cars_csv\", row.names = FALSE) To text: write.table(df, file = \"cars_txt\", sep=\"\\t\") readr To csv: write_csv(df, path = \"export_csv2\", col_names = FALSE) To text: write_delim(df, path = \"export_txt2\") to Excel files xlsx write.xlsx(df, file= \"cars.xlsx\", row.names = FALSE) create workbook: multiple_df \u003c- createWorkbook() create sheet in workbook: car_df \u003c- createSheet(wb = multiple_df, sheetName = \"Cars\") add Data frame: addDataFrame(x = iris, sheet = iris_df) R object File Saving Data as an R object File: save (x, y, file = \"xy.RData\") save.image() (saves all current workspace as .RData) save a single object to file: saveRDS (x, \"x.rds\") restore it under a different name: x2 \u003c- readRDS (\"x.rds\") check if x and x2 are identical: identical (x, x2) ","date":"2021-03-05","objectID":"/datapreprocessing/:7:1","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Module 3 Data Types and levels Check class: Class() Numeric object integer or double: typeof() Level of factor: levels() Creating character class: var1 \u003c- c(“debit”, “credit”) Creating integer variable: var2 \u003c- c(4L, 7L, 14L) Factor Createing factor variable: var4 \u003c- factor (c(“Male”, “Female”, “Male”, “Female”), level = c(“Male”, “Female”), ordered = True) auto sets the levels alphabetically if not stated Result ## [1] Male Female Male Male ## Levels: Male Female Vectors Creating a vector ex1\u003c- c(“a”, “b”, 1, 2, 3) elements of vector must be same type otherwise coerced Order of coercion logical \u003c integer \u003c numeric \u003c character (best) Add vectors to existing one ex4\u003c- c(ex2, 4, 6) ## [1] 1 2 3 1 0 4 6 Subsetting vector ex4[3] take3rd element ex4[c(1,3,5)] 1st 3rd \u0026 5th ex4[c(1:3)] 1 to 3 ex4 [-1] all except first ex4[ex4\u003c3] all elements less than 3 Lists allows combination of different data types and lengths Creating a list list1 \u003c- list(1:3, “a”, c(TRUE, FALSE, TRUE), c(2.5, 4.2)) Check Structure str(list1) shows a list of 4 with different classes Add onto existing list list2\u003c- append(list1, list(c(“credit”, “debit”, “paypay”) shows list of 5 Checking Attributes attributes ( list2 ) metadata of object Add name: names(list2) \u003c- c(“item1”, item2”, “item3”, “item4”,“item5”) gives name for each element Subsetting lists: list2[1]: takes 1st item in list2 List2 [ [ 1 ] ]: take 1st item in list2 without attributes (name) List2$item1: take 1st list item using name List2$item5 [ 3 ]: take 3rd element out of 5th list item Matrices Creating matrices: m1\u003c- matrix(1:6, nrow=2, ncol=3) elements inside must be same class, all columns must be same length, [row,col] Creating matrices using bind eg. V1, V2 vectors with same length, can also use different data structure as long as they are same length. m2 \u003c- cbind(v1, v2) #col bind m2 \u003c- rbind(v1, v2) #row bind Add name to matrice rownames(m4) \u003c- c(\"subject1\",\"subject2\",\"subject3\") colnames (m4) \u003c- c(“var1, “var2, “var3”) check with attributes(m4) Subsetting matrice: m4 [1, 2] # value of 1st row 2nd col m4 [1:2, ] # subset for rows 1 and 2 but keep all columns m4 [, c(1, 3)] # subset for columns 1 and 3 but keep all rows Data Frames df1 \u003c- data.frame(col1=1:3,col2 = c(\"credit\",\"debit\",\"Paypal\"), col3 = c (TRUE, FALSE, TRUE),col4 = c (25.5, 44.2, 54.9), stringsAsFactors=FALSE) Adding to data frame df2\u003c-cbind(df1, v4) can add col or rows(item) Add name df same as matrices attributes(df2) shows col \u0026 row names Subsetting df df behaves like both lists and matrices df[2:3, ] df [c(“subj1”, “subj3”), ] df [2:3, c(1,4)] df2$fraud[2] # 2nd element in the fraud column Convert Data Types/ Structures as.numeric() as.matrix() num_v\u003c- as.vector(8:17) convert to vec is.numeric() is.matrix() is.vector(num_vec) # check if vector ","date":"2021-03-05","objectID":"/datapreprocessing/:7:2","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Module 4 Tidy Data Principles Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Common problems with messy data sets Column headers are values, not variable names gather(): [wide to long](###pivot_longer() function) Or pivot_longer Multiple variables are stored in rows (no primary key) spread: [long to wide](###pivot_wider() function) Or pivot_wider Multiple variables are stored in one column separate() [col having too many variables](###separate() function) Multiple columns forms a variable unite() [inverse of separate() function](###unite() function) dplyr package select() function: -: Select everything but :: Select range contains(): Select columns whose name contains a character string ends_with(): Select columns whose name ends with a string everything(): Select every column matches(): Select columns whose name matches a regular expression num_range(): Select columns named x1, x2, x3, x4, x5 one_of(): Select columns whose names are in a group of names starts_with(): Select columns whose name starts with a character string filter() function uses operators such as \u003c \u003e == != %in% is.na any all arrange() function allows us to order data by variables in ascending or descending order. mutate() function allows us to add new variables while preserving the existing variables. summarise() (a.k.a. summarize() ) function allows us to perform the majority of summary statistics when performing exploratory data analysis. min(), max() mean() median() sum() var(), sd() first() last() nth() n() n_distinct() group_by() + summarise(): summary statistics grouped by a variable Example flights %\u003e% group_by(dest) %\u003e% summarise(mean_delay = mean(dep_delay, na.rm = TRUE)) How would you find the names of people who was born and deceased at the same place? step1 \u003c- death_registration %\u003e% left_join(birth_registration, by=\"name\") # 5 points step1 %\u003e% filter(place.death == place.born) # 5 points Suppose you want to sort the flights that departed from New York City according to their seating (seats) capacity (i.e. from highest seating capacity to less seating capacity). How would you do that? flights %\u003e% left_join(planes, by=\"tailnum\") %\u003e% arrange(desc(seats)) How would you calculate the average distance covered by each plane manufacturer? flights %\u003e% left_join(planes, by=\"tailnum\") %\u003e% group_by(manufacturer) %\u003e% summarise(mean_distance = mean(distance, na.rm=TRUE)) Mutating joins Example Create new table with required values, join new table to match another table with data. Eg. right_join - about the flights that has weather visibility of 5 weather_v5 \u003c- weather %\u003e% filter(visib == 5) flights2 %\u003e% right_join(weather_v5) %\u003e% select(visib, everything()) How would you find the ages of people (in years) when they deceased? died \u003c- death_registration %\u003e% left_join(birth_registration, by = \"name\") # 5 points died %\u003e% mutate (age.at.death = year.death - year.born) # 5 points Filtering Joins Anti-joins are useful for diagnosing join mismatches. For example, when connecting flights and planes, you might be interested to know that there are many flights that don’t have a match in planes Example How would you find the names of people who are still alive? birth_registration %\u003e% anti_join(death_registration, by=\"name\") ","date":"2021-03-05","objectID":"/datapreprocessing/:7:3","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["MATH2349"],"content":"Module 5 Missing Data a numeric missing value is represented by NA (NA stands for “not available”) character missing values are represented by . some other values may represent missing values (i.e. 99, ., .., just space, or NULL) depending on the software (i.e., Excel, SPSS etc. Identifying Missing Data Result: is.na(df$col4) gives logical result TRUE or FALSE Location: which(is.na(x)) gives location of NA Total in whole: sum(is.na(df)) Total in each column: colSums(is.na(df)) Recode Missing Data recode missing values with mean: x[is.na(x)] \u003c- mean(x, na.rm = TRUE) ## Replace NAs in a data frame df \u003c- tibble(x = c(1, 2, NA), y = c(\"a\", NA, \"b\")) df %\u003e% replace_na(list(x = 0, y = \"unknown\")) Define what’s NA: population_NA[population_NA == \".. \" ] \u003c- NA population_NA[which(population_NA == \".. \" )] \u003c- NA Excluding Missing Data: ## excluding NA values mean(x, na.rm = TRUE) complete.cases(df) ### [1] TRUE FALSE TRUE FALSE ## subset data frame with complete.cases to get only complete cases df[complete.cases(df), ] df[!complete.cases(df), ] na.omit(df) Basic Missing Value Imputation Techniques Replace the missing value(s) with some constant, specified by the analyst library(deductive) library(validate) ## Define the rules as a validator expression Rules \u003c- validator( staff + cleaning + housing == total, staff \u003e= 0, housing \u003e= 0, cleaning \u003e= 0) ## Use impute_lr function imputed_df \u003c- impute_lr(df,Rules) library(Hmisc) ## mean imputation (for numerical variables) x1 \u003c- impute(x$x1, fun = mean) ## check which values are imputed is.imputed(x1) ID Special Values -Inf Inf: is.infinite( vector) true ; is.finite(vector) false vector only func NaN: is.nan( ) retuns true apply() Functions - df list and matrices also vectors apply() matrices \u0026 df lapply() for lists (output as list) sapply() for lists (output simplified) tapply() for vectors Write own function: is.special \u003c- function(x){ if (is.numeric(x)) (is.infinite(x) | is.nan(x)) } ## apply this function to the data frame. sapply(df, is.special) ","date":"2021-03-05","objectID":"/datapreprocessing/:7:4","tags":null,"title":"Data Wrangling (Data Preprocessing)","uri":"/datapreprocessing/"},{"categories":["r"],"content":"Summary This course will get you started with learning R and using the RStudio IDE. The course will cover installation of R and RStudio and using RStudio’s basic features and the basics of R programming. ","date":"2021-03-04","objectID":"/rlanguage/:1:0","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["r"],"content":"Learning Objectives By the end of this course, you will have covered the following: Installing R and RStudio An overview of the RStudio interface Basic programming in R ","date":"2021-03-04","objectID":"/rlanguage/:2:0","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["r"],"content":"Why we learn R In this blog, you won’t learn anything about Excel, SPSS, SQL, SAS, Python, Julia, or any other statistical package/programming language useful for data preprocessing. This isn’t because I think that these tools are bad or redundant. They’re not. In practice, most data analytics teams use a mixture of these tools and programming languages. I strongly believe that R is a great place to start your data analysis journey as it is a comprehensive language for data analysis. You can use R effectively in almost each step of data analysis, from data collection to reporting. You can collect, preprocess, visualise and analyse your data using R functions, report and publish your findings using RMarkdown. Since any typical data preprocessing actions like missing value imputation or outlier handling obviously influence the results of statistical analyses, data preprocessing should be viewed as a statistical operation and should be performed in a reproducible way. The R software provides us a good environment for reproducible data preprocessing as all actions can be scripted and therefore reproduced. There are many reasons why R is a good solution for the problems that are covered in this course. According to Munzert et al. (2014), the most important points are: R is freely and easily accessible. You can download, install, and use it wherever and whenever you want. For a software environment with a primarily statistical focus, R has a large community that continues to flourish. R is used by various disciplines, such as social scientists, medical scientists, psychologists, biologists, geographers, linguists, and in business. R is open source. This means that you can easily retrace how functions work and modify them with little effort. R is reasonably fast in ordinary tasks. R is powerful in creating data visualizations. Although this not an obvious plus for data collection, you would not want to miss R’s graphics facilities in your daily workflow. Work in R is mainly command line based. This might sound like a disadvantage to R rookies, but it is the only way to allow to produce reproducible results compared to point-and-click programs. R is not picky about operating systems. It can generally be run under Windows, Mac OS, and Linux. Finally, R is the entire package from start to finish. ","date":"2021-03-04","objectID":"/rlanguage/:3:0","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["r"],"content":"Installion - Prerequisite RStudio is an integrated development environment (IDE) for R. It will allow you to use R in a more efficient manner. RStudio provides the user with a streamlined user interface and access to many powerful tools to make working with R more efficient. There are two versions of RStudio: RStudio Desktop (Run RStudio on your desktop) RStudio Server (Centralize access and computation) - Recommendation RStudio server offers a free version and Pro version, which comes at a cost, adds many useful advanced features including real-time collaboration. However, I think free version is totally enought for us. I recommend you launching RStudio Server in Docker. The link at the below. {% post_link rstudio %}A Once installed, load RStudio. RStudio is an integrated development environment (IDE) for R. It will allow you to use R in a more efficient manner. RStudio provides the user with a streamlined user interface and access to many powerful tools to make working with R more efficient. ","date":"2021-03-04","objectID":"/rlanguage/:4:0","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["r"],"content":"An Overview of the RStudio Interface The videos provide a run-down of basic and most important features of the RStudio Interface. {% youtube JlkKGDHUKeY %} ","date":"2021-03-04","objectID":"/rlanguage/:5:0","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["r"],"content":"Basic Scripts Scripts are plain text files, with the .R extension, which acts as recipe book for your R code. You can type and edit your code in the script file and then issue parts or all of your code to the R console to be executed. You can create a new script in RStudio by going to File –\u003e New File –\u003e R Script. Ensure you save and name your script so you can come back to it later. {% youtube 5T82DS5xLRo %} ","date":"2021-03-04","objectID":"/rlanguage/:5:1","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["r"],"content":"Projects RStudio Projects associate all your files with a common directory (e.g. scripts and data). When you exit or change projects the environment variables are saved so you can come back to your project later without having to re-run all your code. Projects are the best way to organise your R files. {% youtube mH-FLEO-TcQ %} ","date":"2021-03-04","objectID":"/rlanguage/:5:2","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["r"],"content":"Programming Basics In the following sections you will learn the basics of programming in R. If you haven’t programmed before, now is a great opportunity to start. Fortunately, R is fairly intuitive to learn. We will start with the fundamentals for now and get heaps of practice. Its a great skill to possess and can even be a lot of fun if you like problem solving. Let’s get started. ","date":"2021-03-04","objectID":"/rlanguage/:6:0","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["r"],"content":"How to read these notes The following code box refers to commands submitted to the console. You can copy this code directly and paste it into a script of run it directly from the RStudio console. x \u003c- c(1,2,3,4,5,6,7,8,9,10) mean(x) The following code box refers to output produced by the console: ### [1] 5 ","date":"2021-03-04","objectID":"/rlanguage/:6:1","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["r"],"content":"Code, Commands and Syntax We get R to do stuff by running commands from our script files or by typing directly into the R Console. Commands can be a whole range of different things, like basic arithmetic, loading data, producing plots or running statistical functions. To issue commands to R, we write code. The code we write is governed by “syntax” or a set of rules used by a programming language. If we don’t use the right syntax, we will get an error. You will make heaps of errors when you’re learning to use R. Don’t worry about it. Its a normal part of learning. Let’s start looking at the basics of R syntax. Case Sensitivity R is case sensitive so “A” and “a” are two different characters. x \u003c- \"A\" # Assign x as \"A\" y \u003c- \"a\" # Assign y as \"a\" ##Does x = y? x == y ### [1] FALSE Command Separation and Grouping Commands are separated using a new line: x \u003c- rnorm(10000,0,1) # Randomly generate 10,000 normally distributed values mean(x) #Calculate the mean of x ### [1] -0.01520157 sd(x) #Calculate the standard deviation of x ### [1] 0.9937938 …or row, or by using a semi-colon “;” mean(x); sd(x) ### [1] -0.01520157 ### [1] 0.9937938 Comments Use # to add comments to your code. Any code proceeding the # symbol is ignored by the console. Comments are useful to remind yourself and others what your code does. Get into the habit of commenting. Look back at the previous code chunks to see examples of commenting. Objects and Object Assignment R objects can be variables, values, datasets, text or functions. R Studio lists the objects in the Environment window (top right). Stored objects are saved to the R session and can be recalled at any time by typing the object’s name in the console. For example, the following codes assign the value of the mean of x to an object named “m”. We use “\u003c-” to make an assignment. You can also use the “=” symbol, but this should be avoided because it may be confused with the equals sign used as a logical operator. The RStudio keyboard shortcut for writing “\u003c-” is to hit the ALT and “-” key. m\u003c-mean(x) #Assign the mean of x to variable m ##Print the mean of x m ### [1] -0.01520157 Vectors A vector is a simple sequence of stored data. We can store a simple dataset in a vector using the c() function. For example, let’s say we have five random peoples’ heights (cm): 166, 177, 164, 167, 177. We can store them in a vector, named “heights” to make it easy to do statistical calculations. heights\u003c-c(166, 177, 164, 167, 177) mean(heights) #Calculate the mean height of the sample ### [1] 170.2 sd(heights) #Calculate the standard deviation of the sample's heights ### [1] 6.300794 Shortly you will learn to store datasets as data frame objects which is an even more powerful way to work with data. Functions Functions are the workhorse of R. R has many built in functions that do a whole host of different things. R’s functionality can be greatly enhanced by installing packages (see below). You will learn to use a whole range of different functions as the course unfolds. This course has already introduced a number of very useful statistical functions. Recall the use of mean(), sd() and hist(). Packages - Installation and Loading Packages are compilations of functions. For example, the ggplot2 packages provide access to functions designed to create beautiful graphs. Packages are why R is so powerful. Most statisticians develop packages to freely implement their cutting edge statistical methods that are not available in commercial packages. Others develop packages to fill the many gaps of commercial packages. There are an overwhelming number of packages available on the Comprehensive R Archive Network (CRAN), as well as other unofficial packages (many available from GitHub. As of Nov 2016, the CRAN website listed 9,535 available packages! Packages can be installed through RStudio by going to Tools –\u003e Install Packages. You will need to know the name of the package you want to install. The following video shows an example of instal","date":"2021-03-04","objectID":"/rlanguage/:6:2","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["r"],"content":"R Markdown This section will introduce you to the basics of R Markdown and RStudio Notebooks. These documents enable you to create professional and reproducible reports through RStudio. ","date":"2021-03-04","objectID":"/rlanguage/:7:0","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["r"],"content":"Learning Objectives By the end of this section, you will have completed the following: Understand the importance of reproducibility and the role of R Markdown. Install and create an R Markdown document in RStudio. Knit/preview and save an R Markdown document in HTML or PDF. Use basic R Markdown syntax to create reproducible reports. Know where to find further information for using R Markdown. ","date":"2021-03-04","objectID":"/rlanguage/:7:1","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["r"],"content":"Reproducibility Your statistical analysis must be reproducible, not only for yourself, but more importantly for others. Each stage of your analysis must be documented and explainable to others. The ultimate test of reproducibility is to give someone your R Markdown file and data and for them to replicate all of your analysis by running it through R. In practice, things are never this simple, but that’s certainly the goal we should have in mind. ","date":"2021-03-04","objectID":"/rlanguage/:7:2","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["r"],"content":"Creating an R Markdown Document in R Studio In RStudio go to File –\u003e New File –\u003e R Notebook. You could also select R Markdown (R Notebooks are a type of Markdown file), but for this section, we will stick to the Notebook output type. Allow RStudio to install or update any necessary packages. If you have trouble, run the following installation which will force all the required packages to also install: install.packages(\"rmarkdown\") A new notebook will appear in the RStudio script window. This usually forces your console window to disappear, so don’t worry. As we will discover, we won’t need it. Save your Notebook. It will save with an .Rmd extension which tells you it is an R Markdown file. The template includes some basic examples of R Markdown. An RStudio Notebook, or any other R Markdown file, can be both published, e.g. as a HTML, or run like an ordinary R script file. Most of the file looks like a word document. The screenshot above shows the R Notebook template included when you create a new R Notebook. The first paragraph is descriptive text. However, you can also see R code. R code is written in “Chunks”. You can see the R code chunk in the screenshot above. It’s highlighted in grey and sits in between {r} and syntax. performs a plot function on the dataset object cars. If we want to run the R code in an R Notebook, we press the green arow “run”\" button that appears in the top right-hand side of the chunk. Or, we place the cursor in the chunk and click on the Chunks menu (top right hand corner of the script screen). This menu gives us plenty of options. We can use Insert chunk to automatically type out the {r} and syntax, jump to different chunks, and run previous code, current chunk or next chunk. You also have the option to run all chunks together in the R Notebook Document. This is what we would do if we wanted to replicate someone elses’ analysis. Let’s run the first R chunk above. Chunks are sent to the console and executed like a script. The fantastic thing about Notebooks is that output appears underneath the chunk instead of in the console or in the plot window. This significantly improves RStudio workflow and the enjoyment of using R. In the screenshot above you can see the plot of the car data. R Markdown documents are fully reproducible. This means you could send the data and the R Markdown report to another investigator and they should be able to reproduce everything you did. Reproducibility is crucial in statistics, analytics and data science. However, in practice, it is never that simple. Due to a multitude of different factors, you will always have some issues with reproducing someone else’s report. A common issue is different versions of packages. If we’ve finished analysing our data and writing our report, we click the Preview button to create a HTML report (see the screenshot below). Things like R output and plots will be automatically added and scaled to the document. This can save you a lot of editing time. It also includes the option to hide the code. This is a nice touch if you are sharing your report with stakeholders that might not want to see your code. There are many other output options available including PDF and Microsoft Word, but these formats lose interactive feature and can be tricky to compile. A HTML document will be saved to your working directory where you saved your R Markdown file and a preview will appear in the RStudio browser. You can open this file in any web browser or word processor and save in a different format (e.g. PDF or .doc). You also have the option to Publish this document directly to a hosting service. This allows you to publish your Notebooks online or provide secure access to your team or clients. RStudio offers a few different options: RPubs: Free, open access. Only use this for open data projects. Shiny Apps: A free account include 5 applications (notebooks) that can be public or secured. Paid accounts get more. Shiny Apps can also host interactive Shiny Applica","date":"2021-03-04","objectID":"/rlanguage/:7:3","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["r"],"content":"R Markdown Syntax Basics R Markdown is a powerful, light markup language. Using simple code, you can learn to format and present beautiful reports that integrate with your R code and statistical analysis. For detailed guides please see the Reference Guide and Cheatsheet produced by RStudio. The following code will demonstrate some of the basics of R Markdown. YAML Header Start your documents with a YAML Header. There a many options that can be changed. The following header includes the basics. Note the use of — to start and finish the header. ---title:\"R Bootcamp: Course 4\"author:\"James Baglin\"date:\"Tuesday, March 28, 2017\"output:html_notebook--- Headings Using different levels of headings is a great way to structure your reports. ## Header 1 ### Header 2 #### Header 3 ##### Header 4 etc... Text Formatting, Links, and Math Yype There are simple markdown codes used to format text, add hyperlinks and write mathematical formulas. To write a paragraph, use plain text. You can use the following syntax to *italicise*, **bold**, superscript^2^, hyperlink, [link](http://www.rmit.edu.au/), and write inline equations using Latex code, $z = \\frac{x -\\mu}{\\sigma}$. Which will render as… To write a paragraph, use plain text. You can use the following syntax to italicise, bold, superscript^2^, hyperlink, link, and write inline equations using Latex code, $z = \\frac{x -\\mu}{\\sigma}$. Lists Here are some examples of lists. To get second levels working, ensure you use a double tab indent. * Unordered list * Item 2 + Sub-item 1 + Sub-item 2 1. Ordered list 2. Item 2 + Sub-item 1 + Sub-item 2 Which will appear as… Unordered list Item 2 Sub-item 1 Sub-item 2 Ordered list Item 2 Sub-item 1 Sub-item 2 Tables Tables can be manually entered. **Table 1:** Mean and Median for Speed and Distance Variable|Mean |Median --------|-----|------ Speed |15.40|15.0 Distance|42.98|36.0 Renders as… Table 1: Mean and Median for Speed and Distance Variable Mean Median Speed 15.40 15.0 Distance 42.98 36.0 You can also use the knitr::kable() function to convert tables and data frames from R into nicely formatted HTML tables. ```{r} library(knitr) knitr::kable(anscombe, caption = \"Anscombe's Quartet Data\") **NOTE:** {r, echo=FALSE} - to report output without the R code #### In-line Code Using R code in-line allows you to create dynamic documents that can update values in your report by re-running the analysis. Here’s an example… You can also use R Markdown syntax to report R code inline. For example, the mean speed was 15.4, SD = 5.29, n = 50. Ensure you enclose the syntax using two backward single quotes `. This will render as follows: You can also use R Markdown syntax to report R code inline. For example, the mean speed was 15.4, *SD* = 5.29, *n* = 50. Ensure you enclose the syntax using two backward single quotes `. Very handy if you have dynamic reports that are updated on a regular basis. Re-rendering the Markdown files will automatically update all the values. ### Further Reading There is a lot of amazing things you can do with [RStudio and R Markdown](http://rmarkdown.rstudio.com/formats.html). Here are some links to help you learn more. [R Markdown Cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf) [R Markdown Websites](http://rmarkdown.rstudio.com/rmarkdown_websites.html) - This website is an example of R Markdown Website. [R Markdown Presentations](http://rmarkdown.rstudio.com/lesson-11.html). A variety of formats are offered including beamer, ioslides, slidy and revealjs. [Shiny Applications](https://shiny.rstudio.com/): Interactive applications and data visualisations. [Dashboards](http://rmarkdown.rstudio.com/lesson-12.html): Create beautiful dashboards quickly using the power of R and Markdown. [Interactive Documents](http://rmarkdown.rstudio.com/lesson-14.html): Embed a wide range of interactive features into your HTML R Markdown pages. [Plotly](https://plot.ly/r/getting-started/): Embed interactive, web-based, ","date":"2021-03-04","objectID":"/rlanguage/:7:4","tags":["r","r language"],"title":"Getting Started with R language","uri":"/rlanguage/"},{"categories":["MATH1324"],"content":"What is Statistics? Statistics is the science of collecting, organizing, analyzing, and interpreting data in order to make decisions. Some informal definitions of statistics, provided by various well-known statisticians, are The science of learning from (or making sense out of) data (J. Kettenring). The theory and methods of extracting information from observational data for solving real-world problems (C.R. Rao). The science of uncertainty (D.J. Hand). The art of telling a story with data (L. Gaines). ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:1:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Statistics vs. Mathematics Statistics and Mathematics differ in a number of fundamental ways: Mathematics: Problems can exist without context Measurements are assumed to be exact No variability Deterministic answers Statistics: The use of context and data collection Measurement decisions Omnipresence of variability Dealing with uncertainty ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:2:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Orientation This article will introduce you to fundamental statistical concepts and modern statistical practice: Study statistical data investigations, summary statistics, data visualisation and probability as a measure for uncertainty. Then build upon these topics and learn about sampling, sampling distributions and confidence intervals as the basis for statistical inference. The course will finish with a series of modules looking at common hypothesis testing methods for different types of data. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:3:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Why statistics are important? Very useful to be able to estimate business value in your head – employers highly regard the ability to use stats in business logic (e.g. estimate a market size to know if a new product is feasible before investing in detailed market research) Often this acts as a good error check on our own biases/misconceptions Fundamental to many analytics challenges and encompasses a wide range of stats methods from descriptive statistics to hypothesis testing and control group design (all of which we cover in this course). Experience how statistics can improve the quality of your decision-making. {% youtube wV0Ks7aS7YI %} ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:4:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Module One - Dealing Confidently with Uncertainty ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:5:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"The learning objectives: Define and understand the omnipresence of variability by stating major sources and types of variation. Define the discipline and science of statistics, and distinguish it from mathematics. Define a variable, their major types, and levels of measurement. Understand the basics stages of the statistical data investigation process. Understand the major types of statistical data investigations, namely, surveys, observational studies and experiments. Discuss the idea of statistical inference and how the use of samples give rise to uncertainty. Explore the central concepts of variability in a simple statistical data investigation. {% youtube y3A0lUkpAko %} ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:5:1","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Random Variables and Sources of Variation Statistics, as defined by MacGillivray, Utts and Heckard (2014), is the “discipline and science of obtaining, understanding, modelling, interpreting, and using data in all real and complex systems and processes that involve uncertainty and variation” (p.15). Data, variation, and uncertainty are at the core of statistics. What is data? Observations coded as categories or numeric values What is uncertainty? The situation resulting from randomness or some form of unpredictability What is variation? observations or measurements that are not determined and can take on more than one value. Variation leads to uncertainty and is the reason why the field of statistics emerged. Statistical data refer to variables, which are defined as any characteristic or value of a unit that can change or vary. The idea of a unit is very broad and can refer to a person, a time point, a device, or system. Variation is all around us. This idea is referred to as the “omnipresence of variability” and is the reason why the field of statistics emerged. There are many forms of variation that you need understand. These can be summarised into four main categories. Natural or Real Variation: This refers to inherent, natural variability that is left over when all the other sources of variability are accounted for. Take, for example, a person’s height. Height varies greatly in the population, but there a many other variables that can explain why one person is taller than another. Males tend to be taller than females. Adults are taller than children. However, even if we compared males of a similar age, height will still vary. This is the natural or “real” variability that statistics seeks to measure and understand. Natural variability is what makes the world an interesting place. Explainable Variation: This is the variation in one variable that we can explain by considering another variable. The statistical tests and models that you will learn in this course seek to test relationships and effects that different variables have on each other. You already know heaps of examples of variables that “explain” other variables. For example, you know that height can help explain variation in weight, smoking can help us understand why some people are at a greater risk of lung cancer, gender can explain variation in the risk of heart disease, and the amount of hours spent studying can help explain exam scores. Sampling Error: Take a sample from a population, make a measurement and record the result. Now, repeat the process many times. The sample results will all differ to a certain degree. This type of variability is known as sampling variability. Statistical inference and hypothesis testing, to be introduced in later modules, deals with this specific form of variability and the implications it has on drawing conclusions from our studies. We will also consider this important source of variation in an interesting demonstration at the end of this module. Non-sampling Variation: This refers to any artificial variability induced by the research process. As researchers, you try to understand real variability, while acknowledging, accounting or controlling for induced variability. Induced variability can come from many factors. The following are some common examples: Study statistical data investigations, summary statistics, data visualisation and probability as a measure for uncertainty. Then build upon these topics and learn about sampling, sampling distributions and confidence intervals as the basis for statistical inference. The course will finish with a series of modules looking at common hypothesis testing methods for different types of data. (1). Measurement: Sometimes referred to as observational error, this is the variability associated with how we have measured our variables. All measures of a variable are imperfect. We need to understand the reliability and validity of our measurements to account for measurement variability. Meas","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:5:2","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Types of Variables and Levels of Measurement A measurement occurs when a variable is actually measured or recorded. For example, you measure a person’s heart rate (measurement). Heart rate is measured on a scale, or the unit of measurement, for example, beats per minute (BPM). A set of measurement or records from a variable is called data and can come from either a sample, a sub group of a population, or the population itself. When an entire population is measured, this is referred to as a census. Major Types of Variables The two major types of variables are qualitative and quantitative. The type of variables you collect and analyse have a direct bearing on the type of statistical summaries and analyses you can perform. Qualitative - Qualitative variables have different qualities, characteristics or categories, e.g. hair colour (black, brown, blonde,…), disease (cancer, heart disease,…), gender (male, female), country of birth (New Zealand, Japan,…). Qualitative variables are used to categorise quantitative data into groups or to tally frequencies of categories that can be converted to proportions and percentages. Quantitative - Quantitative variables measure a numerical quantity on each unit. Quantitative variables can be either discrete - can only assume a finite or countable number of values, e.g. marks on a test, birthday, number of people in a lecture, or continuous - the value can assume any value corresponding to the points on a number line, e.g. time (seconds), height (cm), weight (kg), age etc. Levels of Measurement When you measure a variable, qualitative and quantitative variables can take on different scales or levels of measurement. Levels of measurement have a direct bearing on the quantitative data analysis techniques you will need to use. We need to understand the language used to describe different scales. The following short video by Nicola Petty provides a great overview. {% youtube hZxnzfnt5v8 %} Categorical or Nominal (Qualitative). Categorical variables are group variables or categories if you will. There are no meaningful measurement differences such as rankings or intervals between the different categories. Categorical or nominal variables include binary variables (e.g. yes/no, male/female) and multinomial variables (e.g. religious affiliation, hair colour, ethnicity, suburb). Ordinal (Qualitative). Ordinal data has a rank order by which it can be sorted but the differences between the ranks are not relative or measurable. Therefore, ordinal data is not strictly quantitative. For example, consider the 1st, 2nd and 3rd place in a race. We know who was faster or slower, but we have no idea by how much. We need to look at the race times. Interval (Quantitative): An interval variable is similar to an ordinal variable except that the intervals between the values of the interval scale are equally spaced. Interval variables have an arbitrary zero point and therefore no meaningful ratios. An example is our calendar year. 1000 AD is not half of 2000 AD, and 20 degrees Celsius is not twice as “hot” as 10 degrees Celsius. This is because our calendar and Celsius scale have an arbitrary value for zero. Zero AD and zero degrees Celsius do not imply the presence of zero time or zero heat energy. Study statistical data investigations, summary statistics, data visualisation and probability as a measure for uncertainty. Then build upon these topics and learn about sampling, sampling distributions and confidence intervals as the basis for statistical inference. The course will finish with a series of modules looking at common hypothesis testing methods for different types of data. Ratio (Quantitative): A ratio variable is similar to an interval variable; however there is an absolute zero point and ratios are meaningful. An example is time given in seconds, length in centimeters, or heart beats per minute. A value of 0 implies the absence of a variable. We can also make statements like 30 seconds is twice the time of 15 s","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:5:3","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Statistical Inference - The Big Idea of Statistics The idea of the following sections is to give you a glimpse into the big picture of this course, that is, statistical inference. What is statistical inference? “Statistical inference moves beyond the data in hand to draw conclusions about some wider universe, taking into account that variation is everywhere and the conclusions are uncertain” (Moore 2007, xxviii) As such, statistics has been referred to as the discipline involved with dealing confidently with uncertainty. Wild, Pfannkuch and Horton (2011) provided the analogy to help explain the big idea behind statistical inference. Throughout this course you will develop a deeper understanding of statistical inference and the uncertainty associated with the use of samples. You will learn how samples impact data and the conclusions you draw. You will also learn how to measure and express your statistical uncertainty and confidently draw appropriate conclusions. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:5:4","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"The Statistical Data Investigation Process Statistical practice is much broader than analysing data. The statistical data investigation process describes how real problems are tackled from a statistical problem solving approach. It is how statistics is applied to investigate questions in science, medicine, agriculture, business, engineering, psychology, or anywhere data are needed and the data exhibit variation. As discussed previously, variation is omnipresent. At almost all levels of government, industry, and science, data are measured, quantified, and interpreted in order to understand variation. By asking statistical questions of the data, taking observations and performing experiments, data can be used by investigators to seek patterns amongst great variation. The entire process of a statistical data investigation involves everything from initial thoughts, through to planning, collecting and exploring data, and reporting findings. The process is depicted and summarised in the following slideshow, along with a brief description and key considerations at each stage. Click on each stage to read more. As you work through the statistical data investigation process, its useful to apply statistical habits of mind. Some useful examples proposed by Hollylynne Lee and Dung Tran include: Always consider the context of data Ensure the best measure of an attribute of interest is used Anticipate, look for, and describe variation Attend to sampling issues Embrace uncertainty, but build confidence in interpretations Use several visual and numerical representations to make sense of data Be a skeptic throughout an investigation Sometimes a data investigation starts with a question, sometimes a hypothesis, sometimes a problem, and sometimes just a general situation to be explored. Statistical questions and problems are not the same as mathematical questions and problems. Don’t confuse the two. Tukey (1953), a world famous statistician, best explained this when he wrote: “Statistics is a science in my opinion, and it is no more a branch of mathematics than are physics, chemistry, and economics; for if its methods fail the test of experience – not the test of logic – they are discarded.” Furthermore, statistical questions can be differentiated based on the following: The use of context and data collection Measurement decisions Omnipresence of variability Dealing with uncertainty In contrast, mathematics questions are characterised by the following: Problems can exist without context Measurements are assumed to be exact. No variability Deterministic answers The following video from the Khan Academy explores how statistical questions are fundamentally different to maths questions. {% youtube qyYSQDcSNlY %} ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:5:5","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Major Types of Statistical Data Investigations There are three major types of statistical data investigations that can be distinguished based on their aims and methods of data collection. We must understand the strengths and weaknesses of each types as these have an impact on the conclusions you draw. We will learn more about different research designs throughout the course when we look at different statistical methods. Surveys Surveys aim to gather data from the population, in the form of responses, in order to investigate or understand some type of phenomenon. For example, you might have had experience filling out course evaluation surveys, product surveys, customer satisfaction surveys, the Australian Census, and job evaluation surveys. Surveys are typically done using samples, but may also be completed by an entire population. When this happens, this is known as a census. The Australian Census seeks to count and describe the characteristics of the Australian population in order to help Australia plan for the future. Surveys are typically administered using paper-based or online questionnaires completed by the participant, but may also involve face-to-face or telephone interviews. Surveys may seem like the easiest and most cost effective way to gather large amounts of data from a population, but that is far from the truth. Surveys have many challenges, including selecting a good sample, poor response rates, response bias, and designing good questions. Experiments In the simplest experimental design, participants or some other unit are randomised to a control group or a treatment group. The investigator’s manipulation of exposure to the treatment is what defines a true experiment. The treatment is referred to as an independent variable. Randomisation is used to maximise the chance of the groups being considered equivalent, in terms of their characteristics, at the start of the experiment, thus minimising systematic bias between groups. The control group does not receive the actual treatment, and may instead, receive an inert form of the treatment called a placebo. Blinding is used to prevent the results being influenced by participant expectations, by ensuring the participants are unaware of their allocated research group or the true nature behind the experiment. The investigator seeks to keep all other factors and variables constant throughout the experiment. At the end of the experiment, the investigator will measure the two groups on a dependent or outcome variable. Because of randomisation of participants and tight control, it is assumed that by the end of the experiment, any significant difference between groups on the dependent variable could ONLY be due to the treatment. Therefore, if a difference between groups is evident at the end of the experiment, its assumed to be the effect of the treatment. Thus, experiments seek to test cause and effect hypotheses. However, experiments are also the most difficult and time consuming of investigation types. Observational or Correlational Studies Observational or correlational research designs look for a relationship between at least two variables. Observational or correlational research do not attempt to manipulate or control an independent variable, which distinguishes it from an experiment. Therefore, these types of studies cannot test cause and effect. Instead, they are used to establish evidence of relationships, associations or correlations between variables that may suggest evidence of causal effects. Conclusions from observational and correlational investigations are always interpreted with this limitation in mind. On the plus side, these types of investigations allow researchers to study relationships between variables that cannot be manipulated in experiments. For example, ethically, you cannot randomise people to smoke cigarettes to test if it increases risk of cancer. However, you can observe and compare the incidence of cancer in people who voluntarily smoke to thos","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:5:6","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"References MacGillivray, H., J. M. Utts, and R. F Heckard. 2014. Mind on statistics. 2nd ed. South Melbourne, VIC: Cengage Learning Australia. Moore, D. S. 2007. The basic practice of statistics. 4th ed. New York, NY: W. H. Freeman; Company. Tukey, J. W. 1953. “The growth of experimental design in a research laboratory.” In Research Operations in Industry, 303–13. New York: King’s Crown Press. Wild, C., M. Pfannkuch, M. Regan, and N. J. Horton. 2011. “Towards more accessible conceptions of statistical inference.” Journal of the Royal Statistical Society 174: 247–95. https://doi.org/10.1111/j.1467-985X.2010.00678.x. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:5:7","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Exercise (Model One) If two researchers gather two random samples in the same circumstances, these two samples are likely to vary. Which of the following statements best explains why? The researchers have not sampled correctly. Different samples are made up of different combinations of units from a population, and will therefore always vary to some degree. The researchers have collected their samples at different time points. Because no matter how careful they are, the researchers could not use the same method. Which of the following characteristics is unique to a statistical problem and helps differentiate it from a mathematical problem? Measurements are assumed to be exact. The problem can be expressed without a context. The problem has a unique answer. The anticipation of variation. During data collection and prior to data exploration, what should you be checking? Consistency of data collection Obvious error or issues Accuracy of data collection All of the above Variation is all around us, and this is the main reason why statistics is needed. Variation exists on many different levels. Which of the following is something that does NOT vary? Students' attitudes towards maths. The mean age calculated from different samples of a population. The ratio of a circle’s circumference to its diameter. Measurement of an individual’s blood pressure. Match the scale of measurement with the correct example. Nominal - Credit card number Ratio - Forced expiratory volume Ordinal - Energy star ratings Interval - IQ Scores Which of the following is an example of real or natural variability? You notice variability in your body weight measurement when using your bathroom scale and the scale in your gym. Driving to your workplace after leaving home at 10am tends to be faster than leaving at 8am. Monday train commuting times between your home station and work is around 24 minutes, but it can be as quick as 21 minutes and as long as 27. Two investigators gather a small sample of 30 adults each and measure their body mass indexes (BMI). The mean BMI for each sample is different. Which of the following is a qualitative variable? Bank account number Travel time Energy intake Wireless signal strength Which of the following is an interval level measure? Mass Degrees Fahrenheit Distance Length Statistical inference refers to: Measuring the population. Drawing conclusions about a sample using census data. Drawing conclusions about a population using sample data. Analysing your sample data. Larger random samples have less sampling error. True False Order the stages, from first to last, of the statistical data investigation process. 1 - Initial questions 2 - Issues and planning 3 - Collecting, handling and checking data 4 - Exploring, interpreting data in contect Thinking statistically is required during which stages of the statistical data investigation process? Select all that apply. Initial questions Issues and planning Collecting, handling and checking data Exploring, interpreting data in context Which of the following is a major challenge for surveys? Can only be done online Unethical Expensive and time-consuming Response rate Why do experimental investigations use placebos? People need to think they’re getting the treatment because peoples' beliefs and biases can impact results. Because they are cheaper than the real treatment. Because placebos are an effective treatment. So participants don’t feel they’re missing out. Why do experimental investigations use randomisation? To ensure participants are blinded to their allocated group. To help establish baseline equivalence between groups. To fairly allocate people to the treatment. To ensure the investigator is blinded. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:5:8","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Model Two - Descriptive Statistics through Visualisation ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:6:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Learning Objectives The learning objectives associated with this module are: Basic descriptive statistics and visualisations for qualitative variables: frequencies cross-tabulations bar charts clustered bar charts Basic descriptive statistics and visualisations for quantitative variables: Dot plots Histograms Mean and standard deviation Quartiles, median, and IQR Box plots Side-by-side box plots Scatter plots to visualise the relationship between two quantitative variables Understand and apply the basic principles of producing good plots. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:6:1","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Descriptive Statistics Descriptive statistics are methods by which complex, noisy or vast amounts of data can be converted into insightful, bite-sized, pieces of usable information. Descriptive statistics summarise characteristics of data using numbers. These include things like the mean, range, mode or percentage. Statistical visualisations are visual displays of descriptive statistics or data, most commonly graphs or plots, that summarise important features or trends. Visualisations make use of our visual sense to assist with interpreting data beyond reading tables of numbers from a page. Visualisations offer a more exciting and pleasing way to summarise data. The following sections will introduce common visualisations used in statistics to summarise difference types of variables. Often these visualisations report descriptive statistics. Therefore, these visualisations will be used to introduce common descriptive statistics. This module is for the romantics. We will consider a large dataset of around 54,000 diamond prices and characteristics, you can download Diamonds dataset from here. Before popping the question to your significant other, ensure you know a little about diamonds… Here’s some information about the dataset, Diamonds. The dataset includes the prices and other attributes of almost 54,000 diamonds. The variables are as follows: price: price in US dollars ($326 - $18,823) carat: weight of the diamond (0.2 - 5.01) cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal) colour: diamond colour, from J (worst) to D (best) clarity: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best)) x: length in mm (0–10.74) y: width in mm (0–58.9) z: depth in mm (0–31.8) depth: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43–79) table: width of top of diamond relative to widest point (43–95) This module will also show you how to use R to summarise and plot your data. Open RStudio, load the Diamonds data (ensure you call the object Diamonds) and reproduce the R code and output below to get further practice with using R. Ensure you save your R script so you can come back to it later. Also ensure you define your factors correctly. The following code ensures that cut, colour and clarity variables are treated correctly as ordered factors. Diamonds$cut\u003c- factor(Diamonds$cut, levels=c('Fair','Good','Very Good','Premium','Ideal'), ordered=TRUE) Diamonds$color\u003c- factor(Diamonds$color, levels=c('J','I','H','G','F','E','D'), ordered=TRUE) Diamonds$clarity\u003c- factor(Diamonds$clarity, levels=c('I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF'), ordered=TRUE) 54,000 is a lot of data. The only way we can get some useful information from a dataset of this size will be to use descriptive statistics. Let’s start with looking at summarising some qualitative variables. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:6:2","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Qualitative Variables Frequency Distributions Cut quality is a qualitative variable, measured on an ordinal scale, ranging from fair to ideal. A frequency table can be used to tally/count the number of times a particular cut quality appears in the dataset. We can use the table() function to generate a basic frequency distribution. library(dplyr) # Required for data management and pipes Diamonds$cut %\u003e% table() ### . ### Fair Good Very Good Premium Ideal ### 1610 4906 12082 13791 21551 Reading the output, we find the “ideal”, 21,551, cut is the most frequently occurring value. The most frequently occurring qualitative value for a variable is called the mode. There, we just covered two types of descriptive statistics in no time. Counts or tallies don’t allow comparison between different datasets. For example, what if we wanted to compare the counts of the Diamonds variable to another smaller dataset? We need to use proportions, f/n, where f = the count or frequency or a value, and n = sample size. We can also readily convert proportions to percentages using f/n * 100. I’ll assume we’re all intimately familiar with percentages. You only have to walk into any shopping centre to be bombarded… In R, we can report the proportions for cuts using: Diamonds$cut %\u003e% table() %\u003e% prop.table() ### . ### Fair Good Very Good Premium Ideal ### 0.02984798 0.09095291 0.22398962 0.25567297 0.39953652 or, percentages: Diamonds$cut %\u003e% table() %\u003e% prop.table()*100 ### . ### Fair Good Very Good Premium Ideal ### 2.984798 9.095291 22.398962 25.567297 39.953652 Bar Charts A simple and effective visualisation of qualitative data is the humble bar chart. Here’s how to get one in R. First we assign the table object name. Let’ asign the percentages to an object named perc. perc \u003c- Diamonds$cut %\u003e% table() %\u003e% prop.table()*100 Now, call the bar plot using the following function. Note how the code defines a title, x axis label and sets the height of the y axis: perc %\u003e% barplot(main = \"Diamond Cut Quality - Percentage\",ylab=\"Percent\", ylim=c(0,50)) The height of each bar represents the percentage of cut quality in the data set. This is much quicker to interpret than written tallies. Bars can also represent raw counts/tallies or proportions. Ensure you label your axes correctly to help the reader interpret your scales. It’s also a good idea to start your y-axis at 0, so as not to distort the scale and to allow relative comparisons across levels. Note One: If you want to draw color bar chart, please, set the col, there is a example at the below. barplot(freq, main = \"Diamond Cut Quality - Percentage\", ylab=\"Percent\", xlab=\"Colour\", col=\"deepskyblue\") Note Two: If you want to Order by frequency, there is a example at the below. barplot(freq[order(freq, decreasing = TRUE)], main = \"Diamond Cut Quality - Percentage\", ylab=\"Percent\", xlab=\"Colour\", col=\"deepskyblue\") Contingency Tables (Cross-tabulations) When we need to explore the relationship between two categorical variables, we can create contingency tables, also known as cross-tabulations. These tables present one categorical variable as the rows and the other categorical variable as the columns. These tables make it easy for us to calculate conditional probabilities or percentages, which makes it easier for us to explore potential associations between variables. In the following example, we will consider the relationship between the cut of a diamond and its clarity. To get the contingency table for raw counts, we use: table(Diamonds$cut,Diamonds$clarity) ### ### I1 SI2 SI1 VS2 VS1 VVS2 VVS1 IF ### Fair 210 466 408 261 170 69 17 9 ### Good 96 1081 1560 978 648 286 186 71 ### Very Good 84 2100 3240 2591 1775 1235 789 268 ### Premium 205 2949 3575 3357 1989 870 616 230 ### Ideal 146 2598 4282 5071 3589 2606 2047 1212 However, because there are differences between the row and column totals for each category, it makes looking at trends difficult. We can calculate the conditional column percentages using the fol","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:6:3","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Quantitative Variables Quantitative variables require different types of statistical summaries and visualisations. Let’s start with a small sample of the diamond data to make the calculations manageable, and then we will scale-up to the full 54,000 for some awesome plotting later in the module. The Diamonds_sample.csv dataset contains a small random sample of 30 diamonds’ mass measured in carats. Diamonds_sample \u003c- read.csv(\"data/Diamonds_sample.csv\") Dot Plots Dot plots are a nice visual representation of small quantitative datasets. Each dot is arranged into bins on the x-axis and stacked on top of each other to report the frequency. Dot plots represent the distribution of a quantitative variable, and granularity of the small sample. We can use dot plots to quickly see where values are clustering and tending towards, as well as unusual cases known as outliers. To get a dot plot in R, you first must install and load the ggplot2 package. install.packages(\"ggplot2\") library(ggplot2) We can use a ggplot2 quick plot function to produce the dot plot. Diamonds_sample %\u003e% qplot(data = ., x = depth, geom = \"dotplot\", binwidth = .25) The dot plot of 30 random diamond depths are arranged into bins with widths of .25 mm. For example, three values were between 63.75 mm and 64mm, five values between 61 and 61.25, etc. It’s a little hard to tell, which is where we encounter some of the issues with dot plots. The main drawback of dot plots is that they don’t show the actual values. So, we can’t tell exactly what they are. They are also sensitive to the number of bins or internals used in the plot. Setting the right bin widths is tricky. Let’s set a larger number of intervals, or bins, in the plot, by making the intervals smaller. Diamonds_sample %\u003e% qplot(data = ., x = depth, geom = \"dotplot\", binwidth = .1) By increasing the number of internals or bins to nint=10, the dot plot drastically changes appearance. This is a drawback to visualising small samples. Often the choice of parameters used in the plot can change interpretations. Be mindful. They are also problematic for large datasets… Diamonds %\u003e% qplot(data = ., x = depth, geom = \"dotplot\", binwidth = .25) Histograms When dealing with large amounts of quantitative data, we can use histograms to explore the distributions of interval and ratio data. Let’s use R to create some histograms of diamond prices: Diamonds$price %\u003e% hist(col=\"grey\",xlim=c(0,20000), xlab=\"Diamond Price\", main=\"Histogram of Diamond Prices\") Take note of how the code limits the x axis to values between $0 and $20,000, we have also set the colour of the bars to grey and a plot title. Histograms break the data into bins, or intervals, depicted using bars, where the height of the bar typically refers to the frequency. Due to the large amount of data, the bars are joined together to form a continuous wall. This is the drawback of using histograms for small samples. The continue wall of bars can give the viewer a false sense of the data’s density. Dot plots are much better for small samples as each dot depicts the granularity of the data. The histogram of prices is interesting. We can quickly lean the following: Prices range goes all the way up to $19,000 U.S. Most diamond prices are below $5,000 The most common diamond prices are between 0 to $1,000. The price distribution is an example of what we would call a positively, or right skewed, distribution. You will see examples of other distributions shortly. We can use R to find the bin intervals used to construct the plot: bins \u003c- Diamonds$price %\u003e% hist() bins$breaks ### [1] 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 ### [13] 12000 13000 14000 15000 16000 17000 18000 19000 bins$counts ### [1] 14524 9683 6129 4225 4665 3163 2278 1668 1307 1076 934 825 ### [13] 701 603 504 513 425 405 312 Using this information, we can create the table to the below which explains how the data were plotted in the histogram above. binstable \u003c- data.frame(Breaks = bins$breaks,","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:6:4","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Scenario 1 2, 4, 9, 8, 6, 5, 3 ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:7:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Ordered 2, 3, 4, 5, 6, 8, 9 ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:8:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Location of Median $$ (n+1)/2 = (7+1)/2 = 4th $$ Therefore, the median is the 4th ordered value, Median =5. In R: x \u003c- c(2, 3, 4, 5, 6, 8, 9) x %\u003e% summary() ### Min. 1st Qu. Median Mean 3rd Qu. Max. ### 2.000 3.500 5.000 5.286 7.000 9.000 ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:9:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Scenario 2 2, 4, 9, 8, 6, 5 ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:10:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Ordered 2, 4, 5, 6, 8, 9 Location of Median Average of the (n/2) and $$(n/2) + 1 $$ observations, so the average of the 3rd and 4th The median is the average of the 3rd and 4th ordered observation so, $$ Median = (5 + 6) /2 = 5.5 $$. In R: y \u003c- c(2, 4, 9, 8, 6, 5) y %\u003e% summary() ### Min. 1st Qu. Median Mean 3rd Qu. Max. ### 2.000 4.250 5.500 5.667 7.500 9.000 Q1 and Q3 are calculated in a similar fashion after the dataset is split at the median, top and bottom 50%. Q1 is the median of the bottom 50% (i.e. 25th percentile) and Q3 is median of the top 50% (i.e. 75th percentile). Q1 and Q3 when n = odd (take median value) Q1 = Median of bottom 50%: For example, Median of 2, 3, 4, 5 = average of 2nd and 3rd value = (3+4)/2 = 3.5 Q3 = Median of top 50%: For example, Median of 5, 6, 8, 9 = average of 2nd and 3rd value = (6+8)/2 = 7 Note how the median is included in both halves. Q1 and Q3 when n = even Q1 = Median of bottom 50%: For example, Median of 2, 4, 5 = 2nd value = 4 Q3 = Median of top 50%: For example, Median of 6, 8, 9 = 2nd value = 8. Note how the median is not included because the median is not an actual data point. Note also that R does not use this method, which I have included as a simple instructional example. R has 9 different methods to calculate quartiles. We will stick to the default method produced by R. You just need to know conceptually what it represents. NOTE One: Diamonds$carat %\u003e% quantile() #Quartiles NOTE Two: Diamonds$carat %\u003e% median() # Median Box Plots Box Plots are used to depict the quartiles of distribution. The following info-graphic puts all the concepts of quartiles, medians and percentiles together. Let’s first dissect a box plot of diamond depth from the small sample: Here are the quartiles: Diamonds_sample$depth %\u003e% summary() ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:11:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Min. 1st Qu. Median Mean 3rd Qu. Max. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:11:1","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"58.20 61.05 62.05 61.78 62.67 63.90 Now let’s discuss the anatomy of the basic box plot below. The interquartile range (IQR) is the middle 50% of data and is depicted as the “box” in the box plot. The IQR is also a measure of variation. $$ IQR = Q3 − Q1 $$ Diamonds$carat %\u003e% IQR() #Interquartile range Box plots also include suspected outliers, depicted using an “o” or a similar symbol. Outliers are values that fall beyond the outlier fences. The outlier fences are defined as the following: $$ Lower outlier \u003c Q1 − 1.5 ∗ IQR $$ $$ Upper outlier \u003e Q3 + 1.5 ∗ IQR $$ Using the depth data summary, we find: $$ IQR = 62.675 − 61.05 = 1.625 $$ $$ Lower outlier \u003c 61.05 − (1.5 ∗ 1.625) = 58.6125 $$ $$ Upper outlier \u003e 62.675 + (1.5 ∗ 1.625) = 65.1125 $$ Outliers are unusual cases that should be investigated by the researcher. Don’t automatically remove outliers until you have a good reason to do so. For example, an outlier might be a data entry error or a measurement that should have been excluded from the investigation. Removing outliers because they don’t look “nice” is not appropriate. When you remove outliers for any reason, this should be made clear when you report your results. Now let’s put histograms, box plots, and measures of central tendency together: The mean and median will be the same when the data have a symmetrical distribution. The median is said to be more robust (less sensitive to unusual cases) than the mean. Therefore, the median is often preferred when a variable’s distribution is skewed or has many outliers present. This is the case for the highly skewed diamond price data. The median appears to give a better account of the central tendency of the data, while the mean seems unduly influenced by the long tail. The following info-graphic summarises the effect of skewness on the mean and median. The mean is the solid red line, and the median is the dashed red line. The tail of a skewed distribution always draws the mean towards it, because the mean takes all values into account. Note that these effects are generally true for large datasets. In small datasets, skewness is often poorly estimated, so the relationship among the mean, median and skewness can behave in unusual ways. Comparing Groups Descriptive statistics and visualisations are often used to compare groups. The following examples show how the basic descriptive R functions and plots can be quickly extended to assist with comparing the distributions of a quantitative variable across qualitative groupings. The example will consider comparing diamond price distributions across colour. First, lets get a descriptive statistics table using the summarise() function from the dplyr package. The table will include min, Q1, median, Q3, max, mean, standard deviation, n and missing value count. I’ve included quite a few descriptive statistics so you can get a sense of how to customise the table. Essentially, the table can incorporate any descriptive functions in R provided the function results in a single value. Take note how we have to include the option na.rm = TRUE to most of these functions. If we didn’t, and missing values were present, the function would fail. Diamonds %\u003e% group_by(cut) %\u003e% summarise(Min = min(price,na.rm = TRUE), Q1 = quantile(price,probs = .25,na.rm = TRUE), Median = median(price, na.rm = TRUE), Q3 = quantile(price,probs = .75,na.rm = TRUE), Max = max(price,na.rm = TRUE), Mean = mean(price, na.rm = TRUE), SD = sd(price, na.rm = TRUE), n = n(), Missing = sum(is.na(price))) ### # A tibble: 5 x 10 ### cut Min Q1 Median Q3 Max Mean SD n Missing ### \u003cord\u003e \u003cint\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cint\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cint\u003e \u003cint\u003e ### 1 Fair 337 2050. 3282 5206. 18574 4359. 3560. 1610 0 ### 2 Good 327 1145 3050. 5028 18788 3929. 3682. 4906 0 ### 3 Very Good 336 912 2648 5373. 18818 3982. 3936. 12082 0 ### 4 Premium 326 1046 3185 6296 18823 4584. 4349. 13791 0 ### 5 Ideal 326 878 1810 4678. 18806 3458. 3808. 21551 0 Next, let’s create a side-by-side box plot. Diamonds %\u003e% box","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:11:2","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Module 2 - Exercises Data from: Download the Cars.csv dataset from here. This dataset contains data from over 400 vehicles from 2003. The following variables, along with their coding, are included: Vehicle_name: Model Name Sports: Sports car? (1 = ‘yes’, 0 =‘no’) Sport_utility: Sports utility vehicle? (1 = ‘yes’, 0 =‘no’) Wagon: Wagon? (1 = ‘yes’, 0 =‘no’) Minivan: Minivan? (1 = ‘yes’, 0 =‘no’) Pickup: Pickup? (1 = ‘yes’, 0 =‘no’) All_wheel_drive: All wheel drive? (1 = ‘yes’, 0 =‘no’) Rear_wheel_drive: Rear wheel drive? (1 = ‘yes’, 0 =‘no’) Retail_price: The recommended retail price ($) Dealer_cost: The cost price for a car dealer ($) Engine_size: Engine size in litres Cylinders: Number of cylinders (-1 = Rotary engine) Kilowatts: Power of the engine in kilowatts. Economy_city: Kilometres per litre for city driving Economy_highway: Kilometres per litre for highway driving Weight: Curb weight of car (kg) Wheel_base: Wheel base of car (cm) Length: Length of car (cm) Width: Width of car (cm) library(dplyr) library(ggplot2) # the url for the online csv file url \u003c- \"https://raw.githubusercontent.com/yanboyang713/RMIT-Data-Repository/main/Cars.csv\" Cars \u003c- read.csv(url) # When you doing plot. No order was specified when you created the factor (by default), so, when R tried to plot it, it just placed the levels in alphabetical order. If you using **ordered=TRUE**, there will create a order to ratings, and your plots should reflect that! Cars$Sports\u003c- Cars$Sports %\u003e% factor(levels=c(0,1), labels=c('No','Yes'), ordered=TRUE) Cars$Sport_utility\u003c- Cars$Sport_utility %\u003e% factor(levels=c(0,1), labels=c('No','Yes'), ordered=TRUE) Cars$Wagon\u003c-Cars$Wagon %\u003e% factor(levels=c(0,1), labels=c('No','Yes'), ordered=TRUE) Cars$Minivan\u003c- Cars$Minivan %\u003e% factor(levels=c(0,1), labels=c('No','Yes'), ordered=TRUE) Cars$Pickup\u003c-Cars$Pickup %\u003e% factor(levels=c(0,1), labels=c('No','Yes'), ordered=TRUE) Cars$All_wheel_drive \u003c- Cars$All_wheel_drive %\u003e% factor(levels=c(0,1), labels=c('No','Yes'), ordered=TRUE) Cars$Rear_wheel_drive \u003c- Cars$Rear_wheel_drive %\u003e% factor(levels=c(0,1), labels=c('No','Yes'), ordered=TRUE) str(Cars) Exercise 1 What is the sample size? length(Cars$Vehicle_name) Exercise 2 Obtain a frequency distribution for the cylinder variable. How many cars had 4 cylinders? Cars$Cylinders %\u003e% table() Cars %\u003e% filter(Cylinders == 4) %\u003e% count() Exercise 3 What percentage of cars had 6 cylinders? (Round response to two decimal places) library(magrittr) Cars$Cylinders %\u003e% table() %\u003e% prop.table() %\u003e% multiply_by(100) %\u003e% round (2) data \u003c- Cars$Cylinders %\u003e% table() %\u003e% prop.table() %\u003e% multiply_by(100) %\u003e% round (2) %\u003e% paste0(., \"%\") CylindersType \u003c- Cars$Cylinders %\u003e% table() %\u003e% prop.table() %\u003e% multiply_by(100) %\u003e% round (2) %\u003e% rownames () df \u003c- data.frame(CylindersType, data) df Exercise 4 What proportion of cars had all wheel drive? (Round response to two decimal places) allWheelDrive \u003c- table(Cars$All_wheel_drive) %\u003e% prop.table() %\u003e% round (2) allWheelDrive allWheelDrive[\"Yes\"] %\u003e% multiply_by(100) %\u003e% paste0(., \"%\") Exercise 5 How many sports cars were in the sample? table(Cars$Sports)[\"Yes\"] Exercise 6 Create a bar chart showing the distribution of the proportion of different total car cylinders in the sample. Save the bar chart as an image and upload it to this exercise. table(Cars$Cylinders) %\u003e% prop.table() %\u003e% barplot(main=\"Distribution of Car Cylinders\",xlab = \"Cylinders\", ylab=\"Proportion\",col='grey') Cars$Cylinders %\u003e% table() %\u003e% prop.table() %\u003e% multiply_by(100) %\u003e% barplot(main = \"Car Cylinders - Percentage\",ylab=\"Percent\", ylim=c(0,50)) Exercise 7 Create a contingency table showing the column proportions of cylinders by sports car. What proportion of sports cars have six or more cylinders? (Round answer to two decimal places) tbl \u003c- table(Cars$Cylinders,Cars$Sports) %\u003e% prop.table(margin = 2) tbl class(tbl) tbl[5:8,2] %\u003e% sum() %\u003e% round(2) Exercise 8 What proportion of non-sports cars have six or more cylinders? (Round answer ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:11:3","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Model Three - Probability: The Language of Uncertainty ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:12:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"learning objectives The learning objectives associated with this module are: List the basic principles of probability. Express uncertainty using probability. Define common probability terms. Solve basic probability problems. Solve problems involving permutations and combinations. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:12:1","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Probability Probability is defined as the proportion of times a random event occurs in a very large number of trials. Probability must always be a value between 0 and 1. What we define as an “event” and a “trial” depends on the context. In statistics, we estimate the probability of an event using a sample and note its relative frequency, f/n, where f is the frequency or number of times an event occurs and n is the total sample size. As the sample size n increases, the sample will begin to approximate the true population probability. In Statistics, we need to infer characteristics concerning a population from a sample (data) selected from the population. Notice that the process of selecting the sample involves an element of variability and the theory of probability, which is the formal study of the law of chance, is just the thing we need to assess this variability. Since nothing in life is certain, probability is widely used in all branches of science, engineering and economics. There are many applications of probability in society. For example, we need to use probability theory to design and analyse experiments in almost any field of science and social science. assign values to financial derivatives. design, dimension and control telecommunications systems, and understand the process of evolution of gene sequences. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:12:2","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Dependencies and risk Modelling dependencies is an important and widespread issue in risk analysis: Bank loans: Dependency between defaults on loans Correlation of default: what is the likelihood that if one company defaults, another will default soon after? Dependency between stocks (e.g. CAC40 \u0026 DAX) Amount of money for an insurance company which is needed to reimburse all claims. Hydrology: Dependency between annual peak of a river and volume Civil engineering: Reliability analysis of highway bridges Risk management: 1% or 5% quantile of an investment portfolio return Insurance industry: Estimating exposure to systemic risks Correlation of deaths for life insurance companies: Marginal distributions: Probabilities of time until death for each spouse Joint distribution: Shows the probability of spouses dying in close succession Aim (actuarial studies): Estimate the conditional probability when one spouse dies, that the succeeding spouse will die shortly afterwards ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:12:3","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Basic Concepts Why probability? It forms the mathematical foundation for statistical models and procedures. Basic Concepts: An experiment is the process by which an observation (or measurement) is obtained. An event is a set of possible outcomes of an experiment, that is a subset of Ω, usually denoted by a capital letter. The outcome space or sample space Ω is the set of all possible outcomes of an experiment, survey or other observation The sample space is the set of all possible outcomes of an experiment. We use the term “experiment” very loosely here to refer to any activity where an outcome can vary. The most common “experiment” in statistics is taking a sample from the population. For examples, Toss of a coin. Ω = {H, T } where H = “head up” T = “tail up” Spin of a roulette wheel. Ω = {0, 1, . . . , 36} (There are 37 numbers on an Australian roulette wheel.) Experiments and Events Experiment Experiment: Record an age A: person is 30 years old B: person is older than 65 Experiment: Toss a die A: observe an odd number B: observe a number greater than 2 Measurement of the number of phone calls passing through a telephone exchange in a fixed time period. A record of the proportion of people in a survey who approve of the prime minister. Events An event that cannot be decomposed is called a simple event. Denoted by E with a subscript. Each simple event will be assigned a probability, measuring “how often” it occurs. The set of all simple events of an experiment is called the sample space, S. Events are sets and so they are subject to the normal set operations. Thus The event A ∪ B is the event that A or B or both occur. The event A ∩ B is the event that A and B both occur. The event A^c or \\bar{A} is the event that A does not occur. We write ω ∈ A to say that the outcome ω is in the event A. We write A ⊆ B to say that A is a subset of B. This includes the possibility that A = B. If A is finite (which will often not be the case), we write #A for the number of elements of A. For illustrative purposes, and to gain intuition, the relationship between events is often depicted using a Venn diagram. Two events A_{1} , A_{2} which have no outcomes in common (A1 ∩ A2 = ∅) are called mutually exclusive or disjoint events. Similarly, events A1 , A2 , . . . are disjoint if no two have outcomes in common, that is A_{i} ∩ A_{j} = ∅ ∀ i 6 != j. Set operations satisfy the distributive laws A ∩ (B ∪ C) = (A ∩ B) ∪ (A ∩ C) A ∪ (B ∩ C) = (A ∪ B) ∩ (A ∪ C) and De Morgan’s laws (A ∪ B)^c = A^c ∩ B^c (A ∩ B)^c = A^c ∪ B^c What do we mean when we say “The probability that a toss of a coin will result in ‘heads’ is 1/2”? An interpretation that is accepted by most people for practical purposes, that such statements are made based upon some information about relative frequencies. People #trials #heads frequency of heads Buffon 4040 2048 0.5069 DeMorgan 4092 2048 0.5005 Feller 10000 4979 0.4979 Pearson 12000 6019 0.5016 Pearson 24000 12012 0.5005 Similar statements can be made about tossing dice, spinning roulette wheels, arrivals of phone calls in a given time period, etc. The Probability of an Event The probability of an event A measures “how often” we think A will occur. We write P(A). Suppose that an experiment is performed n times. The relative frequency for an event A is $$ Number of timesA occurs / n = f / n $$ If we let n get infinitely large, $$ p(A) = \\lim_{n \\to \\infty} f/n $$ Examples: Probability axioms These considerations lead to the following axioms: P(A) ≥ 0, for all events A P(Ω) = 1 3∗. (Finite additivity) For a set of mutually exclusive events {A1 , A2 , A3 , . . . , An } $$ P ( \\cup_{i=1}^{n} A_i ) = \\sum_{i=1}^n P(A_i) $$ In fact, it turns out that we need a slightly stronger version of Axiom 3 . Specifically, it has to hold for infinite sequences of mutually exclusive events. Thus, we use 3. (Countable additivity) $$ P ( \\cup_{i=1}^{\\infty} A_i ) = \\sum_{i=1}^{\\infty} P(A_i) $$ where {A1 , A2 , A3 , . . .} is any sequence of mutua","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:12:4","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Rules We denote probability using Pr(A), where A refers to an event of interest, e.g. Pr(2 serves). Let’s run an experiment by randomly selecting an Australian adult from the population. We will use the ABS data table as the probabilities of different events occurring. We use the relative frequency formula, f/n, to estimate this basic probability. First, we find the trial size, which is the estimated population size of Australians over the age of 18 as of 2012, 17,042, which equates to 17,042,000 people (the table is reported in ’000). Next, we find the frequency of people who eat two serves of fruit a day, 4984. Using the f/n formula, we write… $$Pr(2 serves)=4984/17042=0.292$$ Therefore, the probability of randomly selecting an Australian adult who consumes two serves of fruits a day is .292. The probability of any event, Pr(A), must be between 0 and 1. If event A can never occur, Pr(A)=0. If event A always occurs when the experiment is performed, then Pr(A)=1. The sum of all probabilities of all possible events must equal 1: $$Pr(\u003c 1 serve)+Pr(1serve)+Pr(2 serves)+Pr(3 serves or more)=1$$ $$(3368+5445+4984+3244)/17042 = 17042 / 17042 = 1$$ Two events are said to be mutually exclusive if, when one event occurs, the other cannot and vice versa. Mutually exclusive sets have no intersection: Pr(A∩B)=0. We use ∩ to denote an intersection. The levels of fruit consumption are mutually exclusive. A person cannot occupy more than one category at a particular time. %%Pr(1 serve ∩ 2 serves) = 0%% If two events can occur simultaneously, the events are not mutually exclusive and an intersection is possible, e.g. Pr(\u003c 1 serve ∩ Male). Note that the probability of an intersection can still be 0 even though an intersection is possible. For example, what if all Australian males had their favourite fruit prepared and delivered to them daily? Then, Pr(\u003c 1 serve ∩ Male) would probably be close to 0. When we talk about intersections, the term “and” is often used. For example, what is the probability that a randomly sampled Australian adult is in the “\u003c 1 serve” interval AND male? You need to become comfortable with the language of probability. However, don’t worry, its a lot easier than you think when we apply the rules to real world examples. Using the Fruit Intake table, we can find this probability to be: $$Pr(\u003c 1 serve ∩ Male) = 2036 / 17042 = .119$$ The union of two events A or B, is an event when either A or B, or A and B occur. We write a union using the ∪ symbol. The word “or” is used to refer to a union. For example, consider Pr(1 serve ∪ \u003c 1 serve), or in words, “What is the probability that a randomly sampled Australian will consume 1 serve of fruit OR less?” $$Pr(1 serve ∪ \u003c 1 serve) = (3368 + 5445) / 17042 = .517$$ The complement of an event consists of all outcomes of the experiment that do not result in an event. For example, $$Pr(\\bar{\u003c 1 serve}) = Pr(1 serve) + Pr(2 serves) + Pr(3 serves or more)$$. The complement of an event usually has a bar over the top of the event which is read as “not”. For example: $$Pr(\\bar{\u003c 1 serve}) = Pr(1 serve) + Pr(2 serves) + Pr(3 serves or more) = (5445 + 4984 + 3244) / 17042 = .802$$ or $$ Pr(\\bar{\u003c 1 serve}) = 1 − Pr(\u003c 1 serve) = 1 − 3368/17042 = .802 $$ The figure below provides a notational overview of the basic probability concepts. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:12:5","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Multiplication Law If two events are independent (i.e. the probability of the first event does not impact the probability of the second event), then the intersection is equal to the probability of the first event multiplied by the second event, Pr(A∩B)=Pr(A)×Pr(B). However, if independence does not hold, the two events are said to be dependent. For example, if the consumption of fruit was independent of gender, then Pr(\u003c 1 serve ∩ Male) = Pr(\u003c 1 serve) × Pr(Male). However, is the assumption of independence safe for fruit consumption and gender? It’s often believed that adult males tend to consume less fruits than females. Therefore, the assumption of independence is not safe and the multiplication rule will not hold. Let’s check this assumption using the multiplication rule. Recall… $$ Pr(\u003c 1 serve ∩ Male) = 2036 / 17042 = .119 $$ Now, according to the multiplication rule for independent events: $$ Pr(\u003c 1 serve ∩ Male) = Pr(\u003c 1 serve) × Pr(Male) = 3368 \\ 17042 × 8406 \\ 17042 = .097 $$ The two probabilities are not the same, therefore, as suspected, fruit consumption and gender are not independent. Males are more likely to be in the “\u003c 1 serve” category. The take home message is that the multiplication rule does not apply when events are dependent. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:12:6","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Addition Laws Finding the probability of a union depends on whether or not the events are mutually exclusive. If the events are mutually exclusive, we simply add the events. Recall… $$ Pr(1 serve ∪ \u003c 1 serve) = (3368 + 5445) / 17042 = .517 $$ If the events are not mutually exclusive, we need to subtract the intersection from the addition law. For example: $$ Pr(\u003c 1 serve ∪ Male) = Pr(\u003c 1 serve) + Pr(Male) − Pr(\u003c 1 serve ∩ Male) = 3368 / 17042 + 8406 / 17042 − 2036 / 17042 = .571 $$ ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:12:7","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Conditional Probability The probability that an event, B, will occur given that another event, A, has already occurred is called the conditional probability of B given A. The “|” symbol, read as “given” is used to denote a condition. Conditional probability can be written as follows: Pr(B|A) = Pr(A ∩ B) / Pr(A) Using an example… $$ Pr(\u003c 1 serve|Male) = Pr(Male ∩ \u003c 1 serve) / Pr(Male) = (2036 / 17042) / (8406 / 17042) = .242 $$ We can also use conditional probability to check independence. The two events A and B are independent if and only if Pr(A|B) = Pr(A) or Pr(B|A)=Pr(B). Otherwise, the events are dependent. Let’s use this rule to reconfirm that gender and fruit consumption are dependent. Pr(\u003c 1 serve | Male) = .242 Pr(\u003c 1 serve) = 3368 / 17042 = .198 We find Pr(B|A) ≠ Pr(A). The probabilities are not equal, therefore, dependency is present. Given that you’re male, you are more likely to be consuming less than 1 serve of fruit per day than females. Males need to eat more fruit! NOTE: conditional probability and Multiplication Law to check independence. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:12:8","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Permutations and Combinations Permutations (have rangking) Let’s assume you are voting in a local council election. There are six candidates. You need to vote for the top three. How many possible ways can you assign your votes, 1st, 2nd and 3rd preference? This is an example of a permutation problem. Permutations refers to all the possible ways of selecting something where order matters. Here are three possible permutations for the voting example: Veronica Paskett Milagros Depaolo Loraine Muntz Thuy Silverberg Myriam Hakes Maude Dimery 1st 2nd 3rd - - - - 1st 2nd 3rd - - - 2nd 1st - 3rd - … … … … … … As you can see, there are many more possible ways to assign your votes. To quickly calculate all the possible permutations, we can use the following formula: $$P(n,k)=n! / (n−k)!$$ The ! symbol refers to the factorial of a number. For example, 6!=6×5×4×4×3×2×1=720. In R, we can use factorial(): factorial(6) ### [1] 720 k is the number to choose, in this example, 3. Solving the voting problem: P(6,3) = 6! / (6−3)! =6! / 3! = 7206 Using R: factorial(6) / factorial(6-3) ### [1] 120 Combinations (not ranking) Now let’s change the problem. You have four spare tickets for a sport grand final. You have ten friends that you know would like to go. You need to weigh up the social impact of inviting different combinations of friends. First you need to know how many possible combinations of selecting four out of ten friends need to be considered. Combinations refer to all the possible ways of selecting a certain number of things from a larger group. Here are three possible combinations: Leah Rosalie Marlena Tarra Graham Gilberto Marcos Gladis Otha Jeremiah Ticket - Ticket - Ticket - - - - Ticket - - - - - - Ticket Ticket Ticket Ticket - Ticket - Ticket - Ticket - - - Ticket Notice with combinations that order does not matter. Leah, Marlena, Graham and Jeremiah is the same combination as Marlena, Leah, Jeremiah and Graham. All four friends will go to the game and will be your best friends forever, regardless of the order by which they’re selected. The formula for combinations is as follows: C(n,k) = n! / ((n−k)!k!) This is known as the “choose”\" formula or the binomial coefficient (we will revisit this in Module 4). Solving, we find: C(n,k) = n! / ((n−k)!k!) = 10! / (10−4)!4! = 10! / (6)!4! = 3628800 / 17280 = 210 Using R: choose(10,4) ### [1] 210 That’s incredible. There are 210 different combinations of friends that you could end up taking to the final. Considering the social ramifications of each combination will take weeks of deliberation, leave it to chance. Use a raffle instead. That way no one can claim favouritism… Who said statistics wasn’t useful! ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:12:9","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Exercises Data The table below was adapted from Table 11.1 of the Australian Health Survey: First Results, 2011-12 scat.no.4364.0.55.001. Table 11.1 presents the estimates of persons (reported in 100,000s) from the 2011-2013 Australian Health Survey relating to Australians’ levels of exercise. The values are in 100,000s. Estimates were derived from surveys of approximately 9500 Australian households. Age group 15–17 18–24 25–34 35–44 45–54 55–64 65–74 75+ Total 100’000 Males Sedentary 63.3 307.1 440.3 527.4 518.4 477.3 320.2 253.9 2,907.90 Low 109.8 230.6 446.2 466.6 473 373.9 245.7 165.2 2,511.00 Moderate 125.8 278.8 377.9 305.8 295.4 318 np np 1,701.70 High 136.6 303.3 341.8 249.4 202.3 96.1 np np 1,329.50 Total(b) 435.5 1,119.80 1,606.20 1,549.20 1,489.10 1,265.30 565.9 419.1 8,450.10 Females Sedentary 119.6 336.5 558.6 549.1 570.9 475.7 357 451.2 3,418.60 Low 154.1 426.9 552.2 562.2 516.3 486.2 276.3 154.7 3,128.90 Moderate 93.3 193.9 294.9 319.3 307.6 272.1 np np 1,481.10 High 49.1 112.2 192.6 156.1 137.7 63.8 np np 711.5 Total(b) 416.1 1,069.50 1,598.30 1,586.70 1,532.50 1,297.80 633.3 605.9 8,740.10 Persons Sedentary 182.9 643.6 998.9 1,076.50 1,089.30 953 677.2 705.1 6,326.50 Low 263.9 657.5 998.4 1,028.80 989.3 860.1 522 319.9 5,639.90 Moderate 219.1 472.7 672.8 625.1 603 590.1 np np 3,182.80 High 185.7 415.5 534.4 405.5 340 159.9 np np 2,041.00 Total(b) 851.6 2,189.30 3,204.50 3,135.90 3,021.60 2,563.10 1,199.20 1,025.00 17,190.20 Note. np = not estimated. Use this table to answer the following questions: Exercise 1 According to Table 11.1, gender is mutually exclusive. True or false? A: TRUE Exercise 2 Being male and sedentary is mutually exclusive. True or false? A: FALSE Exercise 3 Substitute T the correct probability C symbol with the statement. ∪ A|B Ā ∩ a. A or B = A __ B b. A and B = A __ B c. Not A = __ d. A given B = A __ B A: a = 1, b = 4, c = 3, d = 2 Exercise 4 What is the probability of an Australian being sedentary? (Round answer to 3 decimal places) (6326.5 / 17190.2) %\u003e% round(3) Exercise 5 According to Table 11.1, what is the probability of being male? (Round answer to 3 decimal places) (8450.1/17190.2) %\u003e% round(3) Exercise 6 According to Table 11.1, what is the probability of being female? (Round answer to 3 decimal places) 1 - (8450.1/17190.2) %\u003e% round(3) (8740.1 / 17190.2) %\u003e% round(3) Exercise 7 What is the probability of being a male and sedentary? (Round answer to 3 decimal places) (2907.90 / 17190.2) %\u003e% round(3) Exercise 8 What is the probability of being female and sedentary? (Round answer to 3 decimal places) (3418.60 / 17190.2) %\u003e% round(3) Exercise 9 In the previous question, you calculated the probability of being female and sedentary. Determine if being sedentary is independent of being female. Ensure you can show your reasoning. Being female is independent from being sedentary. Being sedentary depends on being female. Being female and sedentary are independent because both events are mutually exclusive. Only being male depends on being sedentary. A: Being sedentary depends on being female. From the previous question you should have found that Pr(Female ∩ Sedentary) = 0.199. If these two events are independent, then Pr(Female ∩ Sedentary) = Pr(Female)*Pr(Sedentary). Using Table 11.1: Pr(Female) = 8740.1/17190.2 Pr(Sedentary) = 6326.5/17190.2 Pr(Female)*Pr(Sedentary) = (8740.1/17190.2) * (6326.5/17190.2) This is not equal s to 0.199 calculated from Table 11.1, therefore being sederntary and female is dependent. Exercise 10 What is the probability of being male, aged 25 - 34 and high in exercise level? (Round answer to 3 decimal places) (341.8/17190.2) %\u003e% round(3) Exercise 11 What is the probability of being female, aged 25 - 34 and high in exercise level? (Round answer to 3 decimal places) (192.6/17190.2) %\u003e% round(3) Exercise 12 What is the probability of being aged between 35-44? (Round answer to 3 decimal places) (3135.9/17190.2) %\u003e% round(3) Exercise 13 What is the probability of ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:12:10","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Probability Distributions: Random, but Predictable ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:13:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Overview Despite the omnipresence of variability, many variables follow predicable patterns. That’s not to say we can reliably predict an individual observation with great certainty, but over the course of many repeated observations of a variable, we can predict many informative outcomes. This module introduces two discrete probability distributions and one continuous probability distribution which are know to model the behaviour of many random processes. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:13:1","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Learning Objectives The learning objectives associated with this module are: Define and distinguish between random variables, discrete random variables, and continuous random variables. Define the properties of the Binomial and Poisson distributions. Correctly apply and work with the Binomial and Poisson distributions to solve Binomial and Poisson-based problems. Define the properties of the normal distribution and identify where it can be applied. Work with the normal distribution to solve normal-based problems. Define the standard normal z-distribution. Standardise random variables to standard normal variables, and vice-versa. Compare empirical distributions to theoretical probability distributions ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:13:2","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Probability Distributions This module will introduce you to the first set of fundamental probability distributions used by statisticians to model random processes. Probability distributions are based on the central concept of statistics that, while an individual random event is almost impossible to predict, the behaviour of random processes in the long run can be very well understood. This module introduces discrete and continuous probability distributions. These distributions are used to model quantitative variables that can only take on discrete values (1, 2, 3…) and continuous values (1.45, 5.43, 2.39). These probability distributions can be used to model many random variables including guessing on a multiple choice exam, the number of heads flipped from five tosses of a coin, the number of goals scored in a football match, the infection rate of a disease or the number of people lining up in a queue at your local cafe. Let’s start with the binomial distribution. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:13:3","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Binomial Distribution The binomial distribution is used to model the number of successes/failures in n independent trials where the probability of success at each trial is fixed as p. The probability of failure is 1−p. For example, let’s say a cancer vaccine is effective 85% of the time, p = .85. If we randomly select 12 vaccinated people from the population, n = 12, and expose them to the virus that causes the cancer, what is the probability that the vaccine will be successful for all 12 people? Before we answer this question, let’s take a look at some theory. The binomial distribution has the following mathematical form: $$Pr(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}$$ where k = successes, n = no. of trials and p = probability of success. This formula is known as a probability mass function or PMF. The mean, or expected value, E(x), variance and standard deviation for a binomial distribution are as follows: $$\\text{Mean} = E(x) = np$$ $$\\text{Variance} = np(1 - p)$$ $$\\text{Standard deviation} = \\sqrt{np(1-p)}$$ However, there is no need to remember these formulae. We will learn to use R to do the hard work for us. We will demonstrate R’s binomial function to solve the following questions related to the vaccine scenario. We will assume p = .85 and n = 12. 1.1. What is the probability that the vaccine will work for 10 people? $$Pr(X = k)$$ The key to solving these types of problems is to understand the question. Let’s write it out. The question is asking $Pr(X=10)$ , assuming 12 trials and the vaccine to be effective 85% of the time. X is a short way to refer to the number of successes. We can visualise this in the following plot (the code will be introduced later). The height of each point line refers to the binomial probability of observing 0 to 12 successes in 12 trials assuming, p = 0.85. Each probability was calculated using the binomial PMF formula above. We can see 0 - 7 success all have probabilities below 0.05, while 10 and 11 successes have probabilities approximately 0.30. This makes sense as the expected, or mean value = np = 12*.85 = 10.2. If we added all the probabilities for each point line together, they would sum to 1. This is known as the total probability law. The Pr(X=10) has been coloured as a red line in the plot. We can quickly see Pr(X=10) will be about .30. Now we could use the formula given above to calculate the exact probability… $$Pr(X = 10) = \\binom{12}{10}.85^{10}(1-.85)^{12-10}$$ However, using an R function will do this a lot quicker and far more accurately. In R, we use the dbinom(x, size, prob) function. This function has three arguments: x : The value for k, or number of successes size : The number of trials for each experiment prob : The probability of success (p) For Question 1. We type the following command into R: dbinom(x = 10, size = 12, prob = 0.85) ### [1] 0.2923585 The answer is found to be $Pr(X = 10) = 0.29$. Given that the vaccine is 85% effective, it would be relatively common to find 10 successes in 12 randomly sampled people. In other words, there is a 29% chance that the vaccine will work for exactly 10/12 people. If you’re interested in reproducing the plot above, you can use the following R code. Its a little lengthy, but it should mostly make sense. ## Set binomial parameters. n \u003c- 12 p = .85 ## Define PMF to highlight - Pr(X \u003c x), Pr(X \u003e x), or Pr(a \u003c x \u003c b) ## Leave blank \"\" for no highlights x \u003c- 10 a \u003c- \"\" b \u003c- \"\" ## Set sequence of x values to plot Successes \u003c- seq(0,n) ## Calculate PMF PMF \u003c- dbinom(x = Successes, size = n, prob = p) ## Define points to highlight in plot highlight \u003c- ifelse(Successes \u003c= b \u0026 Successes \u003e= a | Successes == x, \"red\", \"blue\") ## Plot PMF plot(Successes, PMF, type = \"p\", main = paste(\"Binomial Distribution, n = \",n,\", p = \",p), col = highlight) lines(Successes,PMF, type = \"h\", col = highlight) grid() 1.2. What is the probability that the vaccine will work for 8 or less people? $$Pr(X \\leq k)$$ This is a slightly different question. We are asked","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:13:4","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Poisson Distribution The Poisson distribution is used to model the occurrence of discrete events over a specific period of time, t. The Poisson distribution has one parameter, $λ$, which is the expected, E(x), or mean, $μ$, number of events in a unit of time. For example, the expected daily number of patients for a doctor during winter might be $λ=16$. $λ$ can be adjusted to take into account different time periods using $μ=λt$. For example, the mean number of patients for the same doctor over a week in winter is $μ=λt=16∗7=112$. This assumes $λ$ is time constant. The mean, $μ$, and variance, $σ2$, of a Poisson random variable is simply $λ$. The Poisson distribution has the following probability mass function: $$Pr(X = k) = \\frac{e^{-\\mu}\\mu^k}{k!}$$ where, k = the number of occurrences, e = the exponential function, and ! is a factorial. $$ E(x) \u0026 Var(x) $$ The mean, μ,or expected value, E(X), variance, $\\sigma^2$, and standard deviation, $\\sigma$, for a Poisson distributed variable are as follows: $\\mu = E(X) = \\lambda$ $\\sigma^2 = Var(X) = \\lambda$ $\\sigma = \\sqrt{\\lambda}$ We will work through some Poisson problems using R. 2.1. What is the probability the doctor will see exactly 16 patients in a given day? $$Pr(X = x)$$ This is a simple one. We need to find $Pr(X=16)$. The red area in the following plot highlights the probability visually. The plot represents a PMF of the Poisson distribution with $\\lambda = 16$. In R, use the dpois(x, lambda) function, which has the following options: x: This is the x value you want to look up under the Poisson distribution lambda: This the mean rate over time, $\\mu$ Therefore, we use the following formula: dpois(x = 16, lambda = 16) ### [1] 0.09921753 The function dpois() computes the probability of observing a particular number of occurrences for a Poisson distribution. The answer to question 1 is Pr(X=16)=.099. We can use the following code to replicate the visualisation above: ## Set Poisson parameters. lambda \u003c- 16 time_multiplier \u003c- 1 mu \u003c- lambda*time_multiplier ## Define PMF to highlight - Pr(X \u003c x), Pr(X \u003e x), or Pr(a \u003c x \u003c b) ## Leave blank \"\" for no highlights x \u003c- 16 a \u003c- \"\" b \u003c- \"\" ## Set sequence of x values to plot Events \u003c- seq(ifelse(sign(round(mu-sqrt(mu)*4,0))==-1,0, round(mu-sqrt(mu)*4,0)), round(mu+sqrt(mu)*4,0)) ## Calculate PMF PMF \u003c- dpois(x = Events, lambda = mu) ## Define points to highlight in plot highlight \u003c- ifelse(Events \u003c= b \u0026 Events \u003e= a | Events == x, \"red\", \"blue\") ## Plot PMF plot(Events, PMF, type = \"p\", main = paste(\"Poisson Distribution, Mean = \",mu), col = highlight) lines(Events, PMF, type = \"h\", col = highlight) grid() 2.2. What is the probability the doctor will see 12 or less patients in a given day? $$Pr(X \\leq x)$$ This time we’re asked $Pr(X ≤ 12)$. Visually… In R… ppois(q = 12, lambda = 16) ### [1] 0.1931215 Notice that we need to use the function ppois(), which gives $Pr(X \u003c= x)$, rather than the earlier function dpois(), to solve for the cumulative probability, CMF, of the doctor seeing 0 to 12 patients in a given day. Remember the difference between these two functions. Visually, the Poisson CMF looks like the following: ## Set Poisson parameters. Pr(8 \\leq X \\leq 24) = 0.968 lambda \u003c- 16 time_multiplier \u003c- 1 mu \u003c- lambda*time_multiplier ## Define PMF to highlight - Pr(X \u003c x), Pr(X \u003e x), or Pr(a \u003c x \u003c b) ## Leave blank \"\" for no highlights x \u003c- \"\" a \u003c- 0 b \u003c- 12 ## Set sequence of x values to plot Events \u003c- seq(ifelse(sign(round(mu-sqrt(mu)*4,0))==-1,0, round(mu-sqrt(mu)*4,0)), round(mu+sqrt(mu)*4,0)) ## Calculate CMF CMF \u003c- ppois(q = Events, lambda = mu) ## Define points to highlight in plot highlight \u003c- ifelse(Events \u003c= b \u0026 Events \u003e= a | Events == x, \"red\", \"blue\") ## Plot PMF plot(Events, CMF, type = \"p\", main = paste(\"Poisson Distribution, Mean = \",mu), col = highlight) lines(Events, CMF, type = \"h\", col = highlight) grid() 2.3. What is the probability that the doctor will see less than or equal to 100 patients in a week? We need to ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:13:5","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Normal Distribution The continuous, normal, or Gaussian, distribution is ubiquitous in the field of statistics. Many random variables exhibit a normal distribution shape or, at least, do so approximately. Continuous variables that are known to have a normal distribution make determining the probability of certain events easy to calculate. Probabilities for normal distributions can be readily obtained from tables in the back of most statistics textbooks or functions built into spreadsheets and statistical packages. We will focus on using technology to calculate these probabilities. To illustrate the application of the normal distribution, we will consider looking at an example involving IQ or “intelligence” scores. IQ scores are believed to have a normal distribution in the population. Most people score close to the average, while few people score really low (e.g. those with learning disabilities) or really high (e.g. geniuses). The normal distribution has two parameters, a mean, $\\mu$, and a standard deviation, $\\sigma$. For IQ we would denote the theoretical normal distribution as follows: $$IQ \\sim N(\\mu, \\sigma) \\sim N(100,15)$$ The following info-graphic shows the theoretical normal distribution of IQ scores. The shaded areas refer to 1 standard deviation, $\\sigma$. Normal distributions have very specific properties. As you can see from the info graphic, 68% of a normal distribution falls within 1 standard deviation of the mean, $85\u003cx\u003c115$. From 1 to 2 standard deviations, $115\u003cx\u003c130$, 13.5% of values will fall. As the normal distribution is perfectly symmetric, we can also see that 13.5% of values will fall between -1 and -2 standard deviations, $70\u003cx\u003c85$. We also have 2.5% of values falling beyond 2 and -2 standard deviations, $x\u003c70$ and $x\u003e130$. Note that for continuous distributions, there is no distinction between $Pr(X\u003cx)$ and $(X \\leq x)$. These two statements are the same because for a continuous distribution the exact probability $Pr(X=x)$ always equals 0. This course will always use the \u003c sign in place of $\\leq$ because \u003c is quicker to type! So assuming IQ scores are theoretically normally distributed in the population, what types of questions can we answer? Let’s work through some examples to get an idea. Along the way, we will look at using simple functions in R to quickly and effectively answer these questions. 1. What is the probability that a random person from the population will have an IQ score less than 80? The question is asking $Pr(X\u003c80)$. We use X to represent a measurement for a random variable. If we plot the theoretical normal distribution and shade the area red that represents this probability, it would look like the following: So what’s the best way to find this probability? Use the built in functions of R. Specifically, we will use the function, pnorm(q, mean , sd , lower.tail = TRUE). This function has the following arguments: q: This is the value you want to look up under the normal distribution. mean: The population mean. sd: The population standard deviation (don’t confuse this with the variance). lower.tail: If TRUE = Pr(X\u003cx), the cumulative probability up to x . If false, Pr(X\u003ex) will be returned. TRUE is used by default, so it’s often ignored. Therefore, in R, we use the formula: pnorm(q = 80, mean = 100, sd = 15) ### [1] 0.09121122 We find Pr(X\u003c80)=0.091. This means we have a 9.1% chance of randomly selecting a person with an IQ below 80. To reproduce the plot see the code at the end of this section (It’s rather long). 2. What is the probability that you will randomly select a person from the population with an IQ score above 110? $$Pr(X \u003e x)$$ This time we need to find Pr(X \u003e 110). Visually… One way to solve this is to use the rule that Pr(X\u003ex) = 1 − Pr(X\u003cx). In R, we would write the formula as either: 1 - pnorm(q = 110, mean = 100, sd = 15) ### [1] 0.2524925 or… pnorm(q = 110, mean = 100, sd = 15, lower.tail = FALSE) ### [1] 0.2524925 Note the use of lower.tail = FALSE in the second fo","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:13:6","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"The Standard Normal Z-Distribution The standard normal z-distribution is sometimes used in statistics as it allows for probabilities to be looked up using standard tables available in textbooks. We need to know a little bit about the z-distribution, particularly the concept of standardisation, as it appears in a number of other modules. The standard normal distribution has the following properties: $$z \\sim N(0,1)$$ where z refers to a standard normal variable. Let’s look at an example. Thinking back to IQ scores, we can convert IQ scores to a standard normal variable using the equation: $$z = \\frac{x - \\mu}{\\sigma}$$ The z-score divides the difference between an x value and the mean by a population standard deviation. Therefore, z scores are standard deviations. So, suppose we are asked to find Pr(X\u003e110) using the standard normal distribution. First, we convert the IQ variable, x, to z, i.e. a z-score or standard deviation. $$z = \\frac{x - \\mu}{\\sigma} = \\frac{110 - 100}{15} = \\frac{10}{15} = .667$$ Therefore, an IQ score of 110 sits .667 standard deviations above the mean of 100. Simple! Now we can find Pr(Z\u003e.667) using the pnorm() function in R. 1 - pnorm(q = .667, mean = 0, sd = 1) ### [1] 0.2523861 or… pnorm(q = .667, mean = 0, sd = 1, lower.tail = FALSE) ### [1] 0.2523861 We find Pr(X\u003e110)=0.252. You will find the answer to be exactly the same as when we used: 1 - pnorm(q = 110, mean = 100, sd = 15) ### [1] 0.2524925 or pnorm(q = 110, mean = 100, sd = 15, lower.tail = FALSE) ### [1] 0.2524925 You will need to use standardisation techniques throughout the course. This section has aimed to give you a heads up. There is nothing special about the z-distribution. It’s just a way to standardise a variable so that common probability tables can used. To convert a z-score back to its original x value use the following equation: $$x = \\mu + \\sigma{}z$$ E.g… $$x=100+15(.667)=110$$ Normal Distribution Visualisation Code This is the code used to create the visualisations of the normal distribution. ## Set Normal distribution parameters. Use SD, not Var! mu \u003c- 100 sd \u003c- 15 ## Set values to highlight ## x: Pr(x \u003c x) or Pr(X \u003e x) ## a and b: Pr(a \u003c x \u003c b) x \u003c- 80 a \u003c- \"\" b \u003c- \"\" ## Highlight area under curve - Pr(X \u003c x) = \"less\", ## Pr(X \u003e x) = \"greater\", Pr(a \u003c x \u003c b) = \"between\" area \u003c- \"less\" ## Define area to highlight if (area == \"less\") { auc \u003c- c(0,dnorm(seq(mu-sd*4,x,sd*0.01), mean = mu, sd = sd),0) x_values \u003c- c(mu-sd*4,seq(mu-sd*4,x,sd*0.01),x) } else { if (area == \"greater\") { auc \u003c- c(0,dnorm(seq(x,mu+sd*4,sd*0.01), mean = mu, sd = sd),0) x_values \u003c- c(x,seq(x,mu+sd*4,sd*0.01),mu+sd*4) } else { if (area == \"between\") { auc \u003c- c(0,dnorm(seq(a,b,sd*0.01), mean = mu, sd = sd),0) x_values \u003c- c(a,seq(a,b,sd*0.01),b) } } } ## Create probability statement if (area == \"less\") { prob_statement \u003c- paste(\"Pr(X \u003c \",x,\") = \", round(pnorm(x,mu,sd),3), sep = \"\") } else { if (area == \"greater\") { prob_statement \u003c- paste(\"Pr(X \u003e \",x,\") = \", round(pnorm(x,mu,sd,lower.tail = FALSE),3), sep = \"\") } else { if (area == \"between\") { prob_statement \u003c- paste(\"Pr(\",a,\" \u003c x \u003c \",b,\") = \", round(pnorm(b,mu,sd) - pnorm(a,mu,sd),3), sep = \"\")} else { prob_statement \u003c- \"\" } } } ## Plot density curve(expr = dnorm(x,mu,sd), xlim = c(mu-sd*4,mu+sd*4), main = paste(\"Normal Distribution, Mean = \",mu,\", Sigma = \",sd), ylab = \"Density\") if (area != \"\") { polygon(x = x_values, y = auc, col = \"tomato\") text(x = mu-sd*4, y = dnorm(mu-sd/4,mu,sd), labels = prob_statement, pos = 4) } ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:13:7","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Module 4 - Exercises Complete the following module exercises (Submit your solutions via Blackboard) before the deadlines in order to obtain a participation mark (1%). The exercises aim to help develop your conceptual understanding and prepare you for the exam. Exam questions will be very similar to the module exercises (except in the exam you won’t be required to use R). Answers are available following submission. Working together in groups is encouraged, but please don’t give away the answers to other a students who haven’t done the work (they won’t be learning anything!). These exercises are designed to aid your learning. Exercise 1 The risk of females experiencing an anxiety disorder during a given 12-month period is approximately 1 in 5. Suppose a researcher plans to take o a random sample of females and monitor their anxiety over 12 months. Which probability . distribution can be used to model the expected number of females in a sample experiencing an anxiety disorder within this period? Binomial distribution F distribution Normal distribution Poisson Distribution Answer: Binomial distribution Exercise 2 If 20 females are randomly sampled, what is the probability that exactly 10 will experience an anxiety disorder during this 12-month period? (Round answer to 3 decimal places) dbinom(x = 10, size = 20, prob = 0.20) %\u003e% round(3) Exercise 3 If 20 females are randomly sampled, what is the probability that exactly 5 will experience an anxiety disorder? (Round answer to 3 decimal places) dbinom(x = 5, size = 20, prob = 0.20) %\u003e% round(3) Exercise 4 If 30 females are randomly sampled, what is the probability that exactly 5 will experience an anxiety disorder? dbinom(x = 5, size = 30, prob = 0.20) %\u003e% round(3) Exercise 5 If 20 females are randomly sampled, what is the probability that 5 or 6 will experience an anxiety disorder? (Round answer to 3 decimal places) dbinom(x = 5:6, size = 20, prob = 0.20) %\u003e% sum() %\u003e% round(3) Exercise 6 If 10 females are randomly sampled, what is the probability that 5 or more will experience an anxiety disorder? (Round answer to 3 decimal places) 1 - pbinom(q = 4, size = 10, prob = 0.20) %\u003e% round(3) Or, pbinom(q = 4, size = 10, prob = 0.20, lower.tail = FALSE) %\u003e% round(3) Exercise 7 If 20 females are randomly sampled, what is the probability that 2 or more will experience an anxiety disorder? (Round answer to 3 decimal places) 1 - pbinom(q = 1, size = 20, prob = 0.20) %\u003e% round(3) Or, pbinom(q = 1, size = 20 , prob = 0.20, lower.tail = FALSE) %\u003e% round(3) Exercise 8 If 20 females are randomly sampled, what is the probability that 5 or less will experience an anxiety disorder? (Round answer to 3 decimal places) pbinom(q = 5, size = 20, prob = 0.20) %\u003e% round(3) Exercise 9 If 30 females are randomly sampled, what is the probability that 10 or less will experience an anxiety disorder? (Round answer to 3 decimal places) pbinom(q = 10, size = 30, prob = 0.20) %\u003e% round(3) Exercise 10 Suppose the average number of people that become victim to a shark attack in Australia each year is 10. Which probability distribution can be used to model the annual expected number of shark attacks in Australia? Normal distribution Binomial distribution Poisson Distribution F distribution Answer: Poisson distribution Exercise 11 What is the expected number of shark attacks over a two year period? (Round answer to the nearest whole number) 2 * 10 Exercise 12 Assuming the rate of shark attacks in Australia remains constant over time, what is the probability that 10 people will be attacked within a given year? (Round answer to 3 decimal places) dpois(x = 10, lambda = 10) %\u003e% round(3) Exercise 13 Assuming the rate of shark attacks in Australia remains constant over time, what is the probability that 30 people will be attacked by a shark across two years? (Round answer to 3 decimal places) dpois(x = 30, lambda = 10*2) %\u003e% round(3) Exercise 14 Assuming the rate of shark attacks remains constant over time, what is the probab","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:13:8","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Module 5 Sampling: Randomly Representative ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:14:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Introduction to the Module This topic is covered in Module 5 of course website. Often a population is immeasurable. Therefore, statistical investigations must often rely on the use of a sample from the population. This module will introduce sampling from populations. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:14:1","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Learning Objectives The learning objectives associated with this module are: Describe the purpose of sampling for estimation and inference. Define and distinguish between different sampling methods. Define a sampling distribution for a statistic. Define the expected value and variance of a sampling distribution. Use technology to simulate and explore the characteristics of sampling distributions. Define and apply the Central Limit Theorem (CLT). ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:14:2","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Module Video This a nice video that explains the challenge of sampling in ecology research. The video discusses populations, samples and random samples. {% youtube nsMWvSuJm08 %} ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:14:3","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Populations and Samples In this module we will dive deeper into the world of inferential statistics first introduced back in Module 1. Recall, statistical inference refers to methods for estimating population parameters using sample data. A population is the larger group that a researcher wants to generalise their results to. For example, a researcher may need to know the average battery life for a new model of mobile phone, or estimate the average transfer speeds for a new computer hard disk drive. It would be too expensive or infeasible to test every unit manufactured. Therefore, the researcher must use another method. A researcher’s confidence in their ability to infer what’s happening in the population comes down to the quality of the sample and quality of the data collected. In this section we will deal with samples. A sample is a smaller subset of a population. If the sample is chosen appropriately, it can provide a fairly accurate account of the population. Why do we use samples? Cost, time and practical constraints often make measuring the population impossible. Think back to the mobile phone and hard disk drive example in the previous paragraph. When an entire population is measured, it is called a census. The Australian Bureau of Statistics (ABS) conducts the Australian Census every five years at an estimated cost of $440 million (Based on 2011 Census). As you can understand, this amount of time and money is well beyond the means of most statistical investigations. There are good and bad ways of gathering samples. Probability-based methods maximise the chances of gathering a randomly representative sample. Common probability-based methods include simple random sampling, cluster sampling and stratified sampling. Non-probability based methods make no effort to ensure the sample is randomly representative. The best example of these types of methods are convenience sampling, purposive sampling, quota sampling and snowballing. Let’s take a closer look at the probability-based methods. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:14:4","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Sampling Methods Simple Random Sampling (SRS) In SRS, every unit in a population has an equal chance of being selected. For example, every new model mobile phone manufactured has an equal chance of being selected to undertake a battery test. This is the most simple and effective probability-based sampling method. However, it can be tricky to implement. For example, if we were looking at the population, how do we get a list of every single Australian so you can ensure everyone has an equal chance of being selected? A phone book is a good start, but what about people without landlines? This course will focus mainly on simple random sampling. Watch the following video by Steve Mays for a nice overview of SRS. {% youtube yx5KZi5QArQ %} Stratified Sampling Stratified sampling divides the population into subpopulations, called strata (e.g. gender, age bands, ethnicity), and then takes a SRS from each strata proportional to the strata’s representation in the population. For example, the Australian population is approximately 49% male and 51% female. A stratified sample for gender would divide the population into males and females and then proceed to take SRSs of males and females so the resulting sample is approximately 49:51 male:female. Stratified sampling can be more complex as there is no limit to the number of strata and levels within the strata. For example, a researcher may wish to stratify the population by gender, age bands and ethnicity. This would result in a sample that is more likely to be representative, but would require substantially more time and effort. {% youtube sYRUYJYOpG0 %} Cluster Sampling Cluster sampling first divides the population into naturally occurring and homogeneous clusters, e.g. postcodes, towns, manufacturing batches, factories, etc. The investigator then randomly selects a defined number of clusters. For example, referring back to the hard disk drive example, the company may have manufactured 100 batches of hard disk drives on different days. They may decide to randomly select 10 batches which they define as the clusters. Using these randomly sampled clusters, the investigator would then proceed with the use of SRS within each cluster to select their sample. For example, the investigator might decide to randomly select 10 hard disk drives from each of the 10 batches making a total sample size of 100. Cluster sampling can be more economical and less time-consuming than SRS. This is because the researcher is required to perform SRS only within a limited number of clusters and not the entire population. {% youtube QOxXy-I6ogs %} Convenience Sampling Convenience sampling methods, or non-probabilistic sampling, make no effort to randomly sample from the population. Therefore, the degree to which a convenience sample is randomly representative to the population is always unknown. Convenience samples have a high probability of being biased. A biased sample is a sample that cannot be considered representative of the target population. It is possible for a convenience sample to be representative, but the probability is always unknown. Substantial caution must be placed on inferences drawn from the use of convenience samples. Regardless, convenience samples are probably the most common samples used in research due to their low cost and relative ease. Very few researchers have the time and money to use probabilistic methods. That’s not to say you shouldn’t try, but if you’re forced to use a convenience sample, you should always note its limitations. {% youtube MJjq0NILrnk %} It’s important to note that probability-based sampling methods do not guarantee a representative sample either. That’s why we say the sample is randomly representative. There is still uncertainty. This is particularly true for small samples. We can take another look at the info-graphic provided by Wild, Pfannkuch and Horton (2011) that looks at populations, samples and sample size. Note that Wild et al. are referring to probability-based","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:14:5","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Sampling Distributions Take a random sample from the population, of say size n=100, measure a quantitative variable on each unit and calculate the sample mean. Write down the sample mean in a data recording sheet. Now place the sample back into the population and take another random sample of the same size. Measure your variable, calculate the sample mean, and record its value in the same recording sheet. The sample mean won’t be the same, because it’s a slightly different sample. Remember, this is called sampling variability or error. Now, repeat this process many, many times, say one thousand. Now, take all the one thousand sample means you recorded and plot them using a histogram. This histogram of the sample means is an example of a sampling distribution. {% youtube z0Ry_3_qhDw %} A sampling distribution is a hypothetical distribution of a sample statistic, such as a mean, median or proportion, constructed through the repeated sampling from a population. A sampling distribution describes what would happen if we were to repeat a study many times over and for each replicated study we recorded a summary statistic. Sampling distributions are influenced by two major factors. The first factor is the underlying distribution of the random variable. For example, if your random variable is distributed normally, binomially, or exponentially, this will have an effect on the characteristics of the sampling distribution. The second major factor is the sample size n. The sample size of the hypothetical repeated studies has some interesting effects on the sampling distribution, as we will discover shortly. The Cal Poly Sampling Distribution Shiny app will allow you to commence exploring sampling distributions. Activity 1 Set the following inputs: Population Distribution: Normal Population mean: 0 Population standard deviation: 1 Sample size: 10 Statistic: Mean Number of samples: 1 Click Draw samples. This draws one random sample (n=10) from the population. The sample values are displayed in the first histogram. The sample mean is calculated and plotted on the sampling distribution of the mean plot. Number of samples: 1000 Click Draw samples. The app will quickly draw 1000 random samples and plot their means. Note the difference between a sample distribution versus a sampling distribution of the mean. Activity 2 Now let’s change the underlying population distribution and increase the sample size. Set the following inputs: Population Distribution: Left-skewed Population mean: 0 Population standard deviation: 1 Sample size: 100 Statistic: Mean Number of samples: 1000 Click Draw samples. Note that while the population distribution is heavily left-skewed, the sampling distribution of the means is not. You will learn more about this when we look at the central limit theorem. YouTube Data We will use the YouTube.csv data to explore the concepts of a sampling distribution. The data were originally sourced from the UCI Machine Learning Repository. The dataset contains the basic video characteristics of over 24,000 YouTube clips. The variables are as follows: id: Youtube vide id duration: duration of video bitrate: bitrate(total in Kbits) bitrate.video: bitrate(video bitrate in Kbits) height: height of video in pixles width: width of video in pixles framerate: actual video frame rate frame.rate.est.: estimated video frame rate codec: coding standard used for the video category: YouTube video category Population Distribution The Youtube.csv data will be treated as the unknown population. As the dataset contains over 24,000 video characteristics, this isn’t too difficult to imagine, even though the total population size of YouTube is much, much, much higher (I am yet to find a credible estimate! If you find one email me). For the sake of the example, we will imagine this to be the unknown reality that we are trying to estimate. We will look at estimating the average YouTube video duration, measured in seconds (sec). The data has been cleaned to enabl","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:14:6","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Exercises Exercises Complete the following module exercises (Submit your solutions via Canvas) before the deadlines in order to obtain a participation mark (1%). The exercises aim to help develop your conceptual understanding and prepare you for the exam. Exam questions will be very similar to the module exercises (except in the exam you won’t be required to use R). Answers are available following submission. Working together in groups is encouraged, but please don’t give away the answers to other students who haven’t done the work (they won’t be learning anything!). These exercises are designed to aid your learning. Exercise 1 Why do researchers use samples instead of populations? Because a random sample always approximates the population Because samples are always representative of the population Because measuring an entire population is generally impossible Because samples and populations are the same thing Answer: because measuring an entire population is generally impossible Exercise 2 How can a researcher guarantee selecting a representative sample from a population? By using random sampling By using cluster sampling By using stratified sampling There is no way to “guarantee” your sample is representative Answer: There is no way to “guarantee” your sample is representative. Random, cluster and stratified sampling can only ever maximise the probability of selecting a representative sample. Exercise 3 A researcher wants to gather a cluster sample of Australians to survey their exercise habits. Which of the following could be used as a cluster? State Postcode City/Town All of the above Answer: All of the above Exercise 4 Which of the following methods is considered an example of simple random sampling from the Australian population? Randomly sample postcodes and then randomly sample participants within the selected postcodes Divide the population up into age categories and then randomly select participants in each category proportional to the population distribution From a list of the entire population, randomly select the desired number of participants From the hometown of the researcher, approach people in a shopping centre Answer: From a list of the entire population, randomly select the desired number of participants Exercise 5 When is a sample said to be biased? When the sample is likely to not be representative of the population When the sample has a negative attitude towards the research being conducted When the sample is likely to be representative of the population When the sample is small Answer: When the sample is likely to not be representative of the population Sampling bias occurs when some members of a population are systematically more likely to be selected in a sample than others. It is also called ascertainment bias in medical fields. Sampling bias limits the generalizability of findings because it is a threat to external validity, specifically population validity. In other words, findings from biased samples can only be generalized to populations that share characteristics with the sample. Exercise 6 Large samples are representative samples. True or false? Answer: False. Even though a sample might be considered large, whether it is likely to be representative comes down to how it was selected. Large biased samples are still biased. Probability sampling techniques are the best way to maximise the probability of getting a representative sample. Exercise 7 Sampling Distribution Activity We will use the Sampling Distribution Shiny app hosted here - https://calpolystat1.shinyapps.io/sampling_distribution/ to explore the concept of a sampling distribution. For the app, set the following options: Population distribution: Normal Population mean: 100 Population standard deviation: 15 Sample size: 10 Statistic: Mean Number of samples: 1000 Click here to display population characteristics: Select this option. Display summaries of sampling distribution: Checked. Click the Draw Sample button. This will generate 1000 s","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:14:7","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Module 6 Estimating Uncertainty Confidently ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:15:0","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Overview Our best, and often only, estimate for a population parameter is the sample estimate. However, due to sampling variability, the sample estimate is always uncertain. Module 6 will introduce the concept of confidence intervals as an interval estimate that expresses the degree of uncertainty associated with sample statistics. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:15:1","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Learning Objectives The learning objectives associated with this module are: Differentiate between a point and interval estimate. Define the concept of a confidence interval. Discuss the major factors that impact the width of a confidence interval. Use technology to calculate confidence intervals for common statistics including means, proportions and rates. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:15:2","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Types of Inference Estimation: – Estimating or predicting the value of the parameter – “What is (are) the most likely values of μ or p?” Example: A consumer wants to estimate the average price of similar homes in her city before putting her home on the market. Estimation: Estimate μ, the average home price. Hypothesis Testing: (I will teach it on module 7) – Deciding about the value of a parameter based on some preconceived idea. – “Did the sample come from a population with m = 5 or p = .2?” Example: A manufacturer wants to know if a new type of steel is more resistant to high temperatures than an old type was. Hypothesis test: Is the new average resistance, μΝ equal to the old average resistance, $ μ_Ο $? ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:15:3","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Point and Interval Estimation Definitions An estimator is a rule, usually a formula, that tells you how to calculate the estimate based on the sample. Point estimation: A single number is calculated to estimate the parameter. The point estimate of a sample statistic, such as the mean, median, proportion or rate are single, or point values. They are often our best estimate for a population parameter, but do not express the degree of uncertainty for an estimate associated with its sampling variability. Point estimates should be accompanied by additional information to assist with drawing inferences about the population. Interval estimation: Two numbers are calculated to create an interval within which the parameter is expected to lie. Interval estimation serves to overcome this limitation. The idea is to supplement the point estimate with an interval that reflects the degree of uncertainty associated with a statistic. The most common type of interval estimator is called the confidence interval, or CI for short. Some definitions A point estimate is a single number, a confidence interval provides additional information about the variability of the estimate (based on the sampling distribution) Properties of Point Estimators Since an estimator is calculated from sample values, it varies from sample to sample according to its sampling distribution. An estimator is unbiased if the mean of its sampling distribution equals the parameter of interest. – It does not systematically overestimate or underestimate the target parameter. Of all the unbiased estimators, we prefer the estimator whose sampling distribution has the smallest spread or variability. Measuring the Goodness of an Estimator The distance between an estimate and the true value of the parameter is the error of estimation. NOTE: The distance between the bullet and the bull’s-eye. The Margin of Error For unbiased estimators with normal sampling distributions, 95% of all point estimates will lie within 1.96 standard deviations of the parameter of interest. Margin of error: The maximum error of estimation, calculated as Confidence Intervals An interval gives a range of values: Takes into consideration variation in sample statistics from sample to sample Gives information about “closeness” to unknown population parameters Stated in terms of level of confidence – e.g. 95% confident, 99% confident – Can never be 100% confident Let’s consider a typical confidence interval using a hypothetical example. Let’s assume height is normally distributed in the population with a standard deviation of 7 cm (This is unrealistic as we often do not know the population SD, but let’s go with it for now). An investigator takes a random sample of 10 peoples’ height (cm). Say the mean of the sample was found to be 176.5 cm. To calculate a confidence interval for the mean of a normally distributed variable with a known standard deviation, we use the following formula: $$\\bar{x} \\pm z_{1 - (\\alpha/2)}\\frac{\\sigma}{\\sqrt{n}}$$ We need to discuss the z critical value, Z1−(α/2), in the above formula. This value is obtained from the standard normal distribution. Recall, the standard normal distribution has the following properties: $$z \\sim N(0,1)$$ The α value refers to what is known as the significance level. Almost all studies will use a standard level of α=0.05. The α value is based on the level of the confidence interval. A CI is defined as 100(1−α) CI. If we use α=0.05, this means we’re going to calculate a 100(1−0.05) = 95% CI. The z critical value in the confidence interval formula is found by looking up the z-value associated with the 1−α/2=1−0.05/2=1−0.025=.975 percentile of the standard normal distribution. We might write: $$Pr(Z \u003c z) = .975$$ We dealt with solving a similar problem in Module 4. Fortunately, this is easy to solve in R. To solve the formula above we type the following into R: qnorm(p = .975) ### [1] 1.959964 Note, that if we don’t specify the mean and standard deviation for qnorm(","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:15:4","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Confidence Intervals Before we dig deeper, you need to have a definition of a CI in the back of your mind. When we refer to confidence in statistics, we need to understand very carefully what this means. In this course we will use a strict frequentist definition. Not all statistics instructors will be this stringent and in the past you may have been taught something different. However, for my course assessment, I expect you to understand and use this definition. 100(1−α)% CI, is an interval estimate for a population parameter, based on a given sample statistic, where if samples of a certain size n were repeatedly drawn from the population and a CI for each sample’s statistic was calculated, 100(1−α)% of these intervals would capture the population parameter, whereas the other 100(α)% would not. Now with this strange and long-winded definition in mind, let’s start digging deeper into the theory. The General Process The general formula for all confidence intervals is: $$ Point Estimate ± (Critical Value)(Standard Error) $$ Where: Point Estimate is the sample statistic estimating the population parameter of interest Critical Value is a table value based on the sampling distribution of the point estimate and the desired confidence level Standard Error is the standard deviation of the point estimate ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:15:5","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Theory We will explore confidence interval theory using a simulation and visualisation. We will first make some assumptions about the population. Let’s assume height is normally distributed with a mean of 175 and a standard deviation of 7: $$Height∼N(175,7)$$ As investigators, we don’t tend to know population parameters in advance, and hence the reason we need to gather a sample and estimate it. However, for the purpose of this lesson, we need to assume these values. Now imagine drawing 100 random samples of size 10 from the population. For each of the 100 samples, you calculate the sample mean and 95% CI. You can plot all these means and 95% CIs like in Plot 1 below. Plot 2 and 3 repeat the same procedure, each displaying the means and 95% CIs of 100 random samples of size 10. At the top of each plot, the percentage of CIs that miss capturing the population mean, μ=175, are reported. Missed intervals are coloured red. For Plot 1, we see that Missed = 3 % of the CIs missed μ=175, for Plot 2, Missed = 5 %, and for Plot 3, Missed = 4 %. If we repeated this for many thousands of plots and samples, what do you think this missed percentage would average? If you guessed 5%, you’re already on your way to understanding CIs. The 5% of CIs that will miss capturing the population mean is our α=0.05. Think carefully about the plots above and now re-read the CI definition: 100(1−α)% CI, is an interval estimate for a population parameter, based on a given sample statistic, where if samples of a certain size n were repeatedly drawn from the population and a CI for each sample’s statistic was calculated, 100(1−α)% of these intervals would capture the population parameter, whereas the other 100(α)% would not. CIs should now be starting to make a little more sense. Basically, CIs are constructed in a way that in the long run, a certain percentage (e.g. 95%) of CIs calculated by repeating the same random sampling procedure will capture a population parameter, for example μ=175. CIs can be calculated for a wide range of statistics using formulaic approaches. However, the methods vary depending on the type of statistic being estimated and the assumptions we make about the population from which the data are drawn. Later sections in this module explain calculating CIs for various situations and statistics. ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:15:6","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Influencing Factors Sample Size Look at the formula for a CI for the mean of a normally distributed variable with a known standard deviation: $$\\bar{x} \\pm z_{1 - (\\alpha/2)}\\frac{\\sigma}{\\sqrt{n}}$$ You can see the standard error of the mean in the formula: $$SE = \\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{n}}$$ Therefore, it comes as no surprise that CIs share some of the same rules as sampling distributions. Consider the following plots. Note that the number of simulated samples have been increased to 1000 so we get better estimates of the “Missed” percentage. We know from the previous module that sample size shares an inverse relationship with SE. As N increases, SE decreases. Moving from Plot 1 to Plot 3, sample sizes are 10, 50 and100. What happens to the width of the CIs as sample size increases? We see a dramatic decrease in the width of the intervals. Why? Mathematically, as sample size increases, the SE in the CI formula decreases, meaning that the lower and upper bounds fall closer to the sample mean. For example, if in the height example the investigator had used a sample size of 30, the confidence interval would become: $$176.5 \\pm z_{1 - (\\alpha/2)}\\frac{\\sigma}{\\sqrt{n}} = 1.96\\frac{7}{\\sqrt{30}} = 2.50$$ and therefore, x¯=176.5, 95% CI [174.00, 179.00]. This 95% CI is narrower than the interval calculated for n=10, 95% CI [172.16, 180.84]. Conceptually, larger random samples are better estimates of the population and therefore, we can be more certain in their estimates over the use of smaller samples. Confidenece Level Expressed as a percentage (\u003c100%) Suppose confidence level = 95% – Also written (1 - α) = 0.95, (so α = 0.05) A relative frequency interpretation: – 95% of all the confidence intervals that can be constructed will contain the unknown true parameter A specific interval either will contain or will not contain the true parameter – No probability involved in a specific interval While the 95% CI is the most common, it is possible to use other levels of confidence. Let’s compute a 90% and 99% confidence interval for the sample mean height when n=10. This is depicted in the following plot. Plot 1 reports a 99% CI, Plot 2, 95% CI, and Plot 3, 90% CI. As you can see, higher levels of confidence are associated with wider intervals. This makes sense. If you want to be more confident about capturing a population parameter, cast a wider interval! Confidence Interval for μ (σ Known) Assumptions – Population standard deviation σ is known – Population is normally distributed – If population is not normal, use large sample (n \u003e 30) Confidence interval estimate: $$ \\bar{X} \\pm Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}$$ where $ \\bar {X} $ is the point estimate, $ Z_{\\alpha/2} $ is the normal distribution critical value for a probability of $ \\alpha/2 $ in each tail is the standard error. Finding the Critical Value, $ Z_{\\alpha/2} $ Commonly used levels are 90%, 95% and 99% Intervals and Level of Confidence Now let’s look at the formula. We need to change the z critical value for the confidence interval formula: $$\\bar{x} \\pm z_{1 - (\\alpha/2)}\\frac{\\sigma}{\\sqrt{n}}$$ We will begin with the 90% CI. This CI width corresponds to a significance level of α=0.1. We need to find Pr(Z\u003cz)=1−α/2=1−0.1/2=1−0.05=.95. In R: qnorm(p = .95) ### [1] 1.644854 The z-value is found to be 1.64. Now completing the 90% CI equation: $$176.5 \\pm z_{1 - (\\alpha/2)}\\frac{\\sigma}{\\sqrt{n}} = 1.64\\frac{7}{\\sqrt{10}} = 3.64$$ We calculate x¯=176.5, 90% CI [172.86, 180.14]. Now compare this 90% CI to the 95% CI [172.16, 180.84]. The 90% CI is narrower. Why? Because as we are less certain about the CI capturing the population parameter in the long run, the width of the confidence interval will reduce. Think of it like casting a smaller net to capture the population mean. For a 99% CI, we must change the z critical value again. This CI width corresponds to a significance level of α=0.01. Therefore, we need to find Pr(Z\u003cz)=1−α/2=1−0.01/2=1−0.005=.995. In R","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:15:7","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Calculating Confidence Intervals Pizza Data This section will compare the mean diameter for pizzas made by Dominos and Eagle Boys. This is an issue close to many students’ hearts. Eagle Boys claim their pizzas are larger than their main competitor, Dominos. The Pizza dataset contains the diameters (cm) of 125 random pizzas from each company. The dataset is available from the data repository. You can read all about the data here. Specifically, the Pizza dataset contains the following variables: ID: An identifier Store: The pizza store/company; one of Dominos or EagleBoys Crust: The crust type for the pizza; DeepPan, Mid and Thin. Topping: The pizza topping: BBQMeatlovers, Hawaiian and Supreme Diameter: The pizza diameter in centimetres Comparing Means We start with the descriptive statistics and histograms comparing the pizza diameters (cm) of the two companies. library(dplyr) Pizza %\u003e% group_by(Store) %\u003e% summarise(Min = min(Diameter,na.rm = TRUE), Q1 = quantile(Diameter,probs = .25,na.rm = TRUE), Median = median(Diameter, na.rm = TRUE), Q3 = quantile(Diameter,probs = .75,na.rm = TRUE), Max = max(Diameter,na.rm = TRUE), Mean = mean(Diameter, na.rm = TRUE), SD = sd(Diameter, na.rm = TRUE), n = n(), Missing = sum(is.na(Diameter))) ### # A tibble: 2 x 10 ### Store Min Q1 Median Q3 Max Mean SD n Missing ### \u003cfct\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cint\u003e \u003cint\u003e ### 1 Dominos 25.5 26.6 26.9 28.8 29.7 27.4 1.17 125 0 ### 2 EagleBoys 26.6 28.8 29.1 29.5 31.1 29.2 0.626 125 0 library(lattice) library(ggplot2) Pizza %\u003e% histogram(~Diameter | Store, data = .,layout=c(1,2)) There appears to be a strange looking distribution for the Dominos’ pizzas. The distribution appears to be bi-modal. Does the crust type has something to do with it? Pizza %\u003e% group_by(Store, Crust) %\u003e% summarise(Min = min(Diameter,na.rm = TRUE), Q1 = quantile(Diameter,probs = .25,na.rm = TRUE), Median = median(Diameter, na.rm = TRUE), Q3 = quantile(Diameter,probs = .75,na.rm = TRUE), Max = max(Diameter,na.rm = TRUE), Mean = mean(Diameter, na.rm = TRUE), SD = sd(Diameter, na.rm = TRUE), n = n(), Missing = sum(is.na(Diameter))) ### # A tibble: 6 x 11 ### # Groups: Store [2] ### Store Crust Min Q1 Median Q3 Max Mean SD n Missing ### \u003cfct\u003e \u003cfct\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cint\u003e \u003cint\u003e ### 1 Dominos DeepPan 26.0 26.4 26.6 26.8 28.8 26.7 0.462 40 0 ### 2 Dominos Mid 25.5 26.6 26.7 27.0 28.9 26.8 0.510 42 0 ### 3 Dominos Thin 25.6 28.8 29.0 29.2 29.7 28.8 0.801 43 0 ### 4 EagleBoys DeepPan 28.2 28.7 29.0 29.4 30.4 29.1 0.479 43 0 ### 5 EagleBoys Mid 26.6 28.6 28.8 29 29.8 28.8 0.483 43 0 ### 6 EagleBoys Thin 28.5 29.4 29.7 30.0 31.1 29.7 0.550 39 0 The bi-modal appearance of the Dominos’ pizza diameter can be accounted for by the large difference between Dominos’ DeepPan and Mid pizzas when compared to their This pizzas. Because of this confounding variable, let’s focus on comparing only the thin crusts between Eagle Boys and Dominos. Pizza_thin \u003c- Pizza %\u003e% filter(Crust == \"Thin\") Pizza_thin %\u003e% group_by(Store) %\u003e% summarise(Min = min(Diameter,na.rm = TRUE), Q1 = quantile(Diameter,probs = .25,na.rm = TRUE), Median = median(Diameter, na.rm = TRUE), Q3 = quantile(Diameter,probs = .75,na.rm = TRUE), Max = max(Diameter,na.rm = TRUE), Mean = mean(Diameter, na.rm = TRUE), SD = sd(Diameter, na.rm = TRUE), n = n(), Missing = sum(is.na(Diameter))) ### # A tibble: 2 x 10 ### Store Min Q1 Median Q3 Max Mean SD n Missing ### \u003cfct\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cint\u003e \u003cint\u003e ### 1 Dominos 25.6 28.8 29.0 29.2 29.7 28.8 0.801 43 0 ### 2 EagleBoys 28.5 29.4 29.7 30.0 31.1 29.7 0.550 39 0 Pizza_thin %\u003e% boxplot(Diameter ~ Store, data = .) There are also a few outliers that we should pay some attention to. We have four values in the Dominos’ group and one in the Eagle Boys’ group. These outliers can have unwanted effects on our estimates, so it is best to deal with them at this point. There are a few ways that we could do this. A simple approach is to exclude th","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:15:8","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Determining Sample Size for the Mean To determine the required sample size for the mean, you must know: The desired level of confidence (1 - α), which determines the critical value, Zα/2 The acceptable sampling error, e The standard deviation, σ ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:15:9","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":["MATH1324"],"content":"Example If σ = 45, what sample size is needed to estimate the mean within ± 5 with 90% confidence? ","date":"2021-03-03","objectID":"/introductiontostatistics-appliedanalytics/:15:10","tags":null,"title":"Introduction to Statistics (Applied Analytics)","uri":"/introductiontostatistics-appliedanalytics/"},{"categories":null,"content":"Introduction Although data analysis is hidden behind the business system, it has a very important role. The results of data analysis play a pivotal role in decision-making and business development. With the development of big data technology, the exposure of proper terms such as data mining and data exploration is getting higher and higher, but before big data analysis systems similar to the Hadoop become popular, data analysis work has undergone considerable development, especially data analysis based on BI systems already has very mature and stable technical solutions and ecosystems. Cube is a higher-level business model abstraction. A variety of operations can be performed on Cube, such as drilling up, drilling down, and slicing. Most BI systems are based on relational databases. Relational databases use SQL statements to operate, but SQL is relatively weak in terms of multi-dimensional operation and analysis. So Cube has its own unique query language MDX, which has more expressions. Strong multi-dimensional performance capabilities, so the analysis system with Cube as the core basically occupies half of the data statistical analysis. Most database service vendors directly provide BI package software services, and an Olap analysis system can be easily built. However, the problems of BI gradually emerged over time: BI systems are more focused on analyzing business data with high-density and high-value structured data, and are very weak in processing unstructured and semi-structured data, such as image, text, and audio storage and analysis. Because the data warehouse is structured storage, we usually call the ETL process when data enters the data warehouse from other systems. ETL actions are strongly bound to the business, and a dedicated ETL team is usually required to connect with the business and decide how Perform data cleaning and conversion. With the increase of heterogeneous data sources, for example, if there are data sources such as videos, texts, pictures, etc., to analyze the data content and enter the data warehouse, very complicated ETL programs are required, which causes ETL to become too large and bloated. When the amount of data is too large, performance will become a bottleneck, and it will show obvious difficulty in the TB/PB level of data. The database paradigm and other constraint rules focus on solving the problem of data redundancy to ensure data consistency, but for data warehouses, we do not need to modify the data and guarantee consistency. In principle, the data warehouse The original data is read-only, so these constraints will become a factor affecting performance. The pre-supposition and processing of the data by the ETL action results in the data obtained by the machine learning part as hypothetical data, so the effect is not ideal. For example, if you need to use a data warehouse to mine abnormal data, you need to clearly define the feature data that needs to be extracted when the data is stored in the database through ETL, otherwise it cannot be structured into the database, but in most cases it needs to be extracted based on heterogeneous data Out characteristics. Under a series of problems, the big data analysis platform led by the Hadoop system has gradually shown its superiority, and the ecosystem surrounding the Hadoop system has also continued to grow. For the Hadoop system, it has fundamentally solved the bottleneck of the traditional data warehouse. The problem, but also brings a series of problems: Upgrading from a data warehouse to a big data architecture does not have a smooth evolution, which is basically equal to overturning and redoing. Distributed storage under big data emphasizes the read-only nature of data, so similar to Hive, HDFS storage methods do not support update, and HDFS write operations do not support parallelism. These characteristics lead to certain limitations. The data analysis platform based on big data architecture focuses on solving the bottlenecks faced by tra","date":"2021-03-01","objectID":"/bigdataarchitectures/:1:0","tags":["Machine Learning","Big Data"],"title":"4 big data architectures, Data Streaming, Lambda architecture, Kappa architecture and Unifield architecture","uri":"/bigdataarchitectures/"},{"categories":null,"content":"Traditional big data architecture The reason why it is called the traditional big data architecture is because its positioning is to solve the problems of traditional BI. Simply put, the data analysis business has not undergone any changes, but because of data volume and performance issues, the system cannot be used normally and needs to be upgraded. This type of architecture is to solve this problem. It can be seen that it still retains the ETL action, and enters the data storage through the ETL action. Advantages: Simple and easy to implement as per BI system concerns, the basic methodology has not changed. The only change is the selection of technology, replacing the BI components with the big data architecture. Disadvantages: For big data, there is no such complete Cube architecture under BI. Although kylin is currently available, the limitations of kylin are very obvious. It is far from the flexibility and stability of Cube under BI, so it is flexible in business support The degree is not enough, so for scenes with a large number of reports or complex drilling, too much manual customization is required. At the same time, the architecture is still mainly batch processing and lacks real-time support. Applicable scenarios: Data analysis needs are still dominated by BI scenarios, but due to issues such as data volume and performance, they cannot meet daily use. ","date":"2021-03-01","objectID":"/bigdataarchitectures/:2:0","tags":["Machine Learning","Big Data"],"title":"4 big data architectures, Data Streaming, Lambda architecture, Kappa architecture and Unifield architecture","uri":"/bigdataarchitectures/"},{"categories":null,"content":"Data Streaming architecture As compare to traditional big data architecture, the data streaming architecture is typically radical. The batch processing is directly removed, and the data is processed in the form of streams throughout the entire process. Therefore, there is no ETL at the data access end, and it is replaced with a data channel. The data processed by stream processing is directly pushed to consumers in the form of messages. Although there is a storage part, the storage is more stored in the form of windows, so the storage does not occur in the data lake, but in the peripheral system. Advantages: There is no bloated ETL process, and the effectiveness of the data is very high. Disadvantages: For streaming architecture, there is no batch processing, so data replay and historical statistics cannot be well supported. For offline analysis, only analysis within the window is supported. Applicable scenarios: You can use this as early warning, the different monitoring aspects and the data validity period requirements. ","date":"2021-03-01","objectID":"/bigdataarchitectures/:3:0","tags":["Machine Learning","Big Data"],"title":"4 big data architectures, Data Streaming, Lambda architecture, Kappa architecture and Unifield architecture","uri":"/bigdataarchitectures/"},{"categories":null,"content":"Lambda architecture Lambda architecture is a pivotal architecture in big data systems. Most architectures are basically Lambda architecture or architectures based on its variants. Lambda’s data channel is divided into two branches real-time streaming and offline. Real-time streaming is basically depend on much of the streaming architecture to ensure its real-time performance, while offline is mainly batch processing to ensure final consistency. In order to ensure the effectiveness of streaming channel processing, incremental calculation is the main auxiliary reference, while the batch processing layer performs full calculations on the data to ensure its final consistency. Therefore, the outermost layer of Lambda has a real-time layer and an offline layer. The action of merging, this action is a very important action in Lambda, the general idea of merging is as follows: Advantages: both real-time and offline, covering the data analysis scenarios very well. Disadvantages: Although the offline layer and the real-time stream face different scenarios, their internal processing logic is the same, so there are a lot of honors and duplicate modules. Applicable scenarios: There are both real-time and offline requirements. ","date":"2021-03-01","objectID":"/bigdataarchitectures/:4:0","tags":["Machine Learning","Big Data"],"title":"4 big data architectures, Data Streaming, Lambda architecture, Kappa architecture and Unifield architecture","uri":"/bigdataarchitectures/"},{"categories":null,"content":"Kappa architecture The Kappa architecture is optimized on the basis of Lambda, combining the real-time and streaming parts, and replacing the data channel with a message queue. Kappa architecture uses stream processing as mainstay, but the data is stored at the data lake level. When massive offline analytics and various other multiple calculations is required, the data of the data lake can be transferred through the message queue again. . Advantages: The Kappa architecture solves the redundant part of the Lambda architecture. It is designed with an extraordinary idea of replaying data. The entire architecture is very simple. Disadvantages: Although the Kappa architecture looks concise, it is relatively difficult to implement, especially for the data replay part. Applicable scenarios: It provides features like Lambda architecture. ","date":"2021-03-01","objectID":"/bigdataarchitectures/:5:0","tags":["Machine Learning","Big Data"],"title":"4 big data architectures, Data Streaming, Lambda architecture, Kappa architecture and Unifield architecture","uri":"/bigdataarchitectures/"},{"categories":null,"content":"Unifield architecture All of the above architectures are mainly focused on massive data processing, while the Unifield architecture is more radical, combining machine learning and data processing. At the core, Unifield is still based on Lambda, but it has been transformed to stream processing. A new machine learning layer has been added to the layer. The data enters and the data lake through the data channel, a new model training part is added and it is used in the streaming layer. This streaming layer uses the model and continuously training the model. Advantages: Unifield architecture provides a set of architecture solutions that combine data analysis and machine learning, which is a very good solution to the problem of how to combine machine learning with data platforms. Disadvantages: Unifield architecture is more complicated to implement. For machine learning architecture, from software package to hardware deployment, there is a very big difference from the data analysis platform, so the difficulty coefficient in the implementation process is higher. Applicable scenarios: There is a large amount of data to be analyzed, and there is a very large demand or plan for the convenience of machine learning. ","date":"2021-03-01","objectID":"/bigdataarchitectures/:6:0","tags":["Machine Learning","Big Data"],"title":"4 big data architectures, Data Streaming, Lambda architecture, Kappa architecture and Unifield architecture","uri":"/bigdataarchitectures/"},{"categories":["ISYS1055"],"content":"Introduction In this tutorial, you will learn SQLite step by step through extensive hands-on practices. This SQLite tutorial is designed for developers who want to use SQLite as the back-end database or to use SQLite to manage structured data in applications including desktop, web, and mobile apps. SQLite is an open-source, zero-configuration, self-contained, stand-alone, transaction relational database engine designed to be embedded into an application. ","date":"2021-03-01","objectID":"/sqlite/:1:0","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Getting started with SQLite You should go through this section if this is the first time you have worked with SQLite. Follow these 4-easy steps to get started with SQLite fast. ","date":"2021-03-01","objectID":"/sqlite/:2:0","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"First, help you answer the first and important question: what is SQLite? You will have a brief overview of SQLite. What is SQLite SQLite is a software library that provides a relational database management system. The lite in SQLite means lightweight in terms of setup, database administration, and required resources. SQLite has the following noticeable features: self-contained, serverless, zero-configuration, transactional. Serverless Normally, an RDBMS such as MySQL, PostgreSQL, etc., requires a separate server process to operate. The applications that want to access the database server use TCP/IP protocol to send and receive requests. This is called client/server architecture. The following diagram illustrates the RDBMS client/server architecture: SQLite does NOT work this way. SQLite does NOT require a server to run. SQLite database is integrated with the application that accesses the database. The applications interact with the SQLite database read and write directly from the database files stored on disk. The following diagram illustrates the SQLite server-less architecture: Self-Contained SQLite is self-contained means it requires minimal support from the operating system or external library. This makes SQLite usable in any environment especially in embedded devices like iPhones, Android phones, game consoles, handheld media players, etc. SQLite is developed using ANSI-C. The source code is available as a big sqlite3.c and its header file sqlite3.h. If you want to develop an application that uses SQLite, you just need to drop these files into your project and compile it with your code. Zero-configuration Because of the serverless architecture, you don’t need to “install” SQLite before using it. There is no server process that needs to be configured, started, and stopped. In addition, SQLite does not use any configuration files. Transactional All transactions in SQLite are fully ACID-compliant. It means all queries and changes are Atomic, Consistent, Isolated, and Durable. In other words, all changes within a transaction take place completely or not at all even when an unexpected situation like application crash, power failure, or operating system crash occurs. sqlite\u003e .mode column sqlite\u003e .header on sqlite\u003e .read ./commands.txt AlbumId Title 156 …And Justice For All 257 20th Century Masters - The Millennium Collection: The Best of Scorpions 296 A Copland Celebration, Vol. I 94 A Matter of Life and Death 95 A Real Dead One 96 A Real Live One 285 A Soprano Inspired 139 A TempestadeTempestade Ou O Livro Dos Dias 203 A-Sides 160 Ace Of Spades sqlite\u003e SQLite distinctive features SQLite uses dynamic types for tables. It means you can store any value in any column, regardless of the data type. SQLite allows a single database connection to access multiple database files simultaneously. This brings many nice features like joining tables in different databases or copying data between databases in a single command. SQLite is capable of creating in-memory databases that are very fast to work with. ","date":"2021-03-01","objectID":"/sqlite/:2:1","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Second, show you step by step how to download and install the SQLite tools on your computer Download SQLite tools To download SQLite, you open the download page of the SQlite official website. First, go to the https://www.sqlite.org website. Second, open the download page https://www.sqlite.org/download.html SQLite provides various tools for working across platforms e.g., Windows, Linux, and Mac. You need to select an appropriate version to download. Run SQLite tools Installing SQLite is simple and straightforward. open the Ternal. [yanboyang713@boyang ~]$ sqlite3 SQLite version 3.35.1 2021-03-15 16:53:57 Enter \".help\" for usage hints. Connected to a transient in-memory database. Use \".open FILENAME\" to reopen on a persistent database. sqlite\u003e You can type the .help command from the sqlite\u003e prompt to see all available commands in sqlite3.2. you can type the .help command from the sqlite\u003e prompt to see all available commands in sqlite3. sqlite\u003e .help .archive ... Manage SQL archives .auth ON|OFF Show authorizer callbacks .backup ?DB? FILE Backup DB (default \"main\") to FILE .bail on|off Stop after hitting an error. Default OFF .binary on|off Turn binary output on or off. Default OFF To quit the sqlite\u003e, you use .quit command as follows: sqlite\u003e .quit [yanboyang713@boyang ~]$ Install SQLite GUI tool The sqlite3 shell is excellent… However, sometimes, you may want to work with the SQLite databases using an intuitive GUI tool. There are many GUI tools for managing SQLite databases available ranging from freeware to commercial licenses. SQLiteStudio The SQLiteStudio tool is a free GUI tool for managing SQLite databases. It is free, portable, intuitive, and cross-platform. SQLite tool also provides some of the most important features to work with SQLite databases such as importing, exporting data in various formats including CSV, XML, and JSON. ","date":"2021-03-01","objectID":"/sqlite/:2:2","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Third, introduce you to an SQLite sample database and walk you through the steps of using the sample database for practicing. Introduction to chinook SQLite sample database We provide you with the SQLite sample database named chinook. The chinook sample database is a good database for practicing with SQL, especially SQLite. The following database diagram illustrates the chinook database tables and their relationships. Chinook sample database tables There are 11 tables in the chinook sample database. employees table stores employees data such as employee id, last name, first name, etc. It also has a field named ReportsTo to specify who reports to whom. customers table stores customers data. invoices \u0026 invoice_items tables: these two tables store invoice data. The invoices table stores invoice header data and the invoice_items table stores the invoice line items data. artists table stores artists data. It is a simple table that contains only the artist id and name. albums table stores data about a list of tracks. Each album belongs to one artist. However, one artist may have multiple albums. media_types table stores media types such as MPEG audio and AAC audio files. genres table stores music types such as rock, jazz, metal, etc. tracks table stores the data of songs. Each track belongs to one album. playlists \u0026 playlist_track tables: playlists table store data about playlists. Each playlist contains a list of tracks. Each track may belong to multiple playlists. The relationship between the playlists table and tracks table is many-to-many. The playlist_track table is used to reflect this relationship. Download SQLite sample database You can download the SQLite sample database here. In case you want to have the database diagram for reference, you can download both black\u0026white and color versions in PDF format. How to connect to SQLite sample database The sample database file is ZIP format, therefore, you need to extract it to a folder chinook.db Using the following command to connect to the chinook sample database location. [yanboyang713@boyang chinook]$ pwd /home/yanboyang713/Documents/study/ISYS1055-DatabaseConcepts/chinook [yanboyang713@boyang chinook]$ sqlite3 ./chinook.db SQLite version 3.35.1 2021-03-15 16:53:57 Enter \".help\" for usage hints. sqlite\u003e Trying a simple command e.g., .tables to view all the tables available in the sample database. sqlite\u003e .tables albums employees invoices playlists artists genres media_types tracks customers invoice_items playlist_track ","date":"2021-03-01","objectID":"/sqlite/:2:3","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Finally, guide you on how to use the sqlite3 commands Connect to an SQLite database To start the sqlite3, you type the sqlite3 as follows: [yanboyang713@boyang chinook]$ sqlite3 SQLite version 3.35.1 2021-03-15 16:53:57 Enter \".help\" for usage hints. Connected to a transient in-memory database. Use \".open FILENAME\" to reopen on a persistent database. sqlite\u003e By default, an SQLite session uses the in-memory database, therefore, all changes will be gone when the session ends. To open a database file, you use the .open FILENAME command. The following statement opens the chinook.db database: sqlite\u003e .open ./chinook.db If you want to open a specific database file when you connect to the SQlite database, you use the following command: [yanboyang713@boyang chinook]$ sqlite3 ./chinook.db SQLite version 3.35.1 2021-03-15 16:53:57 Enter \".help\" for usage hints. sqlite\u003e If you start a session with a database name that does not exist, the sqlite3 tool will create the database file. For example, the following command creates a database named boyang [yanboyang713@boyang chinook]$ sqlite3 boyang.db SQLite version 3.35.1 2021-03-15 16:53:57 Enter \".help\" for usage hints. Show all available commands and their purposes To show all available commands and their purpose, you use the .help command as follows: .help Show databases in the current database connection To show all databases in the current connection, you use the .databases command. The .databases command displays at least one database with the name: main. For example, the following command shows all the databases of the current connection: sqlite\u003e .database main: /home/yanboyang713/Documents/study/ISYS1055-DatabaseConcepts/chinook/boyang.db r/w To add an additional database in the current connection, you use the statement ATTACH DATABASE. The following statement adds the chinook database to the current connection. sqlite\u003e ATTACH DATABASE \"./chinook.db\" AS chinook; Now if you run the .database command again, the sqlite3 returns two databases: main and chinook. sqlite\u003e .database main: /home/yanboyang713/Documents/study/ISYS1055-DatabaseConcepts/chinook/boyang.db r/w chinook: /home/yanboyang713/Documents/study/ISYS1055-DatabaseConcepts/chinook/chinook.db r/w Exit sqlite3 tool To exit the sqlite3 program, you use the .exit command. sqlite\u003e .exit Show tables in a database To display all the tables in the current database, you use the .tables command. The following commands open a new database connection to the chinook database and display the tables in the database. [yanboyang713@boyang chinook]$ sqlite3 ./chinook.db SQLite version 3.35.1 2021-03-15 16:53:57 Enter \".help\" for usage hints. sqlite\u003e .table albums employees invoices playlists artists genres media_types tracks customers invoice_items playlist_track sqlite\u003e If you want to find tables based on a specific pattern, you use the .table pattern command. The sqlite3 uses the LIKE operator for pattern matching. For example, the following statement returns the table that ends with the string es. sqlite\u003e .table '%es' employees genres invoices media_types sqlite\u003e Show the structure of a table To display the structure of a table, you use the .schema TABLE command. The TABLE argument could be a pattern. If you omit it, the .schema command will show the structures of all the tables. The following command shows the structure of the albums table. sqlite\u003e .schema albums CREATE TABLE IF NOT EXISTS \"albums\" ( [AlbumId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, [Title] NVARCHAR(160) NOT NULL, [ArtistId] INTEGER NOT NULL, FOREIGN KEY ([ArtistId]) REFERENCES \"artists\" ([ArtistId]) ON DELETE NO ACTION ON UPDATE NO ACTION ); CREATE INDEX [IFK_AlbumArtistId] ON \"albums\" ([ArtistId]); sqlite\u003e To show the full schema and the content, you use the .fullschema command. sqlite\u003e.fullschema Show indexes To show all indexes of the current database, you use the .indexes command as follows: sqlite\u003e .indexes IFK_AlbumArtistId IFK_PlaylistTrackTrackId IFK_CustomerSu","date":"2021-03-01","objectID":"/sqlite/:2:4","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Basic SQLite tutorial This section presents basic SQL statements that you can use with SQLite. You will first start querying data from the sample database. If you are already familiar with SQL, you will notice the differences between SQL standard and SQL dialect used in SQLite. ","date":"2021-03-01","objectID":"/sqlite/:3:0","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 1. Simple query Select – query data from a single table using SELECT statement. The SELECT statement is one of the most commonly used statements in SQL. The SQLite SELECT statement provides all features of the SELECT statement in SQL standard. Simple uses of SELECT statement You can use the SELECT statement to perform a simple calculation as follows: sqlite\u003e SELECT 1 + 1; 1 + 1 ----- 2 You can use multiple expressions in the SELECT statement as follows: sqlite\u003e SELECT 10 / 5, 2 * 4 ; 10 / 5 2 * 4 ------ ----- 2 8 sqlite\u003e Querying data from a table using the SELECT statement We often use the SELECT statement to query data from one or more table. The syntax of the SELECT statement is as follows: SELECTDISTINCTcolumn_listFROMtable_listJOINtableONjoin_conditionWHERErow_filterORDERBYcolumnLIMITcountOFFSEToffsetGROUPBYcolumnHAVINGgroup_filter; The SELECT statement is the most complex statement in SQLite. To help easier to understand each part, we will break the SELECT statement into multiple section. In this section, we are going to focus on the simplest form of the SELECT statement that allows you to query data from a single table. SELECTcolumn_listFROMtable; Even though the SELECT clause appears before the FROM clause, SQLite evaluates the FROM clause first and then the SELECT clause, therefore: First, specify the table where you want to get data from in the FROM clause. Notice that you can have more than one table in the FROM clause. We will discuss it in the subsequent tutorial. Second, specify a column or a list of comma-separated columns in the SELECT clause. You use the semicolon (;) to terminate the statement. SQLite SELECT examples Let’s take a look at the tracks table in the sample database. The tracks table contains columns and rows. It looks like a spreadsheet. To get data from the tracks table such as trackid, track name, composer, and unit price, you use the following statement: SELECTtrackid,name,composer,unitpriceFROMtracks; You specify a list column names, which you want to get data, in the SELECT clause and the tracks table in the FROM clause. SQLite returns the following result: To get data from all columns, you specify the columns of the tracks table in the SELECT clause as follows: SELECTtrackid,name,albumid,mediatypeid,genreid,composer,milliseconds,bytes,unitpriceFROMtracks; For a table with many columns, the query would be so long that time-consuming to type. To avoid this, you can use the asterisk (*), which is the shorthand for all columns of the table as follows: SELECT*FROMtracks; NOTE: You should use the asterisk (*) for the testing purpose only, not in the real application development. When you develop an application, you should control what SQLite returns to your application. Suppose, a table has 3 columns, and you use the asterisk (*) to retrieve the data from all three columns. What if someone removes a column, your application would not be working properly, because it assumes that there are three columns returned and the logic to process those three columns would be broken. If someone adds more columns, your application may work but it gets more data than needed, which creates more I/O overhead between the database and application. So try to avoid using the asterisk (*) as a good habit when you use the SELECT statement. ","date":"2021-03-01","objectID":"/sqlite/:3:1","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 2. Sorting rows Order By – sort the result set in ascending or descending order. Introduction to SQLite ORDER BY clause SQLite stores data in the tables in an unspecified order. It means that the rows in the table may or may not be in the order that they were inserted. If you use the SELECT statement to query data from a table, the order of rows in the result set is unspecified. To sort the result set, you add the ORDER BY clause to the SELECT statement as follows: SELECTselect_listFROMtableORDERBYcolumn_1ASC,column_2DESC; The ORDER BY clause comes after the FROM clause. It allows you to sort the result set based on one or more columns in ascending or descending order. In this syntax, you place the column name by which you want to sort after the ORDER BY clause followed by the ASC or DESC keyword. The ASC keyword means ascending. And the DESC keyword means descending. NOTE: If you don’t specify the ASC or DESC keyword, SQLite sorts the result set using the ASC option. In other words, it sorts the result set in the ascending order by default. In case you want to sort the result set by multiple columns, you use a comma (,) to separate two columns. The ORDER BY clause sorts rows using columns or expressions from left to right. In other words, the ORDER BY clause sorts the rows using the first column in the list. Then, it sorts the sorted rows using the second column, and so on. You can sort the result set using a column that does not appear in the select list of the SELECT clause. SQLite ORDER BY clause example Let’s take the tracks table in the sample database for the demonstration. Suppose, you want to get data from name, milliseconds, and album id columns, you use the following statement: SELECTname,milliseconds,albumidFROMtracks; The SELECT statement that does not use ORDER BY clause returns a result set that is not in any order. Suppose you want to sort the result set based on AlbumId column in ascending order, you use the following statement: SELECTname,milliseconds,albumidFROMtracksORDERBYalbumidASC; The result set now is sorted by the AlbumId column in ascending order as shown in the screenshot. SQLite uses ASC by default so you can omit it in the above statement as follows: SELECTname,milliseconds,albumidFROMtracksORDERBYalbumid; Suppose you want to sort the sorted result (by AlbumId) above by the Milliseconds column in descending order. In this case, you need to add the Milliseconds column to the ORDER BY clause as follows: SELECTname,milliseconds,albumidFROMtracksORDERBYalbumidASC,millisecondsDESC; SQLite sorts rows by AlbumId column in ascending order first. Then, it sorts the sorted result set by the Milliseconds column in descending order. If you look at the tracks of the album with AlbumId 1, you find that the order of tracks changes between the two statements. SQLite ORDER BY with the column position Instead of specifying the names of columns, you can use the column’s position in the ORDER BY clause. For example, the following statement sorts the tracks by both albumid (3rd column) and milliseconds (2nd column) in ascending order. SELECTname,milliseconds,albumidFROMtracksORDERBY3,2; The number 3 and 2 refers to the AlbumId and Milliseconds in the column list that appears in the SELECT clause. Sorting NULLs In the database world, NULL is special. It denotes that the information missing or the data is not applicable. Suppose you want to store the birthday of an artist in a table. At the time of saving the artist’s record, you don’t have the birthday information. To represent the unknown birthday information in the database, you may use a special date like 01.01.1900 or an '' empty string. However, both of these values do not clearly show that the birthday is unknown. NULL was invented to resolve this issue. Instead of using a special value to indicate that the information is missing, NULL is used. NULL is special because you cannot compare it with another value. Simply put, if the two pieces of information a","date":"2021-03-01","objectID":"/sqlite/:3:2","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 3. Filtering data Select Distinct – query unique rows from a table using the DISTINCT clause. Introduction to SQLite SELECT DISTINCT clause The DISTINCT clause is an optional clause of the SELECT statement. The DISTINCT clause allows you to remove the duplicate rows in the result set. The following statement illustrates the syntax of the DISTINCT clause: SELECTDISTINCTselect_listFROMtable; In this syntax: First, the DISTINCT clause must appear immediately after the SELECT keyword. Second, you place a column or a list of columns after the DISTINCT keyword. If you use one column, SQLite uses values in that column to evaluate the duplicate. In case you use multiple columns, SQLite uses the combination of values in these columns to evaluate the duplicate. SQLite considers NULL values as duplicates. If you use theDISTINCT clause with a column that has NULL values, SQLite will keep one row of a NULL value. In database theory, if a column contains NULL values, it means that we do not have the information about that column of particular records or the information is not applicable. For example, if a customer has a phone number with a NULL value, it means we don’t have information about the phone number of the customer at the time of recording customer information or the customer may not have a phone number at all. SQLite SELECT DISTINCT examples We will use the customers table in the sample database for demonstration. Suppose you want to know the cities where the customers locate, you can use the SELECT statement to get data from the city column of the customers table as follows: sqlite\u003e SELECT city FROM customers ORDER BY city; City ------------------- Amsterdam Bangalore Berlin Berlin Bordeaux Boston Brasília Brussels Budapest Buenos Aires Chicago Copenhagen Cupertino Delhi ... It returns 59 rows. There are few duplicate rows such as Berlin London, and Mountain View To remove these duplicate rows, you use the DISTINCT clause as follows: SELECTDISTINCTcityFROMcustomersORDERBYcity; It returns 53 rows because the DISTINCT clause has removed 6 duplicate rows. SQLite SELECT DISTINCT on multiple columns The following statement finds cities and countries of all customers. sqlite\u003e SELECT city, country FROM customers ORDER BY country; City Country ------------------- -------------- Buenos Aires Argentina Sidney Australia Vienne Austria Brussels Belgium São José dos Campos Brazil São Paulo Brazil São Paulo Brazil Rio de Janeiro Brazil ... The result set contains duplicate city and country e.g., Sao Paulo in Brazil as shown in the screenshot above. To remove duplicate the city and country, you apply the DISTINCT clause to both city and country columns as shown in the following query: sqlite\u003e SELECT DISTINCT city, country FROM customers ORDER BY country; City Country ------------------- -------------- Buenos Aires Argentina Sidney Australia Vienne Austria Brussels Belgium São José dos Campos Brazil São Paulo Brazil Rio de Janeiro Brazil Brasília Brazil Montréal Canada ... As mentioned earlier, SQLite uses the combination of city and country to evaluate the duplicate. SQLite SELECT DISTINCT with NULL example This statement returns the names of companies of customers from the customers table. sqlite\u003e SELECT company FROM customers; Company ------------------------------------------------ Embraer - Empresa Brasileira de Aeronáutica S.A. JetBrains s.r.o. Woodstock Discos Banco do Brasil S.A. Riotur Telus Rogers Canada Google Inc. Microsoft Corporation Apple Inc. ... It returns 59 rows with many NULL values. Now, if you apply the DISTINCT clause to the statement, it will keep only one row with a NULL value. See the following statement: sqlite\u003e SELECT DISTINCT company FROM customers; Company ------------------------------------------------ Embraer - Empresa Brasileira de Aeronáutica S.A. JetBrains s.r.o. Woodstock Discos Banco do Brasil S.A. Riotur Telus Rogers Canada Google Inc. Microsoft Corporation Apple Inc. sqlite\u003e The statement returns 1","date":"2021-03-01","objectID":"/sqlite/:3:3","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 4. Joining tables Inner Join – query data from multiple tables using the inner join clause Introduction to SQLite inner join clause In relational databases, data is often distributed in many related tables. A table is associated with another table using foreign keys. To query data from multiple tables, you use INNER JOIN clause. The INNER JOIN clause combines columns from correlated tables. Suppose you have two tables: A and B. A has a1, a2, and f columns. B has b1, b2, and f column. The A table links to the B table using a foreign key column named f. The following illustrates the syntax of the inner join clause: SELECTa1,a2,b1,b2FROMAINNERJOINBonB.f=A.f; For each row in the A table, the INNER JOIN clause compares the value of the f column with the value of the f column in the B table. If the value of the f column in the A table equals the value of the f column in the B table, it combines data from a1, a2, b1, b2, columns and includes this row in the result set. In other words, the INNER JOIN clause returns rows from the A table that has the corresponding row in B table. This logic is applied if you join more than 2 tables. See the following example. Only the rows in the A table: (a1,1), (a3,3) have the corresponding rows in the B table (b1,1), (b2,3) are included in the result set. The following diagram illustrates the INNER JOIN clause: SQLite INNER JOIN examples Let’s take a look at the tracks and albums tables in the sample database. The tracks table links to the albums table via AlbumId column. In the tracks table, the AlbumId column is a foreign key. And in the albums table, the AlbumId is the primary key. To query data from both tracks and albums tables, you use the following statement: SELECTtrackid,name,titleFROMtracksINNERJOINalbumsONalbums.albumid=tracks.albumid; For each row in the tracks table, SQLite uses the value in the albumid column of the tracks table to compare with the value in the albumid of the albums table. If SQLite finds a match, it combines data of rows in both tables in the result set. You can include the AlbumId columns from both tables in the final result set to see the effect. SELECTtrackid,name,tracks.albumidASalbum_id_tracks,albums.albumidASalbum_id_albums,titleFROMtracksINNERJOINalbumsONalbums.albumid=tracks.albumid; SQLite inner join – 3 tables example See the following tables:tracks albums and artists One track belongs to one album and one album have many tracks. The tracks table associated with the albums table via albumid column. One album belongs to one artist and one artist has one or many albums. The albums table links to the artists table via artistid column. To query data from these tables, you need to use two inner join clauses in the SELECT statement as follows: SELECTtrackid,tracks.nameAStrack,albums.titleASalbum,artists.nameASartistFROMtracksINNERJOINalbumsONalbums.albumid=tracks.albumidINNERJOINartistsONartists.artistid=albums.artistid; You can use a WHERE clause to get the tracks and albums of the artist with id 10 as the following statement: SELECTtrackid,tracks.nameASTrack,albums.titleASAlbum,artists.nameASArtistFROMtracksINNERJOINalbumsONalbums.albumid=tracks.albumidINNERJOINartistsONartists.artistid=albums.artistidWHEREartists.artistid=10; Left Join – combine data from multiple tables using the left join clause Introduction to SQLite LEFT JOIN clause Similar to the INNER JOIN clause, the LEFT JOIN clause is an optional clause of the SELECT statement. You use the LEFT JOIN clause to query data from multiple related tables. Suppose we have two tables: A and B. A has m and f columns. B has n and f columns. To perform join between A and B using LEFT JOIN clause, you use the following statement: SELECTa,bFROMALEFTJOINBONA.f=B.fWHEREsearch_condition; The expression A.f = B.f is a conditional expression. Besides the equality (=) operator, you can use other comparison operators such as greater than (\u003e), less than (\u003c), etc. The statement returns a result set that includes: Ro","date":"2021-03-01","objectID":"/sqlite/:3:4","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 5. Grouping data Group By – combine a set of rows into groups based on specified criteria. The GROUP BY clause helps you summarize data for reporting purposes. Introduction to SQLite GROUP BY clause The GROUP BY clause is an optional clause of the SELECT statement. The GROUP BY clause a selected group of rows into summary rows by values of one or more columns. The GROUP BY clause returns one row for each group. For each group, you can apply an aggregate function such as MIN, MAX, SUM, COUNT, or AVG to provide more information about each group. The following statement illustrates the syntax of the SQLite GROUP BY clause. SELECTcolumn_1,aggregate_function(column_2)FROMtableGROUPBYcolumn_1,column_2; The GROUP BY clause comes after the FROM clause of the SELECT statement. In case a statement contains a WHERE clause, the GROUP BY clause must come after the WHERE clause. Following the GROUP BY clause is a column or a list of comma-separated columns used to specify the group. SQLite GROUP BY examples We use the tracks table from the sample database for the demonstration. SQLite GROUP BY clause with COUNT function The following statement returns the album id and the number of tracks per album. It uses the GROUP BY clause to groups tracks by album and applies the COUNT() function to each group. SELECTalbumid,COUNT(trackid)FROMtracksGROUPBYalbumid; You can use the ORDER BY clause to sort the groups as follows: SELECTalbumid,COUNT(trackid)FROMtracksGROUPBYalbumidORDERBYCOUNT(trackid)DESC; SQLite GROUP BY and INNER JOIN clause You can query data from multiple tables using the INNER JOIN clause, then use the GROUP BY clause to group rows into a set of summary rows. For example, the following statement joins the tracks table with the albums table to get the album’s titles and uses the GROUP BY clause with the COUNT function to get the number of tracks per album. SELECTtracks.albumid,title,COUNT(trackid)FROMtracksINNERJOINalbumsONalbums.albumid=tracks.albumidGROUPBYtracks.albumid; SQLite GROUP BY with HAVING clause To filter groups, you use the GROUP BY with HAVING clause. For example, to get the albums that have more than 15 tracks, you use the following statement: SELECTtracks.albumid,title,COUNT(trackid)FROMtracksINNERJOINalbumsONalbums.albumid=tracks.albumidGROUPBYtracks.albumidHAVINGCOUNT(trackid)\u003e15; SQLite GROUP BY clause with SUM function example You can use the SUM function to calculate total per group. For example, to get total length and bytes for each album, you use the SUM function to calculate total milliseconds and bytes. SELECTalbumid,SUM(milliseconds)length,SUM(bytes)sizeFROMtracksGROUPBYalbumid; SQLite GROUP BY with MAX, MIN, and AVG functions The following statement returns the album id, album title, maximum length, minimum length, and the average length of tracks in the tracks table. SELECTtracks.albumid,title,min(milliseconds),max(milliseconds),round(avg(milliseconds),2)FROMtracksINNERJOINalbumsONalbums.albumid=tracks.albumidGROUPBYtracks.albumid; SQLite GROUP BY multiple columns example In the previous example, we have used one column in the GROUP BY clause. SQLite allows you to group rows by multiple columns. For example, to group tracks by media type and genre, you use the following statement: SELECTMediaTypeId,GenreId,COUNT(TrackId)FROMtracksGROUPBYMediaTypeId,GenreId; SQLite uses the combination of values of MediaTypeId and GenreId columns as a group e.g., (1,1) and (1,2). It then applies the COUNT function to return the number of tracks in each group. SQLite GROUP BY date example See the following invoices table from the sample database: The following statement returns the number of invoice by years. SELECTSTRFTIME('%Y',InvoiceDate)InvoiceYear,COUNT(InvoiceId)InvoiceCountFROMinvoicesGROUPBYSTRFTIME('%Y',InvoiceDate)ORDERBYInvoiceYear; Here is the output: In this example: The function STRFTIME('%Y', InvoiceDate) returns a year from a date string. The GROUP BY clause groups the invoices by years. The function","date":"2021-03-01","objectID":"/sqlite/:3:5","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 6. Set operators Union – combine result sets of multiple queries into a single result set. We also discuss the differences between UNION and UNION ALL clauses. Introduction to SQLite UNION operator Sometimes, you need to combine data from multiple tables into a complete result set. It may be for tables with similar data within the same database or maybe you need to combine similar data from multiple databases. To combine rows from two or more queries into a single result set, you use SQLite UNION operator. The following illustrates the basic syntax of the UNION operator: query_1UNION[ALL]query_2UNION[ALL]query_3...; Both UNION and UNION ALL operators combine rows from result sets into a single result set. The UNION operator removes eliminate duplicate rows, whereas the UNION ALL operator does not. Because the UNION ALL operator does not remove duplicate rows, it runs faster than the UNION operator. The following are rules to union data: The number of columns in all queries must be the same. The corresponding columns must have compatible data types. The column names of the first query determine the column names of the combined result set. The GROUP BY and HAVING clauses are applied to each individual query, not the final result set. The ORDER BY clause is applied to the combined result set, not within the individual result set. Note that the difference between UNION and JOIN e.g., INNER JOIN or LEFT JOIN is that the JOIN clause combines columns from multiple related tables, while UNION combines rows from multiple similar tables. Suppose we have two tables t1 and t2 with the following structures: CREATETABLEt1(v1INT);INSERTINTOt1(v1)VALUES(1),(2),(3);CREATETABLEt2(v2INT);INSERTINTOt2(v2)VALUES(2),(3),(4); The following statement combines the result sets of the t1 and t2 table using the UNION operator: SELECTv1FROMt1UNIONSELECTv2FROMt2; Here is the output: The following picture illustrates the UNION operation of t1 and t2 tables: The following statement combines the result sets of t1 and t2 table using the UNION ALL operator: SELECTv1FROMt1UNIONALLSELECTv2FROMt2; The following picture shows the output: The following picture illustrates the UNION ALL operation of the result sets of t1 and t2 tables: SQLite UNION examples Let’s take some examples of using the UNION operator. SQLite UNION example This statement uses the UNION operator to combine names of employees and customers into a single list: SELECTFirstName,LastName,'Employee'ASTypeFROMemployeesUNIONSELECTFirstName,LastName,'Customer'FROMcustomers; Here is the output: SQLite UNION with ORDER BY example This example uses the UNION operator to combine the names of the employees and customers into a single list. In addition, it uses the ORDER BY clause to sort the name list by first name and last name. SELECTFirstName,LastName,'Employee'ASTypeFROMemployeesUNIONSELECTFirstName,LastName,'Customer'FROMcustomersORDERBYFirstName,LastName; Here is the output: Except – compare the result sets of two queries and returns distinct rows from the left query that are not output by the right query. Introduction to SQLite EXCEPT operator SQLite EXCEPT operator compares the result sets of two queries and returns distinct rows from the left query that are not output by the right query. The following shows the syntax of the EXCEPT operator: SELECTselect_list1FROMtable1EXCEPTSELECTselect_list2FROMtable2 This query must conform to the following rules: First, the number of columns in the select lists of both queries must be the same. Second, the order of the columns and their types must be comparable. The following statements create two tables t1 and t2 and insert some data into both tables: CREATETABLEt1(v1INT);INSERTINTOt1(v1)VALUES(1),(2),(3);CREATETABLEt2(v2INT);INSERTINTOt2(v2)VALUES(2),(3),(4); The following statement illustrates how to use the EXCEPT operator to compare result sets of two queries: SELECTv1FROMt1EXCEPTSELECTv2FROMt2; The output is 1. The following picture illustrates th","date":"2021-03-01","objectID":"/sqlite/:3:6","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 7. Subquery Subquery – introduce you to the SQLite subquery and correlated subquery. Introduction to SQLite subquery A subquery is a SELECT statement nested in another statement. See the following statement. SELECTcolumn_1FROMtable_1WHEREcolumn_1=(SELECTcolumn_1FROMtable_2); The following query is the outer query: SELECTcolumn_1FROMtable_1WHEREcolum_1= And the following query is the subquery. (SELECTcolumn_1FROMtable_2) You must use a pair of parentheses to enclose a subquery. Note that you can nest a subquery inside another subquery with a certain depth. Typically, a subquery returns a single row as an atomic value, though it may return multiple rows for comparing values with the IN operator. You can use a subquery in the SELECT, FROM, WHERE, and JOIN clauses. SQLite subquery examples We will use the tracks and albums tables from the sample database for the demonstration. SQLite subquery in the WHERE clause example You can use a simple subquery as a search condition. For example, the following statement returns all the tracks in the album with the title Let There Be Rock SELECTtrackid,name,albumidFROMtracksWHEREalbumid=(SELECTalbumidFROMalbumsWHEREtitle='Let There Be Rock'); The subquery returns the id of the album with the title ‘Let There Be Rock’. The query uses the equal operator (=) to compare albumid returned by the subquery with the albumid in the tracks table. If the subquery returns multiple values, you can use the IN operator to check for the existence of a single value against a set of value. See the following employees and customers table in the sample database: For example, the following query returns the customers whose sales representatives are in Canada. SELECTcustomerid,firstname,lastnameFROMcustomersWHEREsupportrepidIN(SELECTemployeeidFROMemployeesWHEREcountry='Canada'); The subquery returns a list of ids of the employees who locate in Canada. The outer query uses the IN operator to find the customers who have the sales representative id in the list. SQLite subquery in the FROM clause example Sometimes you want to apply aggregate functions to a column multiple times. For example, first, you want to sum the size of an album and then calculate the average size of all albums. You may come up with the following query. SELECTAVG(SUM(bytes)FROMtracksGROUPBYalbumid; This query is not valid. To fix it, you can use a subquery in the FROM clause as follows: SELECTAVG(album.size)FROM(SELECTSUM(bytes)SIZEFROMtracksGROUPBYalbumid)ASalbum; ","date":"2021-03-01","objectID":"/sqlite/:3:7","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"AVG(album.size) 338288920.317 In this case, SQLite first executes the subquery in the FROM clause and returns a result set. Then, SQLite uses this result set as a derived table in the outer query. SQLite correlated subquery All the subqueries you have seen so far can be executed independently. In other words, it does not depend on the outer query. The correlated subquery is a subquery that uses the values from the outer query. Unlike an ordinal subquery, a correlated subquery cannot be executed independently. The correlated subquery is not efficient because it is evaluated for each row processed by the outer query. The following query uses a correlated subquery to return the albums whose size is less than 10MB. SELECTalbumid,titleFROMalbumsWHERE10000000\u003e(SELECTsum(bytes)FROMtracksWHEREtracks.AlbumId=albums.AlbumId)ORDERBYtitle; How the query works. For each row processed in the outer query, the correlated subquery calculates the size of the albums from the tracks that belong the current album using the SUM function. The predicate in the WHERE clause filters the albums that have the size greater than or equal 10MB (10000000 bytes). SQLite correlated subquery in the SELECT clause example The following query uses a correlated subquery in the SELECT clause to return the number of tracks in an album. SELECTalbumid,title,(SELECTcount(trackid)FROMtracksWHEREtracks.AlbumId=albums.AlbumId)tracks_countFROMalbumsORDERBYtracks_countDESC; Exists operator – test for the existence of rows returned by a subquery. Introduction to SQLite EXISTS operator The EXISTS operator is a logical operator that checks whether a subquery returns any row. Here is the basic syntax of the EXISTS operator: EXISTS(subquery) In this syntax, the subquery is a SELECT statement that returns zero or more rows. If the subquery returns one or more row, the EXISTS operator return true. Otherwise, the EXISTS operator returns false or NULL. Note that if the subquery returns one row with NULL, the result of the EXISTS operator is still true because the result set contains one row with NULL. To negate the EXISTS operator, you use the NOT EXISTS operator as follows: NOTEXISTS(subquery) The NOT EXISTS operator returns true if the subquery returns no row. SQLite EXISTS operator example See the following Customers and Invoices tables from the sample database: The following statement finds customers who have invoices: SELECTCustomerId,FirstName,LastName,CompanyFROMCustomerscWHEREEXISTS(SELECT1FROMInvoicesWHERECustomerId=c.CustomerId)ORDERBYFirstName,LastName; The following picture shows the partial result set: In this example, for each customer, the EXISTS operator checks if the customer id exists in the invoices table. If yes, the subquery returns one row with value 1 that causes the EXISTS operator evaluate to true. Therefore, the query includes the curstomer in the result set. In case the customer id does not exist in the Invoices table, the subquery returns no rows which causes the EXISTS operator to evaluate to false, hence the query does not include the customer in the result set. Notice that you can use the IN operator instead of EXISTS operator in this case to achieve the same result: SELECTCustomerId,FirstName,LastName,CompanyFROMCustomerscWHERECustomerIdIN(SELECTCustomerIdFROMInvoices)ORDERBYFirstName,LastName; Once the subquery returns the first row, the EXISTS operator stops searching because it can determine the result. On the other hand, the IN operator must scan all rows returned by the subquery to determine the result. Generally speaking, the EXISTS operator is faster than IN operator if the result set returned by the subquery is large. By contrast, the IN operator is faster than the EXISTS operator if the result set returned by the subquery is small. SQLite NOT EXISTS operator example See the following Artists and Albums table from the sample database: This query find all artists who do not have any album in the Albums table: SELECT*FROMArtistsaWHERENOTEXISTS(SE","date":"2021-03-01","objectID":"/sqlite/:4:0","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 8. More querying techniques Case – add conditional logic to the query The SQLite CASE expression evaluates a list of conditions and returns an expression based on the result of the evaluation. The CASE expression is similar to the IF-THEN-ELSE statement in other programming languages. You can use the CASE expression in any clause or statement that accepts a valid expression. For example, you can use the CASE expression in clauses such as WHERE, ORDER BY, HAVING, SELECT and statements such as SELECT, UPDATE, and DELETE. SQLite provides two forms of the CASE expression: simple CASE and searched CASE. SQLite simple CASE expression The simple CASE expression compares an expression to a list of expressions to return the result. The following illustrates the syntax of the simple CASE expression. CASEcase_expressionWHENwhen_expression_1THENresult_1WHENwhen_expression_2THENresult_2...[ELSEresult_else]END The simple CASE expression compares the case_expression to the expression appears in the first WHEN clause, when_expression_1, for equality. If the case_expression equals when_expression_1, the simple CASE returns the expression in the corresponding THEN clause, which is the result_1. Otherwise, the simple CASE expression compares the case_expression with the expression in the next WHEN clause. In case no case_expression matches the when_expression, the CASE expression returns the result_else in the ELSE clause. If you omit the ELSE clause, the CASE expression returns NULL. The simple CASE expression uses short-circuit evaluation. In other words, it returns the result and stop evaluating other conditions as soon as it finds a match. Simple CASE example Let’s take a look at the customers table in the sample database. Suppose, you have to make a report of the customer groups with the logic that if a customer locates in the USA, this customer belongs to the domestic group, otherwise the customer belongs to the foreign group. To make this report, you use the simple CASE expression in the SELECT statement as follows: SELECTcustomerid,firstname,lastname,CASEcountryWHEN'USA'THEN'Domestic'ELSE'Foreign'ENDCustomerGroupFROMcustomersORDERBYLastName,FirstName; SQLite searched CASE expression The searched CASE expression evaluates a list of expressions to decide the result. Note that the simple CASE expression only compares for equality, while the searched CASE expression can use any forms of comparison. The following illustrates the syntax of the searched CASE expression. CASEWHENbool_expression_1THENresult_1WHENbool_expression_2THENresult_2[ELSEresult_else]END The searched CASE expression evaluates the Boolean expressions in the sequence specified and return the corresponding result if the expression evaluates to true. In case no expression evaluates to true, the searched CASE expression returns the expression in the ELSE clause if specified. If you omit the ELSE clause, the searched CASE expression returns NULL. Similar to the simple CASE expression, the searched CASE expression stops the evaluation when a condition is met. Searched CASE example We will use the tracks table for the demonstration. Suppose you want to classify the tracks based on its length such as less a minute, the track is short; between 1 and 5 minutes, the track is medium; greater than 5 minutes, the track is long. To achieve this, you use the searched CASE expression as follows: SELECTtrackid,name,CASEWHENmilliseconds\u003c60000THEN'short'WHENmilliseconds\u003e60000ANDmilliseconds\u003c300000THEN'medium'ELSE'long'ENDcategoryFROMtracks; ","date":"2021-03-01","objectID":"/sqlite/:4:1","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 9. Changing data This section guides you on how to update data in the table using insert, update, delete, and replace statements. Insert – insert rows into a table To insert data into a table, you use the INSERT statement. SQLite provides various forms of the INSERT statements that allow you to insert a single row, multiple rows, and default values into a table. In addition, you can insert a row into a table using data provided by a SELECT statement. SQLite INSERT – inserting a single row into a table To insert a single row into a table, you use the following form of the INSERT statement: INSERTINTOtable(column1,column2,..)VALUES(value1,value2,...); Let’s examine the INSERT statement in more detail: First, specify the name of the table to which you want to insert data after the INSERT INTO keywords. Second, add a comma-separated list of columns after the table name. The column list is optional. However, it is a good practice to include the column list after the table name. Third, add a comma-separated list of values after the VALUES keyword. If you omit the column list, you have to specify values for all columns in the value list. The number of values in the value list must be the same as the number of columns in the column list. We will use the artists table in the sample database for the demonstration. The following statement insert a new row into the artists table: INSERTINTOartists(name)VALUES('Bud Powell'); Because the ArtistId column is an auto-increment column, you can ignore it in the statement. SQLite automatically geneate a sequential integer number to insert into the ArtistId column. You can verify the insert operation by using the following SELECT statement: SELECTArtistId,NameFROMArtistsORDERBYArtistIdDESCLIMIT1; As you see, we have a new row in the artists table. SQLite INSERT – Inserting multiple rows into a table To insert multiple rows into a table, you use the following form of the INSERT statement: INSERTINTOtable1(column1,column2,..)VALUES(value1,value2,...),(value1,value2,...),...(value1,value2,...); Each value list following the VALUES clause is a row that will be inserted into the table. The following example inserts three rows into the artists table: INSERTINTOartists(name)VALUES(\"Buddy Rich\"),(\"Candido\"),(\"Charlie Byrd\"); SQLite issued a message: RowAffected:3 You can verify the result using the following statement: SELECTArtistId,NameFROMartistsORDERBYArtistIdDESCLIMIT3; SQLite INSERT – Inserting default values When you create a new table using the CREATE TABLE statement, you can specify default values for columns, or a NULL if a default value is not specified. The third form of the INSERT statement is INSERT DEFAULT VALUES, which inserts a new row into a table using the default values specified in the column definition or NULL if the default value is not available and the column does not have a NOT NULL constraint. For example, the following statement inserts a new row into the artists table using INSERT DEFAULT VALUES: INSERTINTOartistsDEFAULTVALUES; To verify the insert, you use the following statement: SELECTArtistId,NameFROMartistsORDERBYArtistIdDESC; The default value of the ArtistId column is the next sequential integer . However, the name column does not have any default value, therefore, the INSERT DEFAULT VALUES statement inserts NULL into it. SQLite INSERT – Inserting new rows with data provided by a SELECT statement Suppose you want to backup the artists table, you can follow these steps: First, create a new table named artists_backup as follows: CREATETABLEartists_backup(ArtistIdINTEGERPRIMARYKEYAUTOINCREMENT,NameNVARCHAR); To insert data into the artists_backup table with the data from the artists table, you use the INSERT INTO SELECT statement as follows: INSERTINTOartists_backupSELECTArtistId,NameFROMartists; If you query data from the artists_backup table, you will see all data in the artists table. SELECT*FROMartists_backup; Update – update existing rows in a table. Introduct","date":"2021-03-01","objectID":"/sqlite/:4:2","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 10. Transactions Transaction – show you how to handle transactions in SQLite. SQLite \u0026 ACID SQLite is a transactional database that all changes and queries are atomic, consistent, isolated, and durable (ACID). SQLite guarantees all the transactions are ACID compliant even if the transaction is interrupted by a program crash, operation system dump, or power failure to the computer. Atomic: a transaction should be atomic. It means that a change cannot be broken down into smaller ones. When you commit a transaction, either the entire transaction is applied or not. Consistent: a transaction must ensure to change the database from one valid state to another. When a transaction starts and executes a statement to modify data, the database becomes inconsistent. However, when the transaction is committed or rolled back, it is important that the transaction must keep the database consistent. Isolation: a pending transaction performed by a session must be isolated from other sessions. When a session starts a transaction and executes the INSERT or UPDATE statement to change the data, these changes are only visible to the current session, not others. On the other hand, the changes committed by other sessions after the transaction started should not be visible to the current session. Durable: if a transaction is successfully committed, the changes must be permanent in the database regardless of the condition such as power failure or program crash. On the contrary, if the program crashes before the transaction is committed, the change should not persist. SQLite transaction statements By default, SQLite operates in auto-commit mode. It means that for each command, SQLite starts, processes, and commits the transaction automatically. To start a transaction explicitly, you use the following steps: First, open a transaction by issuing the BEGIN TRANSACTION command. BEGINTRANSACTION; After executing the statement BEGIN TRANSACTION, the transaction is open until it is explicitly committed or rolled back. Second, issue SQL statements to select or update data in the database. Note that the change is only visible to the current session (or client). Third, commit the changes to the database by using the COMMIT or COMMIT TRANSACTION statement. COMMIT; If you do not want to save the changes, you can roll back using the ROLLBACK or ROLLBACK TRANSACTION statement: ROLLBACK; SQLite transaction example We will create two new tables: accounts and account_changes for the demonstration. The accounts table stores data about the account numbers and their balances. The account_changes table stores the changes of the accounts. First, create the accounts and account_changes tables by using the following CREATE TABLE statements: CREATETABLEaccounts(account_noINTEGERNOTNULL,balanceDECIMALNOTNULLDEFAULT0,PRIMARYKEY(account_no),CHECK(balance\u003e=0));CREATETABLEaccount_changes(change_noINTNOTNULLPRIMARYKEY,account_noINTEGERNOTNULL,flagTEXTNOTNULL,amountDECIMALNOTNULL,changed_atTEXTNOTNULL); Second, insert some sample data into the accounts table. INSERTINTOaccounts(account_no,balance)VALUES(100,20100);INSERTINTOaccounts(account_no,balance)VALUES(200,10100); Third, query data from the accounts table: SELECT*FROMaccounts; Fourth, transfer 1000 from account 100 to 200, and log the changes to the table account_changes in a single transaction. BEGINTRANSACTION;UPDATEaccountsSETbalance=balance-1000WHEREaccount_no=100;UPDATEaccountsSETbalance=balance+1000WHEREaccount_no=200;INSERTINTOaccount_changes(account_no,flag,amount,changed_at)VALUES(100,'-',1000,datetime('now'));INSERTINTOaccount_changes(account_no,flag,amount,changed_at)VALUES(200,'+',1000,datetime('now'));COMMIT; Fifth, query data from the accounts table: SELECT*FROMaccounts; As you can see, balances have been updated successfully. Sixth, query the contents of the account_changes table: SELECT*FROMaccount_changes; Let’s take another example of rolling back a transaction. First, attempt to deduct 20,000 from accoun","date":"2021-03-01","objectID":"/sqlite/:4:3","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 11. Data definition In this section, you’ll learn how to create database objects such as tables, views, indexes using SQL data definition language. SQLite Data Types – introduce you to the SQLite dynamic type system and its important concepts: storage classes, manifest typing, and type affinity. Introduction to SQLite data types If you come from other database systems such as MySQL and PostgreSQL, you notice that they use static typing. It means when you declare a column with a specific data type, that column can store only data of the declared data type. Different from other database systems, SQLite uses dynamic type system. In other words, a value stored in a column determines its data type, not the column’s data type. In addition, you don’t have to declare a specific data type for a column when you create a table. In case you declare a column with the integer data type, you can store any kind of data types such as text and BLOB, SQLite will not complain about this. SQLite provides five primitive data types which are referred to as storage classes. Storage classes describe the formats that SQLite uses to store data on disk. A storage class is more general than a data type e.g., INTEGER storage class includes 6 different types of integers. In most cases, you can use storage classes and data types interchangeably. The following table illustrates 5 storage classes in SQLite: Storage Class Meaning NULL NULL values mean missing information or unknown. INTEGER Integer values are whole numbers (either positive or negative). An integer can have variable sizes such as 1, 2,3, 4, or 8 bytes. REAL Real values are real numbers with decimal values that use 8-byte floats. TEXT TEXT is used to store character data. The maximum length of TEXT is unlimited. SQLite supports various character encodings. BLOB BLOB stands for a binary large object that can store any kind of data. The maximum size of BLOB is, theoretically, unlimited. SQLite determines the data type of a value based on its data type according to the following rules: If a literal has no enclosing quotes and decimal point or exponent, SQLite assigns the INTEGER storage class. If a literal is enclosed by single or double quotes, SQLite assigns the TEXT storage class. If a literal does not have quote nor decimal point nor exponent, SQLite assigns REAL storage class. If a literal is NULL without quotes, it assigned NULL storage class. If a literal has the X’ABCD’ or x ‘abcd’, SQLite assigned BLOB storage class. SQLite does not support built-in date and time storage classes. However, you can use the TEXT, INT, or REAL to store date and time values. For the detailed information on how to handle date and time values, check it out the SQLite date and time tutorial. SQLites provides the typeof() function that allows you to check the storage class of a value based on its format. See the following example: sqlite\u003e SELECT typeof(100), typeof(10.0), typeof('100'), typeof(x'1000'), typeof(NULL); typeof(100) typeof(10.0) typeof('100') typeof(x'1000') typeof(NULL) ----------- ------------ ------------- --------------- ------------ integer real text blob null sqlite\u003e A single column in SQLite can store mixed data types. See the following example. First, create a new table named test_datatypes for testing. CREATETABLEtest_datatypes(idINTEGERPRIMARYKEY,val); Second, insert data into the test_datatypes table. INSERTINTOtest_datatypes(val)VALUES(1),(2),(10.1),(20.5),('A'),('B'),(NULL),(x'0010'),(x'0011'); Third, use the typeof() function to get the data type of each value stored in the val column. sqlite\u003e SELECT id, val, typeof(val) FROM test_datatypes; id val typeof(val) -- ---- ----------- 1 1 integer 2 2 integer 3 10.1 real 4 20.5 real 5 A text 6 B text 7 null 8 blob 9 blob You may ask how SQLite sorts data in a column with different storage classes like val column above. To resolve this, SQLite provides the following set of rules when it comes to sorting: NULL storage class has the lowest ","date":"2021-03-01","objectID":"/sqlite/:4:4","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 12. Constraints Primary Key – show you how to define the primary key for a table. Introduction to SQLite primary key A primary key is a column or group of columns used to identify the uniqueness of rows in a table. Each table has one and only one primary key. SQLite allows you to define primary key in two ways: First, if the primary key has only one column, you use the PRIMARY KEY column constraint to define the primary key as follows: CREATETABLEtable_name(column_1INTEGERNOTNULLPRIMARYKEY,...); Second, in case primary key consists of two or more columns, you use the PRIMARY KEY table constraint to define the primary as shown in the following statement. CREATETABLEtable_name(column_1INTEGERNOTNULL,column_2INTEGERNOTNULL,...PRIMARYKEY(column_1,column_2,...)); In SQL standard, the primary key column must not contain NULL values. It means that the primary key column has an implicit NOT NULL constraint. However, to make the current version of SQLite compatible with the earlier version, SQLite allows the primary key column to contain NULL values. SQLite primary key and rowid table When you create a table without specifying the WITHOUT ROWID option, SQLite adds an implicit column called rowid that stores 64-bit signed integer. The rowid column is a key that uniquely identifies the rows in the table. Tables that have rowid columns are called rowid tables. If a table has the primary key that consists of one column, and that column is defined as INTEGER then this primary key column becomes an alias for the rowid column. Notice that if you assign another integer type such as BIGINT and UNSIGNED INT to the primary key column, this column will not be an alias for the rowid column. Because the rowid table organizes its data as a B-tree, querying and sorting data of a rowid table are very fast. It is faster than using a primary key which is not an alias of the rowid. Another important note is that if you declare a column with the INTEGER type and PRIMARY KEY DESC clause, this column will not become an alias for the rowid column: CREATETABLEtable(pkINTEGERPRIMARYKEYDESC,...); Creating SQLite primary key examples The following statement creates a table named countries which has country_id column as the primary key. CREATETABLEcountries(country_idINTEGERPRIMARYKEY,nameTEXTNOTNULL); Because the primary key of the countries table has only one column, we defined the primary key using PRIMARY KEY column constraint. It is possible to use the PRIMARY KEY table constraint to define the primary key that consists of one column as shown in the following statement: CREATETABLElanguages(language_idINTEGER,nameTEXTNOTNULL,PRIMARYKEY(language_id)); However, for tables that the primary keys consist of more than one column, you must use PRIMARY KEY table constraint to define primary keys. The following statement creates the country_languages table whose primary key consists of two columns. CREATETABLEcountry_languages(country_idINTEGERNOTNULL,language_idINTEGERNOTNULL,PRIMARYKEY(country_id,language_id),FOREIGNKEY(country_id)REFERENCEScountries(country_id)ONDELETECASCADEONUPDATENOACTION,FOREIGNKEY(language_id)REFERENCESlanguages(language_id)ONDELETECASCADEONUPDATENOACTION); Adding SQLite primary key example Unlike other database systems e.g., MySQL and PostgreSQL, you cannot use the ALTER TABLE statement to add a primary key to an existing table. You need to follow these steps to work around the limitation: First, set the foreign key constarint check off. Next, rename the table to another table name (old_table) Then, create a new table (table) with exact structure of the table that you have been renamed. After that, copy data from the old_table to the table. Finally, turn on the foreign key constraint check on See the following statements: PRAGMAforeign_keys=off;BEGINTRANSACTION;ALTERTABLEtableRENAMETOold_table;-- define the primary key constraint here CREATETABLEtable(...);INSERTINTOtableSELECT*FROMold_table;COMMIT;PRAGMAforeign_keys=on; The BEGIN TRA","date":"2021-03-01","objectID":"/sqlite/:4:5","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 13. Views Create View – introduce you to the view concept and show you how to create a new view in the database. What is a view In database theory, a view is a result set of a stored query. A view is the way to pack a query into a named object stored in the database. You can access the data of the underlying tables through a view. The tables that the query in the view definition refers to are called base tables. A view is useful in some cases: First, views provide an abstraction layer over tables. You can add and remove the columns in the view without touching the schema of the underlying tables. Second, you can use views to encapsulate complex queries with joins to simplify the data access. SQLite view is read only. It means you cannot use INSERT, DELETE, and UPDATE statements to update data in the base tables through the view. SQLite CREATE VIEW statement To create a view, you use the CREATE VIEW statement as follows: CREATE[TEMP]VIEW[IFNOTEXISTS]view_name[(column-name-list)]ASselect-statement; First, specify a name for the view. The IF NOT EXISTS option only creates a new view if it doesn’t exist. If the view already exists, it does nothing. Second, use the the TEMP or TEMPORARY option if you want the view to be only visible in the current database connection. The view is called a temporary view and SQLite automatically removes the temporary view whenever the database connection is closed. Third, specify a SELECT statement for the view. By default, the columns of the view derive from the result set of the SELECT statement. However, you can assign the names of the view columns that are different from the column name of the table SQLite CREATE VIEW examples Let’s take some examples of creating a new view using the CREATE VIEW statement. Creating a view to simplify a complex query The following query gets data from the tracks, albums, media_types and genres tables in the sample database using the inner join clause. SELECTtrackid,tracks.name,albums.TitleASalbum,media_types.NameASmedia,genres.NameASgenresFROMtracksINNERJOINalbumsONAlbums.AlbumId=tracks.AlbumIdINNERJOINmedia_typesONmedia_types.MediaTypeId=tracks.MediaTypeIdINNERJOINgenresONgenres.GenreId=tracks.GenreId; To create a view based on this query, you use the following statement: CREATEVIEWv_tracksASSELECTtrackid,tracks.name,albums.TitleASalbum,media_types.NameASmedia,genres.NameASgenresFROMtracksINNERJOINalbumsONAlbums.AlbumId=tracks.AlbumIdINNERJOINmedia_typesONmedia_types.MediaTypeId=tracks.MediaTypeIdINNERJOINgenresONgenres.GenreId=tracks.GenreId; From now on, you can use the following simple query instead of the complex one above. SELECT*FROMv_tracks; Creating a view with custom column names The following statement creates a view named v_albums that contains album title and the length of album in minutes: CREATEVIEWv_albums(AlbumTitle,Minutes)ASSELECTalbums.title,SUM(milliseconds)/60000FROMtracksINNERJOINalbumsUSING(AlbumId)GROUPBYAlbumTitle; In this example, we specified new columns for the view AlbumTitle for the albums.title column and Minutes for the expression SUM(milliseconds) / 60000 This query returns data from the v_albums view: SELECT*FROMv_albums; Drop View – show you how to drop a view from its database schema. Introduction to SQLite DROP VIEW statement The DROP VIEW statement deletes a view from the database schema. Here is the basic syntax of the DROP VIEW statement: DROPVIEW[IFEXISTS][schema_name.]view_name; In this syntax: First, specify the name of the view that you wants to remove after the DROP VIEW keywords. Second, specify the schema of the view that you want to delete. Third, use the IF EXISTS option to remove a view only if it exists. If the view does not exist, the DROP VIEW IF EXISTS statement does nothing. However, trying to drop a non-existing view without the IF EXISTS option will result in an error. Note that the DROP VIEW statement only removes the view object from the database schema. It does not remove the data of the base ta","date":"2021-03-01","objectID":"/sqlite/:4:6","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 14. Indexes Index – teach you about the index and how to utilize indexes to speed up your queries. What is an index? In relational databases, a table is a list of rows. In the same time, each row has the same column structure that consists of cells. Each row also has a consecutive rowid sequence number used to identify the row. Therefore, you can consider a table as a list of pairs: (rowid, row). Unlike a table, an index has an opposite relationship: (row, rowid). An index is an additional data structure that helps improve the performance of a query. SQLite uses B-tree for organizing indexes. Note that B stands for balanced, B-tree is a balanced tree, not a binary tree. The B-tree keeps the amount of data at both sides of the tree balanced so that the number of levels that must be traversed to locate a row is always in the same approximate number. In addition, querying using equality (=) and ranges (\u003e, \u003e=, \u003c,\u003c=) on the B-tree indexes are very efficient. How does an index work Each index must be associated with a specific table. An index consists of one or more columns, but all columns of an index must be in the same table. A table may have multiple indexes. Whenever you create an index, SQLite creates a B-tree structure to hold the index data. The index contains data from the columns that you specify in the index and the corresponding rowid value. This helps SQLite quickly locate the row based on the values of the indexed columns. Imagine an index in the database like an index of a book. By looking at the index, you can quickly identify page numbers based on the keywords. SQLite CREATE INDEX statement To create an index, you use the CREATE INDEX statement with the following syntax: CREATE[UNIQUE]INDEXindex_nameONtable_name(column_list); To create an index, you specify three important information: The name of the index after the CREATE INDEX keywords. The name of the table to the index belongs. A list of columns of the index. In case you want to make sure that values in one or more columns are unique like email and phone, you use the UNIQUE option in the CREATE INDEX statement. The CREATE UNIQUE INDEX creates a new unique index. SQLite UNIQUE index example Let’s create a new table named contacts for demonstration. CREATETABLEcontacts(first_nametextNOTNULL,last_nametextNOTNULL,emailtextNOTNULL); Suppose, you want to enforce that the email is unique, you create a unique index as follows: CREATEUNIQUEINDEXidx_contacts_emailONcontacts(email); To test this. First, insert a row into the contacts table. INSERTINTOcontacts(first_name,last_name,email)VALUES('John','Doe','john.doe@sqlitetutorial.net'); Second, insert another row with a duplicate email. INSERTINTOcontacts(first_name,last_name,email)VALUES('Johny','Doe','john.doe@sqlitetutorial.net'); SQLite issued an error message indicating that the unique index has been violated. Because when you inserted the second row, SQLite checked and made sure that the email is unique across of rows in email of the contacts table. Let’s insert two more rows into the contacts table. INSERTINTOcontacts(first_name,last_name,email)VALUES('David','Brown','david.brown@sqlitetutorial.net'),('Lisa','Smith','lisa.smith@sqlitetutorial.net'); If you query data from the contacts table based on a specific email, SQLite will use the index to locate the data. See the following statement: SELECTfirst_name,last_name,emailFROMcontactsWHEREemail='lisa.smith@sqlitetutorial.net'; To check if SQLite uses the index or not, you use the EXPLAIN QUERY PLAN statement as follows: EXPLAINQUERYPLANSELECTfirst_name,last_name,emailFROMcontactsWHEREemail='lisa.smith@sqlitetutorial.net'; SQLite multicolumn index example If you create an index that consists of one column, SQLite uses that column as the sort key. In case you create an index that has multiple columns, SQLite uses the additional columns as the second, third, … as the sort keys. SQLite sorts the data on the multicolumn index by the first column specified in the","date":"2021-03-01","objectID":"/sqlite/:4:7","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 15. Triggers Trigger – manage triggers in the SQLite database. What is an SQLite trigger An SQLite trigger is a named database object that is executed automatically when an INSERT, UPDATE or DELETE statement is issued against the associated table. When do we need SQLite triggers You often use triggers to enable sophisticated auditing. For example, you want to log the changes in the sensitive data such as salary and address whenever it changes. In addition, you use triggers to enforce complex business rules centrally at the database level and prevent invalid transactions. SQLite CREATE TRIGGER statement To create a new trigger in SQLite, you use the CREATE TRIGGER statement as follows: CREATETRIGGER[IFNOTEXISTS]trigger_name[BEFORE|AFTER|INSTEADOF][INSERT|UPDATE|DELETE]ONtable_name[WHENcondition]BEGINstatements;END; In this syntax: First, specify the name of the trigger after the CREATE TRIGGER keywords. Next, determine when the trigger is fired such as BEFORE, AFTER, or INSTEAD OF. You can create BEFORE and AFTER triggers on a table. However, you can only create an INSTEAD OF trigger on a view. Then, specify the event that causes the trigger to be invoked such as INSERT, UPDATE, or DELETE. After that, indicate the table to which the trigger belongs. Finally, place the trigger logic in the BEGIN END block, which can be any valid SQL statements. If you combine the time when the trigger is fired and the event that causes the trigger to be fired, you have a total of 9 possibilities: BEFORE INSERT AFTER INSERT BEFORE UPDATE AFTER UPDATE BEFORE DELETE AFTER DELETE INSTEAD OF INSERT INSTEAD OF DELETE INSTEAD OF UPDATE Suppose you use a UPDATE statement to update 10 rows in a table, the trigger that associated with the table is fired 10 times. This trigger is called FOR EACH ROW trigger. If the trigger associated with the table is fired one time, we call this trigger a FOR EACH STATEMENT trigger. As of version 3.9.2, SQLite only supports FOR EACH ROW triggers. It has not yet supported the FOR EACH STATEMENT triggers. If you use a condition in the WHEN clause, the trigger is only invoked when the condition is true. In case you omit the WHEN clause, the trigger is executed for all rows. Notice that if you drop a table, all associated triggers are also deleted. However, if the trigger references other tables, the trigger is not removed or changed if other tables are removed or updated. For example, a trigger references to a table named people, you drop the people table or rename it, you need to manually change the definition of the trigger. You can access the data of the row being inserted, deleted, or updated using the OLD and NEW references in the form: OLD.column_name and NEW.column_name. the OLD and NEW references are available depending on the event that causes the trigger to be fired. The following table illustrates the rules.: Action Reference INSERT NEW is available UPDATE Both NEW and OLD are available DELETE OLD is available SQLite triggers examples Let’s create a new table called leads to store all business leads of the company. CREATETABLEleads(idintegerPRIMARYKEY,first_nametextNOTNULL,last_nametextNOTNULL,phonetextNOTNULL,emailtextNOTNULL,sourcetextNOTNULL); SQLite BEFORE INSERT trigger example Suppose you want to validate the email address before inserting a new lead into the leads table. In this case, you can use a BEFORE INSERT trigger. First, create a BEFORE INSERT trigger as follows: CREATETRIGGERvalidate_email_before_insert_leadsBEFOREINSERTONleadsBEGINSELECTCASEWHENNEW.emailNOTLIKE'%_@__%.__%'THENRAISE(ABORT,'Invalid email address')END;END; We used the NEW reference to access the email column of the row that is being inserted. To validate the email, we used the LIKE operator to determine whether the email is valid or not based on the email pattern. If the email is not valid, the RAISE function aborts the insert and issues an error message. Second, insert a row with an invalid email into the leads table. INSERTI","date":"2021-03-01","objectID":"/sqlite/:4:8","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 16. Full-text search Full-text search – get started with the full-text search in SQLite. Introduction to SQLite full-text search A virtual table is a custom extension to SQLite. A virtual table is like a normal table. The difference between a virtual table and a normal table is where the data come from i.e., when you process a normal table, SQLite accesses the database file to retrieve data. However, when you access a virtual table, SQLite calls the custom code to get the data. The custom code can have specified logic to handle certain tasks such as getting data from multiple data sources. To use full-text search in SQLite, you use FTS5 virtual table module. The following CREATE VIRTUAL TABLE statement creates an FTS5 table with two columns: CREATEVIRTUALTABLEtable_nameUSINGFTS5(column1,column2...); Notice that you cannot add types, constraints, or PRIMARY KEY declaration in the CREATE VIRTUAL TABLE statement for creating an FTS5 table. If you do so, SQLite will issue an error. Like creating a normal table without specifying the primary key column, SQLite adds an implicit rowid column to the FTS5 table. The following example creates an FTS5 table named posts with two columns title and body.A CREATEVIRTUALTABLEpostsUSINGFTS5(title,body); Notice that you cannot add types, constraints, or PRIMARY KEY declaration in the CREATE VIRTUAL TABLE statement for creating an FTS5 table. If you do so, SQLite will issue an error. Like creating a normal table without specifying the primary key column, SQLite adds an implicit rowid column to the FTS5 table. The following example creates an FTS5 table named posts with two columns title and body. CREATEVIRTUALTABLEpostsUSINGFTS5(title,body); Similar to a normal table, you can insert data into the posts table as follows: INSERTINTOposts(title,body)VALUES('Learn SQlite FTS5','This tutorial teaches you how to perform full-text search in SQLite using FTS5'),('Advanced SQlite Full-text Search','Show you some advanced techniques in SQLite full-text searching'),('SQLite Tutorial','Help you learn SQLite quickly and effectively'); And query data against it: SELECT*FROMposts; Querying data using full-text search You can execute a full-text query against an FTS5 table using one of these three ways. First, use a MATCH operator in the WHERE clause of the SELECT statement. For example, to get all rows that have the term fts5, you use the following query: SELECT*FROMpostsWHEREpostsMATCH'fts5'; Second, use an equal (=) operator in the WHERE clause of the SELECT statement. The following statement returns the same result as the statement above: SELECT*FROMpostsWHEREposts='fts5'; Third, use a tabled-value function syntax. In this way, you use the search term as the first table argument: SELECT*FROMposts('fts5'); By default, FTS5 is case-independent. It treats the terms fts5FTS5 and Fts5 the same. To sort the search results from the most to least relevant, you use the ORDER BY clause as follows: SELECT*FROMpostsWHEREpostsMATCH'text'ORDERBYrank; Using full-text query syntax A full-text search query is made up of phrases, where each phrase is an ordered list of one or more tokens. You can use the “+” operator to concatenate two phrases as the following example: \"learn SQLite\"\"learn + SQLite\" FTS5 determines whether a document matches a phrase if the document contains at least one subsequence of tokens that match the sequence of tokens used to construct the phrase. The following query returns all documents that match the search term Learn SQLite: SELECT*FROMpostsWHEREpostsMATCH'learn SQLite'; Prefix searches You can use the asterisk (*) as a prefix token. When a phrase contains the asterisk (*), it will match any document that contains the token that begins with the phrase. For example, search* matches with search, searching, searches, etc. See the following example: SELECT*FROMpostsWHEREposts='search*'; Boolean operators You can use the Boolean operator e.g., NOT, OR, or AND to combine queries. q1 AND q2: mat","date":"2021-03-01","objectID":"/sqlite/:4:9","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["ISYS1055"],"content":"Section 17. SQLite tools SQLite Commands – show you the most commonly used command in the sqlite3 program. The SQLite project delivers a simple command-line tool named sqlite3 (or sqlite3.exe on Windows) that allows you to interact with the SQLite databases using SQL statements and commands. Connect to an SQLite database To start the sqlite3, you type the sqlite3 as follows: \u003esqlite3 SQLite version 3.29.0 2019-07-10 17:32:03 Enter \".help\" for usage hints. Connected to a transient in-memory database. Use \".open FILENAME\" to reopen on a persistent database. sqlite\u003e By default, an SQLite session uses the in-memory database, therefore, all changes will be gone when the session ends. To open a database file, you use the .open FILENAME command. The following statement opens the chinook.db database: sqlite\u003e .open c:\\sqlite\\db\\chinook.db If you want to open a specific database file when you connect to the SQlite database, you use the following command: \u003esqlite3 c:\\sqlite\\db\\chinook.db SQLite version 3.13.0 2016-05-18 10:57:30 Enter \".help\" for usage hints. sqlite\u003e If you start a session with a database name that does not exist, the sqlite3 tool will create the database file. For example, the following command creates a database named sales in the C:\\sqlite\\db\\ directory: \u003esqlite3 c:\\sqlite\\db\\sales.db SQLite version 3.29.0 2019-07-10 17:32:03 Enter \".help\" for usage hints. sqlite\u003e Show all available commands and their purposes To show all available commands and their purpose, you use the .help command as follows: .help Show databases in the current database connection To show all databases in the current connection, you use the .databases command. The .databases command displays at least one database with the name: main. For example, the following command shows all the databases of the current connection: sqlite\u003e .database seq name file --- --------------- -------------------------- 0 main c:\\sqlite\\db\\sales.db sqlite\u003e To add an additional database in the current connection, you use the statement ATTACH DATABASE. The following statement adds the chinook database to the current connection. sqlite\u003e ATTACH DATABASE \"c:\\sqlite\\db\\chinook.db\" AS chinook; Now if you run the .database command again, the sqlite3 returns two databases: main and chinook. sqlite\u003e .databases seq name file --- --------------- --------------------- 0 main c:\\sqlite\\db\\sales.db 2 chinook c:\\sqlite\\db\\chinook.db Exit sqlite3 tool To exit the sqlite3 program, you use the .exit command. sqlite\u003e.exit Show tables in a database To display all the tables in the current database, you use the .tables command. The following commands open a new database connection to the chinook database and display the tables in the database. \u003esqlite3 c:\\sqlite\\db\\chinook.db SQLite version 3.29.0 2019-07-10 17:32:03 Enter \".help\" for usage hints. sqlite\u003e .tables albums employees invoices playlists artists genres media_types tracks customers invoice_items playlist_track sqlite\u003e If you want to find tables based on a specific pattern, you use the .table pattern command. The sqlite3 uses the LIKE operator for pattern matching. For example, the following statement returns the table that ends with the string es. sqlite\u003e .table '%es' employees genres invoices media_types sqlite\u003e Show the structure of a table To display the structure of a table, you use the .schema TABLE command. The TABLE argument could be a pattern. If you omit it, the .schema command will show the structures of all the tables. The following command shows the structure of the albums table. sqlite\u003e .schema albums CREATE TABLE \"albums\" ( [AlbumId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, [Title] NVARCHAR(160) NOT NULL, [ArtistId] INTEGER NOT NULL, FOREIGN KEY ([ArtistId]) REFERENCES \"artists\" ([ArtistId]) ON DELETE NO ACTION ON UPDATE NO ACTION ); CREATE INDEX [IFK_AlbumArtistId] ON \"albums\" ([ArtistId]); sqlite\u003e To show the schema and the content of the sqlite_stat tables, you use the .fullschema command. sqlite\u003e.fullschema Show ","date":"2021-03-01","objectID":"/sqlite/:4:10","tags":null,"title":"Getting started SQLite","uri":"/sqlite/"},{"categories":["virtual environment"],"content":"Introduction RStudio Server enables you to provide a browser based interface to a version of R running on a remote Linux server, bringing the power and productivity of the RStudio IDE to server-based deployments of R. In this blog, I will talk about how to launch a Docker container that allows us to run RStudio in a browser using the –rm flag when we run Docker makes the container ephemeral that mean it is automatically deleted after we close the container. We do this as to not build up a large collection of containers on our machine and waste space. After, I will talk how to link a volume of our local host disk to the Docker container if we want to be able to access and save data, scripts and any other files. ","date":"2021-03-01","objectID":"/rstudio/:1:0","tags":["r","r language","docker"],"title":"Launching RStudio in Docker","uri":"/rstudio/"},{"categories":["virtual environment"],"content":"Prerequisite Docker installation. Docker is available for Mac/Windows/Linux and is easily installed. You can following the link at the below for install Docker. {% post_link dockerInstall %} ","date":"2021-03-01","objectID":"/rstudio/:2:0","tags":["r","r language","docker"],"title":"Launching RStudio in Docker","uri":"/rstudio/"},{"categories":["virtual environment"],"content":"Launching RStudio in Docker We will ask Docker to run an image that already exists, we will use the verse Docker image from Rocker which will allow us to run RStudio inside the container and has many useful R packages already installed. This command will lead RStudio-Server to launch invisibly. docker run --rm -e PASSWORD=yourpasswordhere -p 8787:8787 rocker/verse NOTE: You must set a unique PASSWORD (not ‘rstudio’) first! Optional: -p and –rm are flags that allow you to customize how you run the container. -p tells Docker that you will be using a port to see RStudio in your web browser (at a location which we specify afterwards as port 8787:8787). Finally, –rm ensures that when we quit the container, the container is deleted. If we did not do this, everytime we run a container, a version of it will be saved to our local computer. This can lead to the eventual wastage of a lot of disk space until we manually remove these containers. Later we will show you how to save your container if you want to do so. If you try to run a Docker container which you have not installed locally then Docker will automatically search for the container on Docker Hub (an online repository for docker images) and download it if it exists. ","date":"2021-03-01","objectID":"/rstudio/:3:0","tags":["r","r language","docker"],"title":"Launching RStudio in Docker","uri":"/rstudio/"},{"categories":["virtual environment"],"content":"Connect RStudio from browser To connect to it, open a browser and enter http://, followed by your ip address, followed by :8787 If you are running a Linux machine, you can use localhost as the ip address. For example: http://localhost:8787 This should lead to you being greeted by the RStudio welcome screen. Log in using: username: rstudio password: The PWD you set. Now you should be able to work with RStudio in your browser in much the same way as you would on your desktop. Importance Note: If you want to bypass the authentication, you can simply set the environmental variable, like below. -e DISABLE_AUTH=true Try to now look at your files of your virtual computer (docker container). Go to file -\u003e open file. You will see that there are actually no files. The reason for this is that this image came with no files. Next, open a new R Script, e.g. by going to file -\u003e New file -\u003e R Script. Enter the following code in the script, run it and save it. ## make x the numbers from 1 to 5, and y the numbers from 6-10 x \u003c- 1:5 y \u003c- 6:10 ## plot x against y plot(x, y) If you look at your files again now, you will see the script file. Importance Note: Now, given that we used the –rm flag when we launched the Docker container, anything we create on the machine will be gone, if we are not link a volume to a Docker container. Please, continue read this blog, how to link a volume to a Docker container at the below. ","date":"2021-03-01","objectID":"/rstudio/:4:0","tags":["r","r language","docker"],"title":"Launching RStudio in Docker","uri":"/rstudio/"},{"categories":["virtual environment"],"content":"Stopping RStudio in Docker Close the browser tab where you have RStudio open, and then go to your terminal window from where you launched the Docker container and type Contol+C. This shuts down the Docker container. ","date":"2021-03-01","objectID":"/rstudio/:5:0","tags":["r","r language","docker"],"title":"Launching RStudio in Docker","uri":"/rstudio/"},{"categories":["virtual environment"],"content":"Linking a volume to a Docker container to access data and save files This section, we will talk about how can we save our work if the container is deleted when we exit the container. One solution is to link a volume (for example your local hard drive) to the container so that you can access the data there as well as being able to save things there. This time when we launch our container we will use the -v flag along with the path to our project’s directory path. Your launch command should look something like this at the below, although the path will differ depending on where you saved the data to on your computer. On the left hand side of the : is the path on your own computer. On the right hand side is the path on the container. This should almost always start with /home/rstudio/. From your project’s root directory: docker run --rm -e PASSWORD=yourpasswordhere -v $(pwd):/home/rstudio -p 8787:8787 rocker/verse Using full directory path: docker run --rm -e PASSWORD=happyman -v /home/yanboyang713/Documents/Rdemo:/home/rstudio/ -p 8787:8787 rocker/verse Again, typing like http://127.0.0.1:8787 in your browser as the url to get RStudio to run. This time when you launch RStudio in a Docker container and you try to open/create a file you should be able to see in your local host directories. There is a example at the below. Let’s also save \u0026\u0026 run the script as demo.R. After you can run this script. Now close the RStudio browser and exit your Docker container via the terminal. Then look inside the directories on your local host to see if you can see the two files you created. ## load ggplot library library(ggplot2) ## make x the numbers from 1 to 5, and y the numbers from 6-10 x \u003c- 1:5 y \u003c- 6:10 ## plot qplot(x, y) ## save the plot ggsave(filename = 'demo.pdf') [yanboyang713@boyang Rdemo]$ ls demo.pdf demo.R ","date":"2021-03-01","objectID":"/rstudio/:6:0","tags":["r","r language","docker"],"title":"Launching RStudio in Docker","uri":"/rstudio/"},{"categories":["Opencv"],"content":"Introduction OpenCV is a popular framework widely used in the development of products for intelligent video analytics. Such solutions use both classic algorithms of computer vision (e.g. an algorithm for optical flow detection), and AI-based approaches, in particular, neural networks. ","date":"2021-02-06","objectID":"/opencv/:1:0","tags":["Opencv"],"title":"Getting started with Opencv","uri":"/opencv/"},{"categories":["Opencv"],"content":"Installing with Docker build: docker build -t my_pi_opencv_img . run: docker run -it --rm \\ -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY \\ -v /home/yanboyang713/Documents/imagezmq:/home \\ --name my_opencv_app_run \\ my_pi_opencv_img \\ /bin/bash python -c “import cv2; print(cv2.version)” \\ –device=/dev/video0:/dev/video0 allows use of webcam -v /tmp/.X11-unix:/tmp/.X11-unix helps in X11 forwarding so that we can use functions like cv::imshow. ","date":"2021-02-06","objectID":"/opencv/:2:0","tags":["Opencv"],"title":"Getting started with Opencv","uri":"/opencv/"},{"categories":["Video Processing"],"content":"Introduction ","date":"2021-02-06","objectID":"/realtimedistributedvideoprocessing/:1:0","tags":["Opencv","ZeroMQ"],"title":"High-performance Scalable Realtime Distributed Video Processing","uri":"/realtimedistributedvideoprocessing/"},{"categories":["Video Processing"],"content":"Opencv ","date":"2021-02-06","objectID":"/realtimedistributedvideoprocessing/:2:0","tags":["Opencv","ZeroMQ"],"title":"High-performance Scalable Realtime Distributed Video Processing","uri":"/realtimedistributedvideoprocessing/"},{"categories":["Azure"],"content":"Create a virtual network using the Azure CLI A virtual network enables Azure resources, like virtual machines (VMs), to communicate privately with each other, and with the internet. In this quickstart, you learn how to create a virtual network. After creating a virtual network, you deploy two VMs into the virtual network. You then connect to the VMs from the internet, and communicate privately over the new virtual network. ","date":"2021-01-05","objectID":"/azurevnet/:1:0","tags":["VNET"],"title":"Azure Networking","uri":"/azurevnet/"},{"categories":["Azure"],"content":"Create a resource group and a virtual network Before you can create a virtual network, you have to create a resource group to host the virtual network. Create a resource group with az group create. This example creates a resource group named myResourceGroup in the eastus location: az group create --name myResourceGroup --location eastus Create a virtual network with az network vnet create. This example creates a default virtual network named myVirtualNetwork with one subnet named default: az network vnet create \\ --name myVirtualNetwork \\ --resource-group myResourceGroup \\ --subnet-name default ","date":"2021-01-05","objectID":"/azurevnet/:1:1","tags":["VNET"],"title":"Azure Networking","uri":"/azurevnet/"},{"categories":["Azure"],"content":"Create virtual machines Create two VMs in the virtual network. ","date":"2021-01-05","objectID":"/azurevnet/:1:2","tags":["VNET"],"title":"Azure Networking","uri":"/azurevnet/"},{"categories":["Azure"],"content":"Create the first VM Create a VM with az vm create. If SSH keys don’t already exist in a default key location, the command creates them. To use a specific set of keys, use the –ssh-key-value option. The –no-wait option creates the VM in the background, so that you can continue to the next step. This example creates a VM named myVm1: boyang@Azure:~$ az vm create \\ \u003e --resource-group myResourceGroup \\ \u003e --name myVm1 \\ \u003e --image UbuntuLTS \\ \u003e --generate-ssh-keys \\ \u003e --no-wait SSH key files '/home/boyang/.ssh/id_rsa' and '/home/boyang/.ssh/id_rsa.pub' have been generated under ~/.ssh to allowSSH access to the VM. If using machines without permanent storage, back up your keys to a safe location. ","date":"2021-01-05","objectID":"/azurevnet/:1:3","tags":["VNET"],"title":"Azure Networking","uri":"/azurevnet/"},{"categories":["Azure"],"content":"Create the second VM Since you used the –no-wait option in the previous step, you can go ahead and create the second VM named myVm2. boyang@Azure:~$ az vm create \\ \u003e --resource-group myResourceGroup \\ \u003e --name myVm2 \\ \u003e --image UbuntuLTS \\ \u003e --generate-ssh-keys {- Finished .. \"fqdns\": \"\", \"id\": \"/subscriptions/bfc7bf32-524b-4960-802a-e2881b6ab634/resourceGroups/myResourceGroup/providers/Microsoft.Compute/virtualMachines/myVm2\", \"location\": \"eastus\", \"macAddress\": \"00-0D-3A-11-56-48\", \"powerState\": \"VM running\", \"privateIpAddress\": \"10.0.0.5\", \"publicIpAddress\": \"52.186.147.136\", \"resourceGroup\": \"myResourceGroup\", \"zones\": \"\" } NOTE: The publicIpAddress. You will use this address to connect to the VM from the internet in the next step. ","date":"2021-01-05","objectID":"/azurevnet/:1:4","tags":["VNET"],"title":"Azure Networking","uri":"/azurevnet/"},{"categories":["Azure"],"content":"Connect to a VM from the internet In this command, replace with the public IP address of your myVm2 VM: ssh 52.186.147.136 ","date":"2021-01-05","objectID":"/azurevnet/:1:5","tags":["VNET"],"title":"Azure Networking","uri":"/azurevnet/"},{"categories":["Azure"],"content":"NSG ","date":"2021-01-05","objectID":"/azurevnet/:2:0","tags":["VNET"],"title":"Azure Networking","uri":"/azurevnet/"},{"categories":["Azure"],"content":"Create VM ","date":"2021-01-05","objectID":"/azurevnet/:3:0","tags":["VNET"],"title":"Azure Networking","uri":"/azurevnet/"},{"categories":["Azure"],"content":"Create a public IP address for the MySQL VM. az network public-ip create \\ --resource-group $RgName \\ --name MyPublicIP-Sql ","date":"2021-01-05","objectID":"/azurevnet/:3:1","tags":["VNET"],"title":"Azure Networking","uri":"/azurevnet/"},{"categories":["Azure"],"content":"Create a NIC for the MySQL VM. az network nic create \\ --resource-group $RgName \\ --name MyNic-Sql \\ --vnet-name MyVnet \\ --subnet MySubnet-BackEnd \\ --network-security-group MyNsg-BackEnd \\ --public-ip-address MyPublicIP-Sql ","date":"2021-01-05","objectID":"/azurevnet/:3:2","tags":["VNET"],"title":"Azure Networking","uri":"/azurevnet/"},{"categories":["Azure"],"content":"Create a MySQL VM in the back-end subnet. az vm create \\ --resource-group $RgName \\ --name MyVm-Sql \\ --nics MyNic-Sql \\ --image UbuntuLTS \\ --admin-username azureadmin \\ --generate-ssh-keys ","date":"2021-01-05","objectID":"/azurevnet/:3:3","tags":["VNET"],"title":"Azure Networking","uri":"/azurevnet/"},{"categories":["Azure"],"content":"Create a new Azure Machine Learning I recommand you create a new AML workspace use an Azure Resource Manager Template from Azure CLI. The Azure Resource Manager template can be found in the 201-machine-learning-advanced in GitHub repository. This template creates the following Azure services, all those various services are required by the Azure Machine Learning workspace: Azure Storage Account Azure Key Vault Azure Application Insights Azure Container Registry Azure Machine Learning workspace Firstly, we need create a resource group, there is a example at the below. The resource group is the container that holds the services. Create a new resource group, name: aml and location: eastus az group create -l eastus -n aml NOTE: List all of Azure location command at the below. az account list-locations I suggest you deploy all dependent resources behind a virtual network. az deployment group create \\ --name \"boyyan-ml\" \\ --resource-group \"aml\" \\ --template-uri \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/201-machine-learning-advanced/azuredeploy.json\" \\ --parameters workspaceName=\"boyyanworkspace\" \\ location=\"eastus\" \\ vnetOption=\"new\" \\ vnetName=\"mlvnet\" \\ storageAccountBehindVNet=\"true\" \\ keyVaultBehindVNet=\"true\" \\ containerRegistryBehindVNet=\"true\" \\ containerRegistryOption=\"new\" \\ containerRegistrySku=\"Premium\" \\ privateEndpointType=\"AutoApproval\" NOTE: Total transitive private endpoint usage 0 is equal or greater than quota 0. Please increase quotaby following the doc here(https://docs.microsoft.com/azure/machine-learning/how-to-manage-quotas#private-endpoint-and-private-dns-quota-increases). ","date":"2021-01-04","objectID":"/aml/:1:0","tags":["AML"],"title":"Azure Machine Learning","uri":"/aml/"},{"categories":null,"content":"Introduction ","date":"2021-01-04","objectID":"/lldb/:1:0","tags":["LLDB"],"title":"Getting started with LLDB","uri":"/lldb/"},{"categories":null,"content":"Install yay -S clang lldb ","date":"2021-01-04","objectID":"/lldb/:2:0","tags":["LLDB"],"title":"Getting started with LLDB","uri":"/lldb/"},{"categories":null,"content":"Enable Debug Symbols gcc: c++ -g demo.cpp Clang: clang++ -g demo.cpp CMAKE: cmake -DCMAKE_BUILD_TYPE=Debug ","date":"2021-01-04","objectID":"/lldb/:3:0","tags":["LLDB"],"title":"Getting started with LLDB","uri":"/lldb/"},{"categories":null,"content":"Introduction ","date":"2021-01-04","objectID":"/bashscriptingwithpython/:1:0","tags":["Bash","Python"],"title":"Replacing Bash Scripting with Python","uri":"/bashscriptingwithpython/"},{"categories":["virtual environment"],"content":"Overview https://docs.oracle.com/en/operating-systems/oracle-linux/podman/podman-containers.html#podman-command-reference Let’s we retire Docker and move to Podman. A whole new revolution of containerization started with the Docker where the daemon process manages the whole bunch of things and became one of the most popular and widely used container management systems. However, with the advent of standards from the Open Container Initiative (OCI), other runtimes are being developed. One such runtime is CRI-O, which was created along with a set of tools to provide alternate ways to work directly with containers. The following text describes some of the tools being developed to use with the CRI-O runtime or as alternative container stand-alone tools. These tools can be used with docker-formatted containers or OCI-conformant containers. While some of these commands were made to use with CRI-O, others can also interact with the docker daemon (as a replacement for features of the docker command) or used to manage containers with no active runtime environment. podman: The podman command can run and manage containers and container images. It supports most of the same features and command options you find in the docker command, with the main differences being that podman doesn’t require the docker service or any other active container runtime for the command to work. Also, podman stores its data in the same directory structure used by CRI-O, which will allow podman to eventually work with containers being actively managed by CRI-O in OpenShift. skopeo: The skopeo command lets you inspect images from container image registries, get images and image layers, and use signatures to create and verify images. Buildah: The buildah command can be used in place of docker build to build container images from Dockerfiles and, ultimately, files in other formats. As much as possible, Podman and its related utilities, Buildah and Skopeo, are designed to work independently of each other. For example, Buildah has no dependency on Podman, which means it is possible to separate the container build infrastructure from environments in which the containers are intended to run. You can install the buildah package on the same system that you run Podman; or, you can install the package on an alternate system, if required. Similarly, you can install Skopeo separate from the other utilities, according to your specific requirements. The following sections describe Docker and Podman comparison, Podman, runc, skopeo, and buildah. ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:1:0","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Why we from Docker move to Podman: Docker and Podman comparison ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:2:0","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Docker Docker is a containerization stage where we can bundle our application with its libraries and conditions inside that container. Docker Container is a to some degree like a virtual machine. Unlike virtual machines where hardware is virtualized, In Docker, the containers running share the host OS kernel. Docker Flow Two main blocks of docker are : Docker Daemon and Docker CLI. Docker Daemon: A constant background process that helps to manage/create Docker images, containers, networks, and storage volumes. Docker Engine REST API: An API used by applications to interact with the Docker daemon; it can be accessed by an HTTP client. Docker CLI: A Docker command line client for interacting with the Docker daemon. a.k.a the Docker command. If we think differently we could just connect some problems with Docker: As we all know Docker runs on a single process it could result into single point of failure. All the child processes are owned by this process. At any point if Docker daemon fails, all the child process losses their track and enters into orphaned state. Security vulnerabilities. All the steps needs to be performed by root for Docker operations. Now we know how Docker works, let’s come to the main topic about Podman And how we can overcome on most of the problems associated with containers. So, you must be wondering “What is Podman?” ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:2:1","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Podman Podman is a daemon-less container engine for developing, managing, and running OCI Containers on your Linux System. Containers can either be run as root or in rootless mode. Podman directly interacts with Image registry, containers and image storage. As we know Docker is built on top of runC runtime container and uses daemon, Instead of using daemon in Podman, it is directly using runC runtime container. There are a few things to unpack about podman No need to start or manage a daemon process like the Docker daemon. The commands which works with Docker works the same for Podman. alias docker=podman There is Compatibility between Podman and Docker images. ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:2:2","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Podman Podman provides a lightweight utility to run and manage Open Container Initiative (OCI) compatible containers. As such, a Podman deployment can re-use existing container images that are designed for Kubernetes. Podman integrates with Docker Hub and Oracle Container Registry to share applications in a software-as-a-service (SaaS) cloud. Using podman to work with containers. The podman command lets you run containers as standalone entities, without requiring that Kubernetes, the Docker runtime, or any other container runtime be involved. It is a tool that can act as a replacement for the docker command, implementing the same command-line syntax, while it adds even more container management features. The podman features include: Based on docker interface: Because podman syntax mirrors the docker command, transitioning to podman should be easy for those familiar with docker. Managing containers and images: Both Docker- and OCI-compatible container images can be used with podman to: Run, stop and restart containers Create and manage container images (push, commit, configure, build, and so on) Working with no runtime: No runtime environment is used by podman to work with containers. Podman uses the CRI-O back-end store directory, /var/lib/containers, instead of using the Docker storage location (/var/lib/docker), by default. ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:3:0","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Installing podman Installing Podman Official Guide in here. I am using Manjaro Linux. So, I am using pacman install podman. sudo pacman -S podman If you have problems when running Podman in rootless mode follow the instructions here. ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:3:1","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Verifying Podman Use the podman info command to display information about the configuration and version of Podman. podman info ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:3:2","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Running containers with podman If you are used to using the docker command to work with containers, you will find most of the features and options match those of podman. Because the use of podman mirrors the features and syntax of the docker command, you can refer to Working with Docker Formatted Container Images for examples of how to use those options to work with containers. Simply replace docker with podman in most cases. Here are some examples of using podman. Pull a container image to the local system [yanboyang713@boyang ~]$ podman pull docker.io/archlinux Trying to pull docker.io/library/archlinux:latest... Getting image source signatures Copying blob 503b62125c98 done Copying blob cc7ff1c0e722 done Copying config c689d2874b done Writing manifest to image destination Storing signatures c689d2874bf23af60a962f3f4d1ea7532f7c30992839277415e4e49042f9ba7f List local container images [yanboyang713@boyang ~]$ podman images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/library/archlinux latest c689d2874bf2 10 days ago 416 MB Run a container image: This runs a container image and opens a shell inside the container [yanboyang713@boyang ~]$ podman run -it archlinux /bin/bash [root@760265e5bb5e /]# pwd / List containers that are running or have exited [yanboyang713@boyang ~]$ podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 760265e5bb5e archlinux /bin/bash About a minute ago Exited (0) 41 seconds ago gracious_tharp Remove a container or image: remove a container by its container ID [yanboyang713@boyang ~]$ podman rm 760265e5bb5e 760265e5bb5e259af2a344c762fd66ca280430719e6a8134738dabbab883540e Remove a container image by its image ID or name (use -f to force) ## podman rmi registry.access.redhat.com/rhel7/rhel-minimal ## podman rmi de9c26f23799 ## podman rmi -f registry.access.redhat.com/rhel7/rhel:latest Build a container Create Dockerfile [yanboyang713@boyang ~]$ echo -e \"FROM docker.io/ubuntu\\nENTRYPOINT echo \\\"Podman build this container.\\\"\" | tee ./Dockerfile FROM docker.io/ubuntu ENTRYPOINT echo \"Podman build this container.\" Build Image [yanboyang713@boyang ~]$ podman build -t podbuilt . STEP 1: FROM docker.io/ubuntu STEP 2: ENTRYPOINT echo \"Podman build this container.\" STEP 3: COMMIT podbuilt --\u003e a8f66c6fb63 a8f66c6fb6385f1417c616b098fbda24c7a76e1b70de43c90103d5e28b6b0cd3 Run [yanboyang713@boyang ~]$ podman run podbuilt Podman build this container. ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:3:3","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Buildah Buildah is a utility for creating Open Container Intiative (OCI) compatible container images. Buildah provides a wider range of customization options than the more generic podman build command. If you create container images by using Buildah, you do not need a running daemon for the utility to function. Buildah also does not cache builds by default. In addition, the utility can push container images to container registries, so it is well-suited for use with deployment scripts and automated build pipelines. create a working container, either from scratch or using an image as a starting point create an image, either from a working container or via the instructions in a Dockerfile images can be built in either the OCI image format or the traditional upstream docker image format mount a working container’s root filesystem for manipulation unmount a working container’s root filesystem use the updated contents of a container’s root filesystem as a filesystem layer to create a new image delete a working container or an image rename a local container ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:4:0","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Installing I am using Manjaro Linux. So, I am using pacman install buildah and fuse-overlayfs. yay -S buildah fuse-overlayfs NOTE: If you want to run as non-root user, also install fuse-overlayfs for better performance and storage space efficiency. ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:4:1","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Verifying Buildah Check the current version of Buildah by specifying the –version flag. buildah --version ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:4:2","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Skopeo Skopeo is a utility for managing container images on remote container registries. This utility is particularly useful for inspecting the contents of a container image without needing to first download it. If you host container images in your own container registry, you can use Skopeo to seamlessly move container images from one location to another. In particular, Skopeo is useful for bulk-deleting unneeded container images. With the skopeo command, you can work with container images from registries without using the docker daemon or the docker command. Registries can include the Docker Registry, your own local registries, or Atomic registries. Activities you can do with skopeo include: inspect: The output of a skopeo inspect command is similar to what you see from a docker inspect command: low-level information about the container image. That output can be in json format (default) or raw format (using the –raw option). copy: With skopeo copy you can copy a container image from a registry to another registry or to a local directory. layers: The skopeo layers command lets you download the layers associated with images so that they are stored as tarballs and associated manifest files in a local directory. Like the buildah command and other tools that rely on the containers/image library, the skopeo command can work with images from container storage areas other than those associated with Docker. Available transports to other types of container storage include: containers-storage (for images stored by buildah and CRI-O), ostree (for atomic and system containers), oci (for content stored in an OCI-compliant directory), and others. See the skopeo man page for details. ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:5:0","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Installing Skopeo Installing Skopeo Official Guide in here. I am using Manjaro Linux. So, I am using pacman install podman. sudo pacman -S skopeo ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:5:1","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Verifying Skopeo Use the skopeo -h command for version information and a command reference. skopeo -h ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:5:2","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Inspects Show properties of archlinux skopeo inspect docker://docker.io/library/archlinux Show container configuration from archlinux skopeo inspect --config docker://docker.io/library/archlinux | jq NOTE: If show bash: jq: command not found, please install jq run: yay -S jq ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:5:3","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Copying images skopeo can copy container images between various storage mechanisms, including: Container registries The Quay, Docker Hub, OpenShift, GCR, Artifactory … Container Storage backends github.com/containers/storage (Backend for Podman, CRI-O, Buildah and friends) Docker daemon storage Local directories Local OCI-layout directories Copy from one Docker registry to another Use the skopeo copy command to copy an image between registries without needing to download it locally. skopeo copy docker://docker.io/library/oraclelinux:8-slim \\ docker://example.com/os/oraclelinux:8-slim NOTE: If the destination registry requires a signature, provide the required key-id by using the –sign-by parameter. Copy from one Docker Registry to local Podman container storage You can also copy an image to your local Podman container storage by using the containers-storage: prefix skopeo copy docker://docker.io/library/oraclelinux:8-slim \\ containers-storage:oraclelinux:8-slim Copy a Docker Registry to local directory To download an image and review its internal content offline you can specify a directory with the dir: prefix. For example, to extract to the oraclelinux folder in your home directory skopeo copy docker://docker.io/library/oraclelinux:8-slim \\ dir:/home/$USER/oraclelinux In that example, the oraclelinux folder contains a manifest.json file and multiple tarballs representing the image that has been copied. ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:5:4","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["virtual environment"],"content":"Delete an image Delete an image from container storage with Skopeo. Use the skopeo delete command to delete an image from a remote registry or your local container storage. skopeo delete containers-storage:oraclelinux:8-slim ","date":"2021-01-02","objectID":"/podmanandskopeoandbuildah/:5:5","tags":["Podman","Skopeo","Buildah"],"title":"The Ultimate Guide to Podman, Skopeo and Buildah","uri":"/podmanandskopeoandbuildah/"},{"categories":["Azure"],"content":"Introduction Azure Data Science Virtual Machine(DSVM) is Azure Virtual Machine image, pre-installed, configured and tested with several popular tools that are data analytics, machine learning and AI training. DSVM will be your best option as your develop environment base on cloud. DSVM helps in flexible virtualization without the need for purchasing and maintenance of hardware. You will see the DSVM highlights at the below section. DSVM almost cover all of popular tools for data science. This post document will fouced on how to create DSVM step by step. Others post will introduce DSVM integrate tool one by one. ","date":"2020-09-09","objectID":"/dsvm/:1:0","tags":["Azure","Virtual Machine"],"title":"Azure Data Science Virtual Machine Getting Started","uri":"/dsvm/"},{"categories":["Azure"],"content":"DSVM compare with Azure Machine Learning Compute Instances(AML-CI) I personal recommend you using DSVM instead of AML-CI. A couple of reason list at the below. DSVM is fully managed by yourself. You have all the permissions, so you can do whatever you want. DSVM have Windows OS and Ubuntu OS avaliable, AML-CI only have Ubuntu avaliable. But, I tend to use Ubuntu. DSVM have RDP Access feature. AML-CI have NOT this feature. Note: RDP (Remote Desktop Protocol) is a network communications protocol developed by Microsoft, which allows users to remotely connect to another computer. It is an extension of the T.120 protocols that are standards of the ITU (International Telecommunications Union). RDP provides a graphical interface for connecting two computers. To use RDP, the computer from which the end user originates the request must be running RDP client software. The computer that is being accessed must be running RDP server software. RDP client software provided by Microsoft is called Remote Desktop Connection (it used to be called “Terminal Services Client” and you may occasionally see it referred to that way.) Many non-Microsoft RDP clients and servers are available as well, including the open source client rdesktop. rdesktop is a command-line client; there are graphical user interface clients built on top of rdesktop. The copyright of this Note belong to ericom, original article and more details can be found on ericom website ","date":"2020-09-09","objectID":"/dsvm/:2:0","tags":["Azure","Virtual Machine"],"title":"Azure Data Science Virtual Machine Getting Started","uri":"/dsvm/"},{"categories":["Azure"],"content":"DSVM some highlights Anaconda Python Jupyter, JupyterLab, and JupyterHub Deep learning with TensorFlow and PyTorch Machine learning with xgboost, Vowpal Wabbit, and LightGBM Julia Azure SDKs and libraries Azure Machine Learning SDKs and sample notebooks R support Spark This image is pre-configured with NVIDIA drivers, CUDA Toolkit, and cuDNN library for GPU workloads if using NC class VM SKUs. ","date":"2020-09-09","objectID":"/dsvm/:3:0","tags":["Azure","Virtual Machine"],"title":"Azure Data Science Virtual Machine Getting Started","uri":"/dsvm/"},{"categories":["Azure"],"content":"How to create a DSVM You need to do is log into the Azure Management portal through https://portal.azure.com . You should have an active Azure subscription to use the port. If you don’t have a subscription, you can sign up for a free trial account. After logging in, you have to click Create a resource at the first position in Azure Services Section. In the Marketplace bar, search Data Science Virtual Machine for Linux (Ubuntu) Click create Now, we are enter the really steps for create our DSVM. Here, you would find seven distinct tabs on the Top of the screen. They are “Basics”, “Disks”, “Networking”, “Management”, “Advanced”, “Tags”, as well as “Review + create”. I will one by one, talk how to set in details. ","date":"2020-09-09","objectID":"/dsvm/:4:0","tags":["Azure","Virtual Machine"],"title":"Azure Data Science Virtual Machine Getting Started","uri":"/dsvm/"},{"categories":["Azure"],"content":"“Basics” Tab configuration ","date":"2020-09-09","objectID":"/dsvm/:4:1","tags":["Azure","Virtual Machine"],"title":"Azure Data Science Virtual Machine Getting Started","uri":"/dsvm/"},{"categories":["Azure"],"content":"“Disks” Tab configuration ","date":"2020-09-09","objectID":"/dsvm/:4:2","tags":["Azure","Virtual Machine"],"title":"Azure Data Science Virtual Machine Getting Started","uri":"/dsvm/"},{"categories":["Azure"],"content":"“Networking” Tab configuration ","date":"2020-09-09","objectID":"/dsvm/:4:3","tags":["Azure","Virtual Machine"],"title":"Azure Data Science Virtual Machine Getting Started","uri":"/dsvm/"},{"categories":["Azure"],"content":"“Management” Tab configuration ","date":"2020-09-09","objectID":"/dsvm/:4:4","tags":["Azure","Virtual Machine"],"title":"Azure Data Science Virtual Machine Getting Started","uri":"/dsvm/"},{"categories":["Azure"],"content":"“Advanced” Tab configuration ","date":"2020-09-09","objectID":"/dsvm/:4:5","tags":["Azure","Virtual Machine"],"title":"Azure Data Science Virtual Machine Getting Started","uri":"/dsvm/"},{"categories":["Azure"],"content":"“Tags” Tab configuration ","date":"2020-09-09","objectID":"/dsvm/:4:6","tags":["Azure","Virtual Machine"],"title":"Azure Data Science Virtual Machine Getting Started","uri":"/dsvm/"},{"categories":["Azure"],"content":"“Review + create” Tab configuration This step will be your last step. This step will vaild all of your configations. If failed, below image will be show. ","date":"2020-09-09","objectID":"/dsvm/:4:7","tags":["Azure","Virtual Machine"],"title":"Azure Data Science Virtual Machine Getting Started","uri":"/dsvm/"},{"categories":["virtual environment"],"content":"Introduction This post document is about using Conda to management Python and R language packages (libraries/dependencies), as well as virtual environment. Conda is an open source package management system and environment management system. Conda can quickly installs, runs and updates packages and their dependencies. Conda easily creates, saves, loads and switches between environments on your local computer. It was created for Python programs, but it can package and distribute software for any language. Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because conda is also an environment manager. With just a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run your usual version of Python in your normal environment. Anaconda is a distribution of conda. It is a data science platform that comes with a lot of packages.Unlike Anaconda, Miniconda doesn’t come with any installed packages by default. For this tutorial, I am using miniconda. If you have Anaconda, the commands will be the same! This also applies across operating systems (Except for the activation of environment). This document is base on geohackweek’s introduction to conda. At the end, there are two example for Python and R language. ","date":"2020-09-08","objectID":"/conda/:1:0","tags":["Machine Learning","Conda","Python"],"title":"Getting started with Python using Conda","uri":"/conda/"},{"categories":["virtual environment"],"content":"Conda ","date":"2020-09-08","objectID":"/conda/:2:0","tags":["Machine Learning","Conda","Python"],"title":"Getting started with Python using Conda","uri":"/conda/"},{"categories":["virtual environment"],"content":"Installation Installation details, please have a look the Official Installation guide There are brief steps at the below. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh ","date":"2020-09-08","objectID":"/conda/:2:1","tags":["Machine Learning","Conda","Python"],"title":"Getting started with Python using Conda","uri":"/conda/"},{"categories":["virtual environment"],"content":"Managing Conda Let’s first start by checking if conda is installed. conda --version Once it has been confirmed that conda has been installed, we will now make sure that it is up to date. conda update conda Conda will compare versions and let you know what is available to install. It will also tell you about other packages that will be automatically updated or changed with the update. If there are newer version available, follow the instruction to install the newest version of conda. TIPS: To see the full documentation for any command, type the command followed by –help. For example, to learn about the conda update command: conda update --help ","date":"2020-09-08","objectID":"/conda/:2:2","tags":["Machine Learning","Conda","Python"],"title":"Getting started with Python using Conda","uri":"/conda/"},{"categories":["virtual environment"],"content":"Managing Environments Using conda, you can create an isolated environment for your project. An environment is a set of packages that can be used in one or multiple projects. The default environment is the root environment, which contains default packages listed here. There are two ways of creating a conda environment. An environment file in YAML format (such as environment.yml). Manual specifications of packages. Creating environment with an environment file YAML stands for YAML Ain’t Markup Language. It is a human friendly data serialization standard for all programming languages. This is an example of an environment file that will install python 3.6, python-pip, and pyjokes library using pip. Conda is friendly with pip, so if some packages are not found in Anaconda Cloud, then you can install them with pip install. Open up your favorite text editor, copy and paste the code below, save your file as environment.yml. name: playenv channels: - conda-forge dependencies: - python=3.6 - pip - pip: - pyjokes Now, let’s install environment.yml environment file above so that we can create a conda environment called playenv. conda env create --file environment.yml TIPS: There is instruction for how to activate and deactivate environment. This command is slighty different between operating systems. Use an environment: Linux, OS X: source activate playenv `` Windows: activate playenv `` Deactivate an environment (goes back to root): Linux, OS X: source deactivate `` Windows: deactivate `` Creating environment by manually specifying packages We can create test_env conda environment by specifying the name, channel, and list of packages within the terminal window. In the example below, I am creating the test_env environment that uses python 2.7 and a list of libraries: numpy, matplotlib, pandas. conda create -c conda-forge -n test_env python=2.7 numpy matplotlib pandas Conda will solve any dependencies between the packages like before and create a new environment with those packages. Verifying current environment To know the current environment that you’re in you can either look at your terminal: (test_env) D-69-91-135-15:env_files lsetiawan$ The (test_env) in the beginning of the line indicates that I’m curently using the test_env conda environment. Another way that you can check for your current active environment is a command: foo@bar:~$ conda env list test_env * //anaconda/envs/test_env playenv //anaconda/envs/playenv root //anaconda The current environment is indicated by (*) character. This is also a great way to see the list of environments that have been created. In the list, the path to each environment is also shown. Sharing Environments with others To share an environment, you can export your conda environment to an environment file. By doing this, the resulting environment file is very detailed with specific version listing. Exporting your environment to a file called myenv.yml: conda env export -f test_env.yml -n test_env This will export a very detailed environment file that you can share with others. This file specifies the package=version=build. Note that this environment file will not work to share across platforms, since the builds and versions might be different for different operating systems. Best practice to share environments When starting a new environment, always generate it from an environment file rather than the command line. As you add packages to the environment, be sure to update the environment file. Unless you have to, try to avoid specifying the version of each package. This will ensure you have the most up to date version that will work across platform. If you follow these guidelines, you should be able to give your environment file to anyone, and they will be able to install your packages with no problem. Making an exact copy of an environment and deleting environments Copying an environment We can make an exact copy of an environment to an environment with a different name. This maybe useful for any test","date":"2020-09-08","objectID":"/conda/:2:3","tags":["Machine Learning","Conda","Python"],"title":"Getting started with Python using Conda","uri":"/conda/"},{"categories":["virtual environment"],"content":"Managing Packages Seeing what packages are available We will now check packages that are available to us. The command below will list all the packages in an environment, in this case test_env. The list will include versions of each package, the specific build, and the channel that the package was downloaded from. conda list is also useful to ensure that you have installed the packages that you desire. foo@bar:~$ conda list -n test_env ## packages in environment at //anaconda/envs/test_env: ## Using Anaconda Cloud api site https://api.anaconda.org blas 1.1 openblas conda-forge ca-certificates 2016.9.26 0 conda-forge certifi 2016.9.26 py27_0 conda-forge cycler 0.10.0 py27_0 conda-forge freetype 2.6.3 1 conda-forge functools32 3.2.3.2 py27_1 conda-forge libgfortran 3.0.0 0 conda-forge ... Searching for a certain package Some packages might not be available in conda, but are available in pypi. For example, we will search for rasterio within the anaconda cloud. It is not necessary to create an account with anaconda cloud, unless you’d like to contribute in the future when you are pro with conda. Anaconda Cloud and Trusted Sources Anaconda Cloud is a package management service that makes it easy to find, access, store and share public and private notebooks, environments, and conda and PyPI packages, and to keep up with updates made to the packages and environments you’re using (Ref. Anaconda Cloud Doc). Anaconda Cloud is made up of channels/owners. Each channels contains one or more conda packages. It is important to be careful when downloading any packages from an untrusted source. Conda forge is a reliable source for many popular python packages. It is wise to research about the source of a conda package. In this example, we will use rasterio from conda-forge. The anaconda cloud page for rasterio will show how to install the package, compatible OS, individual files for that package, etc. If you are using anaconda, you can do this search within the command line: foo@bar:~$ anaconda search rasterio Using Anaconda Cloud api site https://api.anaconda.org Run 'anaconda show \u003cUSER/PACKAGE\u003e' to get more details: Packages: Name | Version | Package Types | Platforms ------------------------- | ------ | --------------- | --------------- IOOS/rasterio | 1.0a2 | conda | linux-64, win-32, win-64, osx-64 Terradue/rasterio | 0.32.0 | conda | linux-64 : Fast and direct raster I/O for use with Numpy and SciPy anaconda/rasterio | 0.36.0 | conda | linux-64, win-32, win-64, linux-32, osx-64 conda-forge/rasterio | 1.0a2 | conda | linux-64, win-32, win-64, osx-64 : Rasterio reads and writes geospatial raster datasets dharhas/rasterio | 0.23.0 | conda | win-64 : Rasterio reads and writes geospatial raster datasets. erdc/rasterio | 0.23.0 | conda | win-64 : Rasterio reads and writes geospatial raster datasets. jesserobertson/rasterio | 0.23.0 | conda | linux-64, linux-32, osx-64 jhamman/rasterio_to_xarray | 2016.03.16-1558 | ipynb | : IPython notebook krisvanneste/rasterio | 0.26.0 | conda | win-64 ocefpaf/rasterio | 0.19.1 | conda | linux-64, osx-64 omgarcia/rasterio | 0.25.0 | conda | linux-64 pypi/rasterio | 0.13.2 | pypi | : Fast and direct raster I/O for Python programmers who use Numpy robintw/rasterio | 0.35.1 | conda | osx-64 : Rasterio reads and writes geospatial raster datasets sgillies/rasterio | 0.15 | conda | osx-64 ztessler/rasterio | 0.31.0 | conda | osx-64 : Fast and direct raster I/O for use with Numpy and SciPy Found 15 packages Installing conda package Under the name column of the result in the terminal or the package column in the Anaconda Cloud listing, shows the necessary information to install the package. Ex. conda-forge/rasterio. The first word list the channel that this package is from and the second part shows the name of the package. To install the latest version available within the channel, do not specify in the install command. We will install version 0.35 of rasterio from conda-forge into test_env in this example. Conda wi","date":"2020-09-08","objectID":"/conda/:2:4","tags":["Machine Learning","Conda","Python"],"title":"Getting started with Python using Conda","uri":"/conda/"},{"categories":["virtual environment"],"content":"Python There is a example for using conda install Jupyter Notebooks and set Jupyter environment. conda create -n myenv python=3.7.7 conda activate myenv conda install notebook ipykernel ipython kernel install --user --name myenv --display-name \"Python (myenv)\" pip install azureml-sdk[notebooks,automl] ","date":"2020-09-08","objectID":"/conda/:3:0","tags":["Machine Learning","Conda","Python"],"title":"Getting started with Python using Conda","uri":"/conda/"},{"categories":["virtual environment"],"content":"R ","date":"2020-09-08","objectID":"/conda/:4:0","tags":["Machine Learning","Conda","Python"],"title":"Getting started with Python using Conda","uri":"/conda/"},{"categories":["virtual environment"],"content":"Conclusion ","date":"2020-09-08","objectID":"/conda/:5:0","tags":["Machine Learning","Conda","Python"],"title":"Getting started with Python using Conda","uri":"/conda/"},{"categories":["Networking"],"content":"Introduction This post document is about how to set-up SSH server base on Ubuntu OS. A lots of users have some concerns about SSH security, especially when multi-user or multi-group share to use with same SSH tunnel. So, this article will fouce on SSH server security practices with multi-user. In this article, first I will talk Linux User and Group Management. TIPS: When you using SSH connect with a host, CLI Text Editor is importance for editing some of config file. I recommand you using VIM, if you don’t know how to use VIM, please have a look {% post_link vim %} ","date":"2020-09-08","objectID":"/ssh/:1:0","tags":["Networking","SSH"],"title":"SSH server set-up with multi-user and Security Practices","uri":"/ssh/"},{"categories":["Networking"],"content":"User Scenario Multi-user connect to cloud Virtual Machine(VM) using SSH is the most used scenario. Using SSH configure Edge Devices ","date":"2020-09-08","objectID":"/ssh/:2:0","tags":["Networking","SSH"],"title":"SSH server set-up with multi-user and Security Practices","uri":"/ssh/"},{"categories":["Networking"],"content":"Linux user and group management Since Linux is a multi-user operating system, several people may be logged in and actively working on a given machine at the same time. Security-wise, it is never a good idea to allow users to share the credentials of the same account. In fact, best practices dictate the use of as many user accounts as people needing access to the machine. At the same time, it is to be expected that two or more users may need to share access to certain system resources, such as directories and files. User and group management in Linux allows us to accomplish both objectives. ","date":"2020-09-08","objectID":"/ssh/:3:0","tags":["Networking","SSH"],"title":"SSH server set-up with multi-user and Security Practices","uri":"/ssh/"},{"categories":["Azure"],"content":"Introduction This post document is a brief introduction about Azure Machine Learning Service Compute Instance(CI). CI can be as your development environment or training compute tartget. TIPS: However, if your specialty on Linux, I recommand you using DSVM ({% post_link DSVM %}) as your development environment. For training process, I recommand you using Azure Machine Learning training cluster(Compute cluster). ","date":"2020-09-07","objectID":"/amlci/:1:0","tags":["Azure","Compute instance","Azure Machine Learning"],"title":"Azure Machine Learning Compute Instance Getting Started","uri":"/amlci/"},{"categories":["Azure"],"content":"Managing a compute instance ","date":"2020-09-07","objectID":"/amlci/:2:0","tags":["Azure","Compute instance","Azure Machine Learning"],"title":"Azure Machine Learning Compute Instance Getting Started","uri":"/amlci/"},{"categories":["Azure"],"content":"From portal In your workspace in Azure Machine Learning studio, select Compute, then select Compute Instance on the top. You can perform the following actions: Create a compute instance. Refresh the compute instances tab. Start, stop, and restart a compute instance. You do pay for the instance whenever it is running. Stop the compute instance when you are not using it to reduce cost. Stopping a compute instance deallocates it. Then start it again when you need it. Delete a compute instance. Filter the list of compute instanced to show only those you have created. For each compute instance in your workspace that you can use, you can: Access Jupyter, JupyterLab, RStudio on the compute instance SSH into compute instance. SSH access is disabled by default but can be enabled at compute instance creation time. SSH access is through public/private key mechanism. The tab will give you details for SSH connection such as IP address, username, and port number. Get details about a specific compute instance such as IP address, and region. TIPS: RBAC allows you to control which users in the workspace can create, delete, start, stop, restart a compute instance. All users in the workspace contributor and owner role can create, delete, start, stop, and restart compute instances across the workspace. However, only the creator of a specific compute instance, or the user assigned if it was created on their behalf, is allowed to access Jupyter, JupyterLab, and RStudio on that compute instance. A compute instance is dedicated to a single user who has root access, and can terminal in through Jupyter/JupyterLab/RStudio. Compute instance will have single-user log in and all actions will use that user’s identity for RBAC and attribution of experiment runs. SSH access is controlled through public/private key mechanism. These actions can be controlled by RBAC: Microsoft.MachineLearningServices/workspaces/computes/read Microsoft.MachineLearningServices/workspaces/computes/write Microsoft.MachineLearningServices/workspaces/computes/delete Microsoft.MachineLearningServices/workspaces/computes/start/action Microsoft.MachineLearningServices/workspaces/computes/stop/action Microsoft.MachineLearningServices/workspaces/computes/restart/action Details please check official website Managing CI ","date":"2020-09-07","objectID":"/amlci/:2:1","tags":["Azure","Compute instance","Azure Machine Learning"],"title":"Azure Machine Learning Compute Instance Getting Started","uri":"/amlci/"},{"categories":["Azure"],"content":"Using Azure Machine Learning SDK All of management CI methods can be found in ComputeInstance class. Details please check official link at the below. Managing CI SDK Delete CI object methods Official link Remove the ComputeInstance object from its associated workspace. delete(wait_for_completion=False, show_output=False) Parameters wait_for_completion - default value: False show_output - default value: False Exceptions Remarks If this object was created through Azure ML, the corresponding cloud based objects will also be deleted. If this object was created externally and only attached to the workspace, it will raise exception and nothing will be changed. ","date":"2020-09-07","objectID":"/amlci/:2:2","tags":["Azure","Compute instance","Azure Machine Learning"],"title":"Azure Machine Learning Compute Instance Getting Started","uri":"/amlci/"},{"categories":["Azure"],"content":"REST API REST API also can be used to manage CIs, How to delete CIs, please, have a check the link at the below. Delete CIs ","date":"2020-09-07","objectID":"/amlci/:2:3","tags":["Azure","Compute instance","Azure Machine Learning"],"title":"Azure Machine Learning Compute Instance Getting Started","uri":"/amlci/"},{"categories":["Machine Learning"],"content":"Introduction Regardless of the algorithm and framework used to train the neural network, the amount of image data is always an important prerequisite for determining the quality of the training model. Data expansion is a common method for training models, which is very important for the robustness and accuracy of the model. This post document will use OpenCV to show some of tips for how we can extented image training set. ","date":"2020-09-07","objectID":"/extentedimagetrainingset/:1:0","tags":["Machine Learning","Extented Image Training Set","OpenCV"],"title":"Extented Image Training Set Tips","uri":"/extentedimagetrainingset/"},{"categories":["Machine Learning"],"content":"Random cropping of images import cv2 import random img = cv2.imread(\"lena.jpg\") width, height, depth = img.shape img_width_box = width * 0.2 img_width_box = height * 0.2 for _ in range(9): start_pointX = int(random.uniform(0, img_width_box)) start_pointY = int(random.uniform(0, img_height_box)) copyImg = img[start_pointX:200, start_pointY:200] cv2.imshow (\"test\", copyImg) cv2.waitKey(0) ","date":"2020-09-07","objectID":"/extentedimagetrainingset/:2:0","tags":["Machine Learning","Extented Image Training Set","OpenCV"],"title":"Extented Image Training Set Tips","uri":"/extentedimagetrainingset/"},{"categories":["editor"],"content":"Introduction This post document is brief introduction about how to use VIM/VI text editor and help you remember the shortcuts. The Vim editor is a command-line based tool that’s an enhanced version of the venerable vi editor. Despite the abundance of graphical rich text editors, familiarity with Vim will help every Linux user – from an experienced system administrator to a newbie Raspberry Pi user. One important thing to note when using Vim, is that the function of a key depends on the “mode” the editor is in. For example, pressing the alphabet “j” will move the cursor down one line in the “command mode”. You’ll have to switch to the “insert mode” to make the keys input the character they represent. TIPS: I recommend you using Spacemacs, which is a new way to experience Emacs, you can use VI’s editing styles with emacs' extendibility. ","date":"2020-09-07","objectID":"/vim/:1:0","tags":["vim"],"title":"VIM Keyboard Shortcuts Cheatsheet","uri":"/vim/"},{"categories":["editor"],"content":"User Scenario Most of Embedded system have not GUI avabliable. Nowadays, edge computing become poplar, using command-line text editor can be inportance for configure Edge Devices. Cloud Virual Machine(VM) - marjor of VM only support using SSH login, when you login the cloud VM, CLI will be the only way. ","date":"2020-09-07","objectID":"/vim/:2:0","tags":["vim"],"title":"VIM Keyboard Shortcuts Cheatsheet","uri":"/vim/"},{"categories":["editor"],"content":"Cheatsheet Here’s a cheatsheet to help you get the most out of Vim. Shortcut Keys Function Main (Change Mode) Escape key Gets out of the current mode into the “command mode”. All keys are bound of commands. i “Insert mode” for inserting text. Keys behave as expected. : “Last-line mode” where Vim expects you to enter a command such as to save the document. v Enter “visual mode” Navigation keys (used in command mode) h moves the cursor one character to the left j or Ctrl + J moves the cursor down one line k or Ctrl + P moves the cursor up one line l moves the cursor one character to the right 0 moves the cursor to the beginning of the line $ moves the cursor to the end of the line ^ moves the cursor to the first non-empty character of the line w move forward one word (next alphanumeric word) W move forward one word (delimited by a white space) 5w move forward five words b move backward one word (previous alphanumeric word) B move backward one word (delimited by a white space) 5b move backward five words G move to the end of the file gg move to the beginning of the file Navigate around the document ( jumps to the previous sentence ) jumps to the next sentence { jumps to the previous paragraph } jumps to the next paragraph [[ jumps to the previous section ]] jumps to the next section [] jump to the end of the previous section ][ jump to the end of the next section Insert text a Insert text after the cursor A Insert text at the end of the line i Insert text before the cursor o Begin a new line below the cursor O Begin a new line above the cursor Special inserts :r [filename] Insert the file [filename] below the cursor :r ![command] Execute [command] and insert its output below the cursor Delete text x delete character at cursor dw delete a word d0 delete to the beginning of a line d$ delete to the end of a line d) delete to the end of sentence dgg delete to the beginning of the file dG delete to the end of the file dd delete line 3dd delete three lines Simple replace text r{text} Replace the character under the cursor with {text} R Replace characters instead of inserting them Copy/Paste text yy copy current line into storage buffer [“x]yy Copy the current lines into register x p paste storage buffer after current line P paste storage buffer before current line [“x]p paste from register x after current line [“x]P paste from register x before current line Undo/Redo operation u undo the last operation Ctrl+r redo the last undo Search and Replace keys /search_text search document for search_text going forward ?search_text search document for search_text going backward n move to the next instance of the result from the search N move to the previous instance of the result :%s/original/replacement Search for the first occurrence of the string “original” and replace it with “replacement” :%s/original/replacement/g Search and replace all occurrences of the string “original” with “replacement” :%s/original/replacement/gc Search for all occurrences of the string “original” but ask for confirmation before replacing them with “replacement” Bookmarks m {a-z A-Z} Set bookmark {a-z A-Z} at the current cursor position :marks List all bookmarks `{a-z A-Z} Jumps to the bookmark {a-z A-Z} Select text v Enter visual mode per character V Enter visual mode per line Esc Exit visual mode Modify selected text (used in visual mode) ~ Switch case d delete a word c change y yank \u003e shift right \u003c shift left ! filter through an external command Save and quit :q Quits Vim but fails when file has been changed :w Save the file :w new_name Save the file with the new_name filename :wq Save the file and quit Vim :q! Quit Vim without saving the changes to the file ZZ Write file, if modified, and quit Vim ZQ Same as :q! Quits Vim without writing changes ","date":"2020-09-07","objectID":"/vim/:3:0","tags":["vim"],"title":"VIM Keyboard Shortcuts Cheatsheet","uri":"/vim/"},{"categories":["Networking"],"content":"What is frp? frp is a fast reverse proxy to help you expose a local server behind a NAT or firewall to the Internet. As of now, it supports TCP and UDP, as well as HTTP and HTTPS protocols, where requests can be forwarded to internal services by domain name. frp also has a P2P connect mode. ","date":"2020-09-06","objectID":"/frp/:1:0","tags":["Networking"],"title":"Getting Started with Reverse Proxy","uri":"/frp/"},{"categories":["Networking"],"content":"Architecture architecture ","date":"2020-09-06","objectID":"/frp/:2:0","tags":["Networking"],"title":"Getting Started with Reverse Proxy","uri":"/frp/"},{"categories":["Networking"],"content":"Installation Download the latest programs from Release page according to your operating system and architecture. There is a example: yanboyang713@boyang$ wget https://github.com/fatedier/frp/releases/download/v0.34.0/frp_0.34.0_linux_amd64.tar.gz yanboyang713@boyang$ tar -xzf frp_0.34.0_linux_amd64.tar.gz yanboyang713@boyang$ cd frp_0.34.0_linux_amd64 yanboyang713@boyang$ ls frpc frpc_full.ini frpc.ini frps frps_full.ini frps.ini LICENSE systemd Put frps and frps.ini onto your server A (usually is cloud VM) with public IP. There is a example: yanboyang713@boyang:~/Downloads/frp_0.34.0_linux_amd64$ sftp boyyan@52.141.58.83 boyyan@52.141.58.83's password: Connected to 52.141.58.83. sftp\u003e ls sftp\u003e lls frpc frpc_full.ini frpc.ini frps frps_full.ini frps.ini LICENSE systemd sftp\u003e put frps.ini Uploading frps to /home/boyyan/frps.ini frps.ini sftp\u003e put frps Uploading frps to /home/boyyan/frps frps 100% 13MB 7.4MB/s 00:01 sftp\u003e ls frps frps.ini sftp\u003e exit Put frpc and frpc.ini onto your server B in LAN (that can’t be connected from public Internet). ","date":"2020-09-06","objectID":"/frp/:3:0","tags":["Networking"],"title":"Getting Started with Reverse Proxy","uri":"/frp/"},{"categories":["Networking"],"content":"Access your computer in LAN by SSH Modify frps.ini on server A and set the bind_port to be connected to frp clients: [common] bind_port = 7000 Start frps on server A: ./frps -c ./frps.ini On server B, modify frpc.ini to put in your frps server public IP as server_addr field: [common] server_addr = 20.194.22.192 server_port = 7000 [ssh] type = tcp local_ip = 127.0.0.1 local_port = 22 remote_port = 6000 Note that local_port (listened on client) and remote_port (exposed on server) are for traffic goes in/out the frp system, whereas server_port is used between frps. Start frpc on server B: ./frpc -c ./frpc.ini From another machine, SSH to server B like this (assuming that username is test): ssh -oPort=6000 test@x.x.x.x ","date":"2020-09-06","objectID":"/frp/:4:0","tags":["Networking"],"title":"Getting Started with Reverse Proxy","uri":"/frp/"},{"categories":["Linux"],"content":"Hardware Information There are plenty of commands to check information about the hardware of Linux system. Some commands report only specific hardware components like CPU or memory while the rest cover multiple hardware units. This section will look at some of the most commonly used commands to check information and configuration details about various hardware peripherals and devices. The list includes lscpu, hwinfo, lshw, dmidecode, lspci etc. ","date":"2020-09-06","objectID":"/linux/:1:0","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"lscpu The lscpu command reports information about the cpu and processing units. It does not have any further options or functionality. $ lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 8 On-line CPU(s) list: 0-7 Thread(s) per core: 1 Core(s) per socket: 8 Socket(s): 1 NUMA node(s): 1 Vendor ID: GenuineIntel CPU family: 6 Model: 158 Model name: Intel(R) Core(TM) i7-9700F CPU @ 3.00GHz Stepping: 13 CPU MHz: 3710.930 CPU max MHz: 4700.0000 CPU min MHz: 800.0000 BogoMIPS: 6000.00 Virtualization: VT-x L1d cache: 32K L1i cache: 32K L2 cache: 256K L3 cache: 12288K NUMA node0 CPU(s): 0-7 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d arch_capabilities ","date":"2020-09-06","objectID":"/linux/:1:1","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"lshw - List Hardware A general purpose utility, that reports detailed and brief information about multiple different hardware units such as cpu, memory, disk, usb controllers, network adapters etc. Lshw extracts the information from different /proc files. Display Full Information Running lshw without any options would generate full information report about all detected hardware. It would generate a big output with quite a lot of technical details. sudo lshw Display information in short With the “-short” the lshw command would generate a brief information report about the hardware devices that would quickly give an idea about the hardware profile of the system. yanboyang713@boyang$ sudo lshw -short H/W path Device Class Description ===================================================== system MS-7C22 (Default string) /0 bus Z390 PLUS (MS-7C22) /0/0 memory 64KiB BIOS /0/39 memory 16GiB System Memory /0/39/0 memory [empty] /0/39/1 memory 8GiB DIMM DDR4 Synchronous 2667 MHz (0.4 ns) /0/39/2 memory [empty] /0/39/3 memory 8GiB DIMM DDR4 Synchronous 2667 MHz (0.4 ns) /0/43 memory 512KiB L1 cache /0/44 memory 2MiB L2 cache /0/45 memory 12MiB L3 cache /0/46 processor Intel(R) Core(TM) i7-9700F CPU @ 3.00GHz /0/100 bridge Intel Corporation /0/100/1 bridge Xeon E3-1200 v5/E3-1500 v5/6th Gen Core Processor PCIe Cont /0/100/1/0 display NVIDIA Corporation /0/100/1/0.1 multimedia NVIDIA Corporation /0/100/1/0.2 bus NVIDIA Corporation /0/100/1/0.2/0 usb3 bus xHCI Host Controller /0/100/1/0.2/1 usb4 bus xHCI Host Controller /0/100/1/0.3 bus NVIDIA Corporation /0/100/8 generic Xeon E3-1200 v5/v6 / E3-1500 v5 / 6th/7th Gen Core Processo /0/100/12 generic Cannon Lake PCH Thermal Controller /0/100/14 bus Cannon Lake PCH USB 3.1 xHCI Host Controller /0/100/14/0 usb1 bus xHCI Host Controller /0/100/14/0/2 input HP Basic USB Keyboard /0/100/14/0/8 input 2.4G Mouse /0/100/14/1 usb2 bus xHCI Host Controller /0/100/14.2 memory RAM memory /0/100/16 communication Cannon Lake PCH HECI Controller /0/100/17 storage Cannon Lake PCH SATA AHCI Controller /0/100/1b bridge Intel Corporation /0/100/1b.4 bridge Cannon Lake PCH PCI Express Root Port 21 /0/100/1b.4/0 enp3s0f0 network I350 Gigabit Network Connection /0/100/1b.4/0.1 enp3s0f1 network I350 Gigabit Network Connection /0/100/1b.4/0.2 enp3s0f2 network I350 Gigabit Network Connection /0/100/1b.4/0.3 enp3s0f3 network I350 Gigabit Network Connection /0/100/1c bridge Intel Corporation /0/100/1c.4 bridge Intel Corporation /0/100/1c.4/0 enp5s0 network RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller /0/100/1d bridge Cannon Lake PCH PCI Express Root Port 9 /0/100/1d/0 storage Micron/Crucial Technology /0/100/1f bridge Intel Corporation /0/100/1f.3 multimedia Cannon Lake PCH cAVS /0/100/1f.4 bus Cannon Lake PCH SMBus Controller /0/100/1f.5 bus Cannon Lake PCH SPI Controller /1 power To Be Filled By O.E.M. /2 docker0 network Ethernet interface /3 br0 network Ethernet interface Display only memory information To display information about the memory, specify the memory class. yanboyang713@boyang $ sudo lshw -short -class memory H/W path Device Class Description ===================================================== /0/0 memory 64KiB BIOS /0/39 memory 16GiB System Memory /0/39/0 memory [empty] /0/39/1 memory 8GiB DIMM DDR4 Synchronous 2667 MHz (0.4 ns) /0/39/2 memory [empty] /0/39/3 memory 8GiB DIMM DDR4 Synchronous 2667 MHz (0.4 ns) /0/43 memory 512KiB L1 cache /0/44 memory 2MiB L2 cache /0/45 memory 12MiB L3 cache /0/100/14.2 memory RAM memory Display processor information With class processor, lshw would display information about the cpu. It is better to not use the short option and get full details about the processor. yanboyang713@boyang$ sudo lshw -class processor *-cpu description: CPU product: Intel(R) Core(TM) i7-9700F CPU @ 3.00GHz vendor: Intel Corp. physical id: 46 bus info: cpu@0 version: Intel(R) Core(TM) i7-9700F CPU @ 3.00GHz serial: To Be Filled By O.E.M. slot: U3E1 size: 4631MHz capacity: 4700M","date":"2020-09-06","objectID":"/linux/:1:2","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"lspci - List PCI The lspci command lists out all the pci buses and details about the devices connected to them. The vga adapter, graphics card, network adapter, usb ports, sata controllers, etc all fall under this category. yanboyang713@boyang$ lspci 00:00.0 Host bridge: Intel Corporation Device 3e30 (rev 0d) 00:01.0 PCI bridge: Intel Corporation Xeon E3-1200 v5/E3-1500 v5/6th Gen Core Processor PCIe Controller (x16) (rev 0d) 00:08.0 System peripheral: Intel Corporation Xeon E3-1200 v5/v6 / E3-1500 v5 / 6th/7th Gen Core Processor Gaussian Mixture Model 00:12.0 Signal processing controller: Intel Corporation Cannon Lake PCH Thermal Controller (rev 10) 00:14.0 USB controller: Intel Corporation Cannon Lake PCH USB 3.1 xHCI Host Controller (rev 10) 00:14.2 RAM memory: Intel Corporation Cannon Lake PCH Shared SRAM (rev 10) 00:16.0 Communication controller: Intel Corporation Cannon Lake PCH HECI Controller (rev 10) 00:17.0 SATA controller: Intel Corporation Cannon Lake PCH SATA AHCI Controller (rev 10) 00:1b.0 PCI bridge: Intel Corporation Device a340 (rev f0) 00:1b.4 PCI bridge: Intel Corporation Cannon Lake PCH PCI Express Root Port 21 (rev f0) 00:1c.0 PCI bridge: Intel Corporation Device a338 (rev f0) 00:1c.4 PCI bridge: Intel Corporation Device a33c (rev f0) 00:1d.0 PCI bridge: Intel Corporation Cannon Lake PCH PCI Express Root Port 9 (rev f0) 00:1f.0 ISA bridge: Intel Corporation Device a305 (rev 10) 00:1f.3 Audio device: Intel Corporation Cannon Lake PCH cAVS (rev 10) 00:1f.4 SMBus: Intel Corporation Cannon Lake PCH SMBus Controller (rev 10) 00:1f.5 Serial bus controller [0c80]: Intel Corporation Cannon Lake PCH SPI Controller (rev 10) 01:00.0 VGA compatible controller: NVIDIA Corporation Device 1f02 (rev a1) 01:00.1 Audio device: NVIDIA Corporation Device 10f9 (rev a1) 01:00.2 USB controller: NVIDIA Corporation Device 1ada (rev a1) 01:00.3 Serial bus controller [0c80]: NVIDIA Corporation Device 1adb (rev a1) 03:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01) 03:00.1 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01) 03:00.2 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01) 03:00.3 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01) 05:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller (rev 15) 06:00.0 Non-Volatile memory controller: Micron/Crucial Technology Device 2263 (rev 03) Filter out specific device information with grep. $ lspci -v | grep \"VGA\" -A 12 ","date":"2020-09-06","objectID":"/linux/:1:3","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"lsscsi - List scsi devices Lists out the scsi/sata devices like hard drives and optical drives. sudo apt install lsscsi ","date":"2020-09-06","objectID":"/linux/:1:4","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"lsusb - List usb buses and device details This command shows the USB controllers and details about devices connected to them. By default brief information is printed. Use the verbose option “-v” to print detailed information about each usb port $ lsusb On the above system, 1 usb port is being used by the mouse. ","date":"2020-09-06","objectID":"/linux/:1:5","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"Inxi Inxi is a 10K line mega bash script that fetches hardware details from multiple different sources and commands on the system, and generates a beautiful looking report that non technical users can read easily. $ inxi -Fx ","date":"2020-09-06","objectID":"/linux/:1:6","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"lsblk - List block devices List out information all block devices, which are the hard drive partitions and other storage devices like optical drives and flash drives $ lsblk ","date":"2020-09-06","objectID":"/linux/:1:7","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"df - disk space of file systems Reports various partitions, their mount points and the used and available space on each. $ df -H ","date":"2020-09-06","objectID":"/linux/:1:8","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"Pydf - Python df An improved df version written in python, that displays colored output that looks better than df $ pydf ","date":"2020-09-06","objectID":"/linux/:1:9","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"fdisk Fdisk is a utility to modify partitions on hard drives, and can be used to list out the partition information as well. $ sudo fdisk -l ","date":"2020-09-06","objectID":"/linux/:1:10","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"mount The mount is used to mount/unmount and view mounted file systems. $ mount | column -t Again, use grep to filter out only those file systems that you want to see $ mount | column -t | grep ext ","date":"2020-09-06","objectID":"/linux/:1:11","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"free - Check RAM Check the amount of used, free and total amount of RAM on system with the free command. $ free -m ","date":"2020-09-06","objectID":"/linux/:1:12","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"dmidecode The dmidecode command is different from all other commands. It extracts hardware information by reading data from the SMBOIS data structures (also called DMI tables). ## display information about the processor/cpu $ sudo dmidecode -t processor ## memory/ram information $ sudo dmidecode -t memory ## bios details $ sudo dmidecode -t bios Check out the man page for more details. ","date":"2020-09-06","objectID":"/linux/:1:13","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"/proc files Many of the virtual files in the /proc directory contain information about hardware and configurations. Here are some of them CPU/Memory information ## cpu information $ cat /proc/cpuinfo ## memory information $ cat /proc/meminfo Linux/kernel information $ cat /proc/version Linux version 3.11.0-12-generic (buildd@allspice) (gcc version 4.8.1 (Ubuntu/Linaro 4.8.1-10ubuntu7) ) #19-Ubuntu SMP Wed Oct 9 16:20:46 UTC 2013 SCSI/Sata devices $ cat /proc/scsi/scsi Partitions $ cat /proc/partitions ","date":"2020-09-06","objectID":"/linux/:2:0","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Linux"],"content":"hdparm The hdparm command gets information about sata devices like hard disks. $ sudo hdparm -i /dev/sda ","date":"2020-09-06","objectID":"/linux/:2:1","tags":["Linux command"],"title":"linux basic commands and common tools","uri":"/linux/"},{"categories":["Azure"],"content":"Could you try using config.json file connect to your Azure ML Workspace first? The steps you can following at the below. Go to https://portal.azure.com/ Download config.json config.json Story config.json to the same directory as your Python code. run python code at the below from azureml.core import Workspace ws = Workspace.from_config() print(\"Found workspace {} at location {}\".format(ws.name, ws.location))  If using config.json file connect to your Azure ML Workspace does not work, please check your SDK version and update your SDK. Check SDK version python code: import azureml.core print(azureml.core.VERSION) Update SDK version - Upgrade a previous version, make sure you upgrade all the dependencies as well: pip install --upgrade --upgrade-strategy eager azureml-sdk If Interactive Authentication does not work, please using Azure CLI Authentication or Service Principal Authentication. Details at the below. https://notebooks.azure.com/azureml/projects/azureml-getting-started/html/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azure-ml.ipynb ","date":"2020-09-05","objectID":"/amlworkspaceauthentication/:0:0","tags":["Azure","Machine Learning"],"title":"Azure Machine Learning Workspace Authentication Troubleshooting Guide","uri":"/amlworkspaceauthentication/"},{"categories":["Patent"],"content":"Introduction ","date":"2020-09-05","objectID":"/patent/:1:0","tags":["Patent"],"title":"Intellectual Property Foundation and Patent Mining","uri":"/patent/"},{"categories":["Patent"],"content":"Scope of Intellectual Property Intellectual property Also known as “intellectual property rights”, it refers to the broad fields of industry, technology, science and art The exclusive legal rights involved in intellectual creation, invention and innovation. Intellectual property rights include: Copyright (copyright), trademark, patent, Trade secrets, new plant varieties, semiconductor integrated circuit design, trade name (company name) ","date":"2020-09-05","objectID":"/patent/:2:0","tags":["Patent"],"title":"Intellectual Property Foundation and Patent Mining","uri":"/patent/"},{"categories":["Patent"],"content":"Patent Type Total have three type of patent Invention New technical solutions for products, methods or their combination. Products: articles, devices, equipment, tools, materials, etc. Method: manufacturing method, use method, communication method, processing method, etc. (Pure intellectual activity rules and methods cannot be granted patent rights.) Utility model New practical technical solutions proposed for the shape, structure or combination of the product. (It can be directly observed from the definite spatial shape; it solves the technical problem, not just for more beautiful appearance.) Such as: the shape of the cup, the relative position relationship of the parts and the mechanical cooperation relationship, the relationship between the components that constitute the circuit Connection relationship, etc. Appearance design A new design that is aesthetically pleasing and suitable for industrial applications based on the shape, pattern or combination of the product and the combination of color, shape and pattern. Such as: the style of clothes, the outline of the product, the graphics on the surface, etc. ","date":"2020-09-05","objectID":"/patent/:3:0","tags":["Patent"],"title":"Intellectual Property Foundation and Patent Mining","uri":"/patent/"},{"categories":["Patent"],"content":"Patent Characteristics Exclusivity The patent right for invention creation is unique, monopoly and exclusive. Timeliness Term of protection: 20 years for invention, 10 years for utility model, and 10 years for appearance. Regional The patent laws of each country are independent, and the patent right is only valid in the ratifying country. ","date":"2020-09-05","objectID":"/patent/:4:0","tags":["Patent"],"title":"Intellectual Property Foundation and Patent Mining","uri":"/patent/"},{"categories":["Patent"],"content":"Legal conditions for the establishment of a patent ","date":"2020-09-05","objectID":"/patent/:5:0","tags":["Patent"],"title":"Intellectual Property Foundation and Patent Mining","uri":"/patent/"},{"categories":["Networking"],"content":"Introduction This post document is about how to build a Linux gateway base on Ubuntu OS. The gateway connects an internal network to an external network. Basically, performing Network Address Translation (NAT) for hosts on the internal network. It is exceptionally similar to what your home router does. ","date":"2020-09-04","objectID":"/ubunturouter/:1:0","tags":["Networking"],"title":"Setting up an ubuntu Router with bridge","uri":"/ubunturouter/"},{"categories":["Networking"],"content":"User Scenario Acting as a router at home or in office as a local area network. Connecting multi-network cameras to Ubuntu Server for transfer image data. ","date":"2020-09-04","objectID":"/ubunturouter/:2:0","tags":["Networking"],"title":"Setting up an ubuntu Router with bridge","uri":"/ubunturouter/"},{"categories":["Networking"],"content":"Terminology interface is used to mean the operating system’s name for a place which sends or receives data packets. It is often, but not necessarily, the same as a device. An interface may have several devices associated (e.g. a bridge), or a single device may have several interfaces. device will refer here to the bit of hardware dealing with your network connections. A network adapter is the component of a computer’s internal hardware that is used for communicating over a network with another computer. It enable a computer to connect with another computer, server or any networking device over an LAN connection. A network adapter can be used over a wired or wireless network. A gateway is a piece of networking hardware used in telecommunications for telecommunications networks that allows data to flow from one discrete network to another. Gateways are distinct from routers or switches in that they communicate using more than one protocol to connect a bunch of networks and can operate at any of the seven layers of the open systems interconnection model (OSI). ","date":"2020-09-04","objectID":"/ubunturouter/:3:0","tags":["Networking"],"title":"Setting up an ubuntu Router with bridge","uri":"/ubunturouter/"},{"categories":["Networking"],"content":"Hardware Requirments In our scenario, we need one multi-port network adapters with your desktop PC at the less. One port for connect with external network. Others for connect internal network. Because, we have muti-devices need to connect to internal network, So I recommand, we can using monther-board wired adapter connect with external network and one multi-port PCIE network card connect with internal network. All of PCIE network card can be connect to a bridge. The details how to set-up, I will talk at the below. Tips, there are two type of PCIE adapters can be use in our project. First type have POE (Power over Ethernet) support and another type does not support POE. I recommend you buy a PoE support card because when you want to connect a network camera or access point (AP), which can be power by PCIE card directly, you need not unnecessary power cable and power supply to power cameras or APs, this will convience for you. 4 port Gigabit POE network card about 120 US dollars. ","date":"2020-09-04","objectID":"/ubunturouter/:4:0","tags":["Networking"],"title":"Setting up an ubuntu Router with bridge","uri":"/ubunturouter/"},{"categories":["Networking"],"content":"Settings ","date":"2020-09-04","objectID":"/ubunturouter/:5:0","tags":["Networking"],"title":"Setting up an ubuntu Router with bridge","uri":"/ubunturouter/"},{"categories":["Networking"],"content":"Configuring the bridge and Network file settings Linux supports the implementation of a software network bridge to reproduce the function of a network bridge, a networking device that interconnects two or more communication networks or network segments providing a way for them to work as a single network. It acts almost like a network switch, and in a software sense, it is used to implement the concept of a “virtual network switch”. In our scenario, we will use software network bridging to connect all of ports for PCIE network card directly to the host server network. If you want to you also can add your virtual machines(VMs) to this bridge. This way, the all of PCIE ports are deployed on the same subnet as the host and can access services such as DHCP and much more. Installing Network Bridge Utilities Begin by installing the bridge-utils package which contains utilities for configuring the Ubuntu ethernet bridge using the apt package manager as shown. sudo apt-get update sudo apt-get install bridge-utils Distinguish External and Internal Interfaces' Name Identify the interface name for your ethernet device using the IP command as shown. $ ip ad OR $ ip add From this screenshot, as you can see: enp5s0 interface is my monther-board wired adapter(only have one port) connect with external network enp3s0f0 interface, enp3s0f1 interface, enp3s0f2 interface and enp3s0f3 interface are my PCIE network card (4 ports) br0 is my bridge, which connect with all of PCIE ports. The bridge will as internal network. This interface have not yet. Please following the next creating the bridge section. Creating and Setting the Bridge at Start-up We need to edit the /etc/network/interfaces file. This file shows an example of a bridge configure. Sample /etc/network/interfaces file auto lo iface lo inet loopback ##External Interface auto enp5s0 iface enp5s0 inet dhcp ##bridge auto br0 iface br0 inet static address 192.168.1.1 network 192.168.1.0 netmask 255.255.255.0 broadcast 192.168.1.255 bridge-ports enp3s0f0 enp3s0f1 enp3s0f2 enp3s0f3 ** TIPS: please run the below command backup your interfaces file first. cp /etc/network/interfaces /etc/network/interfaces.backup From this interface configure file, you can see: External interface will use Dynamic Host Configuration Protocol (DHCP), whereby external network DHCP server will dynamically assigns an IP adress and other network configuration paprameters to this external interface. Created a new interface, names br0. Using static settings, I manuelly assign IP address 192.168.1.1 to this bridge, when you set-up internal network clients 192.168.1.1 should be the gateway. internal network IP should be 192.168.1.x(x minimum valu is 2, the maxmum value is 255) All of my PCIE ports connected with this bridge. ** When you finished settings for this section, please restart networking. The command at the below ** sudo /etc/init.d/networking restart ","date":"2020-09-04","objectID":"/ubunturouter/:5:1","tags":["Networking"],"title":"Setting up an ubuntu Router with bridge","uri":"/ubunturouter/"},{"categories":["Networking"],"content":"Enable IP forwarding and Masquerading Doing the above might not be enough to make the Ubuntu machine a real router which does NAT (Network Address Translation) and IP Forwarding. The following script configures the Kernel IPTable and IP forwarding. You will have to configure at least the script’s 2 variables; the 1st is the external network interface; the 2nd is the internal network interface. echo -e \"\\n\\nLoading simple rc.firewall-iptables version $FWVER..\\n\" DEPMOD=/sbin/depmod MODPROBE=/sbin/modprobe EXTIF=\"eth0\" # Change eth0 as your External Network Interface Name INTIF=\"eth1\" # Change eth1 as bridge name (Internal Network Interface) echo \" External Interface: $EXTIF\" echo \" Internal Interface: $INTIF\" ##====================================================================== ##== No editing beyond this line is required for initial MASQ testing == echo -en \" loading modules: \" echo \" - Verifying that all kernel modules are ok\" $DEPMOD -a echo \"----------------------------------------------------------------------\" echo -en \"ip_tables, \" $MODPROBE ip_tables echo -en \"nf_conntrack, \" $MODPROBE nf_conntrack echo -en \"nf_conntrack_ftp, \" $MODPROBE nf_conntrack_ftp echo -en \"nf_conntrack_irc, \" $MODPROBE nf_conntrack_irc echo -en \"iptable_nat, \" $MODPROBE iptable_nat echo -en \"nf_nat_ftp, \" $MODPROBE nf_nat_ftp echo \"----------------------------------------------------------------------\" echo -e \" Done loading modules.\\n\" echo \" Enabling forwarding..\" echo \"1\" \u003e /proc/sys/net/ipv4/ip_forward echo \" Enabling DynamicAddr..\" echo \"1\" \u003e /proc/sys/net/ipv4/ip_dynaddr echo \" Clearing any existing rules and setting default policy..\" iptables-restore \u003c\u003c-EOF *nat -A POSTROUTING -o \"$EXTIF\" -j MASQUERADE COMMIT *filter :INPUT ACCEPT [0:0] :FORWARD DROP [0:0] :OUTPUT ACCEPT [0:0] -A FORWARD -i \"$EXTIF\" -o \"$INTIF\" -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT -A FORWARD -i \"$INTIF\" -o \"$EXTIF\" -j ACCEPT -A FORWARD -j LOG COMMIT EOF echo -e \"\\nrc.firewall-iptables v$FWVERdone.\\n\" After configuring the 2 variables, save the script below as nat.sh and make it executable by doing chmod a+x nat.sh Now, test the script by running as root sudo sh nat.sh ","date":"2020-09-04","objectID":"/ubunturouter/:5:2","tags":["Networking"],"title":"Setting up an ubuntu Router with bridge","uri":"/ubunturouter/"},{"categories":["Networking"],"content":"Clients network settings Each clients at the less MUST setting the corrent IP address, netmask, gateway and DNS. ** TIPS: If you using the above bridge settings, you can following the below list to set-up your client network interface. The bridge IP is 192.168.1.1, so your clent IP should be 192.168.1.x (x minimum valu is 2, the maxmum value is 255), all of clients IP must be unique within this internal network(subnet). netmask should be 255.255.255.0 gateway should be 192.168.1.1, which is bridge’s IP address DNS can be setting as your ISP DNS or public DNS, such as Google’s DNS 8.8.8.8, 8.8.4.4 ** For Linux Using ifconfig Setting up IP address and Netmask command line sudo ifconfig eth0 192.168.1.2 netmask 255.255.255.0 ** TIPS: ** eth0 is network interface name, you need change this. 192.168.1.2 is client IP address you want to set. netmask 255.255.255.0 is good. Setting up gateway command line sudo route add default gw 192.168.1.1 eth0 ** TIPS: ** eth0 is network interface name, you need change this. gw xxx.xxx.xxx.xxx is your gateway IP address Ubuntu GUI For Mac On your Mac, choose Apple menu \u003e System Preferences, then click Network. Select the network connection you want to use (such as Ethernet) in the list. Choose Manually and enter the address in the IP Address field. Also, subnet mask, gateway, and Domain Name System (DNS) server address. For Windows Open Start on Windows 10. Search for Command Prompt, right-click the result and select the Run as administrator option to open the console. Type the following command to see your current networking configuration and press Enter: ipconfig /all Type the following command to assign a static IP address and press Enter: netsh interface ip set address name=\"Ethernet0\" static 192.168.1.2 255.255.255.0 192.168.1.1 In the above command make sure to change Ethernet0 for the name of your network adapter, and you must change 192.168.1.2 255.255.255.0 192.168.1.1 with the device IP address, subnet mask, and default gateway address that corresponds to your network configuration. Type the following command to set a DNS server address and press Enter: netsh interface ip set dns name=\"Ethernet0\" static 10.1.2.1 In the above command make sure to change Ethernet0 with the name of your adapter and 10.1.2.1 with the DNS server address of your network. 6. Type the following command to set an alternate DNS server address and press Enter: netsh interface ip add dns name=\"Ethernet0\" 8.8.8.8 index=2 In the above command make sure to change Ethernet0 with the name of your adapter and 8.8.8.8 with an alternate DNS server address. ** After you complete the steps, you can test the new configuration using the ping command (for example, ping google.com) to see if the internet is working. Alternatively, you can simply open your web browser and try to navigate to a website to see if the configuration works. ** ","date":"2020-09-04","objectID":"/ubunturouter/:5:3","tags":["Networking"],"title":"Setting up an ubuntu Router with bridge","uri":"/ubunturouter/"},{"categories":["Networking"],"content":"Troubleshooting ","date":"2020-09-04","objectID":"/ubunturouter/:6:0","tags":["Networking"],"title":"Setting up an ubuntu Router with bridge","uri":"/ubunturouter/"},{"categories":["Basler"],"content":"Introduction This post document is about using Basler Camera with Opencv on Python. At the beginning of this guide I will introduct how to set-up development environment with Pylon and opencv within Conda virual environment. Seconly, I will talk about how to check and set some of basic Basler feature base on pypylon. At the end, I will give a example about using Opencv read Basler Camera data stream. ","date":"2020-09-03","objectID":"/baslerwithopencv/:1:0","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Hardware and Software Testing Environment ","date":"2020-09-03","objectID":"/baslerwithopencv/:2:0","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Hardware Environment Camera: Basler (a2A1920 - 51gcPRO) Network Adapter: Intel I350 Gigabit Network adapter (POE) CPU: Intel(R) Core(TM) i7-9700F CPU @ 3.00GHz RAM: 16GiB system memory ","date":"2020-09-03","objectID":"/baslerwithopencv/:2:1","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Software Environment Ubuntu 18.04 (Linux version 5.4.0-48-generic) Need set-up Network exchange environment, the document link: {% post_link ubuntuRouter %} ","date":"2020-09-03","objectID":"/baslerwithopencv/:2:2","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Development Environment Set-up ","date":"2020-09-03","objectID":"/baslerwithopencv/:3:0","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Creating conda environment You can follow the below steps to create the conda environment. If you want to know more details about Conda, please have a look link: {% post_link conda %} conda create -n baslerOpencv python=3.7.7 conda activate baslerOpencv conda install notebook ipykernel ipython kernel install --user --name baslerOpencv --display-name \"Python (Basler with Opencv)\" ","date":"2020-09-03","objectID":"/baslerwithopencv/:3:1","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Installing Pylon Go to Pylon Official Website. Go to Downloads Section Click link: pylon 6.1.1 Camera Software Suite Linux x86 (64 Bit) - Debian Installer Package Filling in your personal INFOs and click “Start the Download!” Goto you download directory(Using cd command) You can use the apt command for install deb file sudo apt install xxxx.deb When you done, you can see there are three applications be installed, which are Pylon IP Configurator, pylon viewer and Basler Product Documention. ","date":"2020-09-03","objectID":"/baslerwithopencv/:3:2","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Installing pypylon binary wheel file to conda environment pypylon ","date":"2020-09-03","objectID":"/baslerwithopencv/:3:3","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"For the Impatient Download a binary wheel from the releases page. https://github.com/basler/pypylon/releases/download/1.6.0/pypylon-1.6.0-cp37-cp37m-linux_x86_64.whl Install the wheel using pip3 install .whl Look at samples/grab.py in this repository TIPS: Traceback (most recent call last): File \"opencv.py\", line 6, in \u003cmodule\u003e from pypylon import pylon File \"/home/yanboyang713/miniconda3/envs/baslerOpencv/lib/python3.7/site-packages/pypylon/pylon.py\", line 40, in \u003cmodule\u003e from . import _pylon ImportError: libpython3.7m.so.1.0: cannot open shared object file: No such file or directory export LD_LIBRARY_PATH=/home/yanboyang713/miniconda3/envs/front/lib echo $LD_LIBRARY_PATH ","date":"2020-09-03","objectID":"/baslerwithopencv/:3:4","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Set-up pypylon-opencv-viewer(Option) pip install pypylon-opencv-viewer ","date":"2020-09-03","objectID":"/baslerwithopencv/:3:5","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Installing Opencv to conda environment pip install opencv-python==3.4.2.17 pip install opencv-contrib-python==3.4.2.17 ","date":"2020-09-03","objectID":"/baslerwithopencv/:3:6","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Image Control ","date":"2020-09-03","objectID":"/baslerwithopencv/:4:0","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Image ROI (Area of Insterest) Getting the width of the maximum value and the height of the maximum value print (\"MaxWidth: \", camera.Width.GetMax()) print (\"MaxHeight: \", camera.Height.GetMax()) Setting the width of the image ROI and the height of the image ROI ## Set the width to 1920 camera.Width.SetValue(1920); ## Set the height to 1080 camera.Height.SetValue(1080) Center the image ROI To enable Center X, set the CenterX parameter to true. The camera adjusts the OffsetX parameter value to center the image ROI horizontally. When you change the width of the image ROI, the OffsetX parameter value automatically adapts. The OffsetX parameter becomes read-only. To enable Center Y, set the CenterY parameter to true. The camera adjusts the OffsetY parameter value to center the image ROI vertically. When you change the height of the image ROI, the OffsetY parameter value automatically adapts. The OffsetY parameter becomes read-only. camera.BslCenterX.Execute(); camera.BslCenterY.Execute(); Setting the offset of image ROI If you already center the image ROI, offset parameter becomes read-only. You cannot set offset value again. camera.OffsetX.SetValue(0); camera.OffsetY.SetValue(0); ''' Print the ROI INFOs print (\"camera Width increment: \", camera.Width.GetInc()) print (\"camera Height increment: \", camera.Height.GetInc()) print (\"camera Minimum Width: \", camera.Width.GetMin()) print (\"camera Minimum Height: \", camera.Height.GetMin()) print (\"camera Width: \", camera.Width.GetValue()) print (\"Camera offsetX: \", camera.OffsetX.GetValue()) print (\"camera Height: \", camera.Height.GetValue()) print (\"Camera offsetY: \", camera.OffsetY.GetValue() ) assert camera.Width.GetValue() + camera.OffsetX.GetValue() \u003c= camera.Width.GetMax() assert camera.Height.GetValue() + camera.OffsetY.GetValue() \u003c= camera.Height.GetMax() ","date":"2020-09-03","objectID":"/baslerwithopencv/:4:1","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Image/Pixel Format Control The offical Pixel Formats list your camera available Pixel Formats print(camera.PixelFormat.Symbolics) show your current Pixel Formats in used print (\"camera.PixelFormat.GetValue: \", camera.PixelFormat.GetValue()) Mono Formats If a monochrome camera uses one of the mono pixel formats, it outputs 8, 10, or 12 bits of data per pixel. If a color camera uses the Mono 8 pixel format, the values for each pixel are first converted to the YUV color model. The Y component of this model represents a brightness value and is equivalent to the value that would be derived from a pixel in a monochrome image. So in essence, when a color camera is set to Mono 8, it outputs an 8-bit monochrome image. This type of output is sometimes referred to as “Y Mono 8”. camera.PixelFormat.SetValue(\"Mono8\") camera.PixelFormat.SetValue(\"Mono12\") camera.PixelFormat.SetValue(\"Mono12p\") Bayer Formats Color cameras are equipped with a Bayer color filter and can output color images based on the Bayer pixel formats given below. If a color camera uses one of these Bayer pixel formats, it outputs 8, 10, or 12 bits of data per pixel. The pixel data is not processed or interpolated in any way. For each pixel covered with a red filter, you get 8, 10, or 12 bits of red data. For each pixel covered with a green filter, you get 8, 10, or 12 bits of green data. For each pixel covered with a blue filter, you get 8, 10, or 12 bits of blue data. This type of pixel data is sometimes referred to as “raw” output. camera.PixelFormat.SetValue(\"BayerRG8\") camera.PixelFormat.SetValue(\"BayerRG12\") camera.PixelFormat.SetValue(\"BayerRG12p\") RGB and BGR Formats When a color camera uses the RGB 8 or BGR 8 pixel format, the camera outputs 8 bit of red data, 8 bit of green data, and 8 bit of blue data for each pixel in the acquired frame. The pixel formats differ by the output sequences for the color data (red, green, blue or blue, green, red). camera.PixelFormat.SetValue(\"RGB8\") camera.PixelFormat.SetValue(\"BGR8\") YUV Formats Color cameras can also output color images based on pixel data in YUV (or YCbCr) format. If a color camera uses this format, each pixel value in the captured image goes through a conversion process as it exits the sensor and passes through the camera. This process yields Y, U, and V color information for each pixel value. Info The values for U and V normally range from -128 to +127. Because the camera transfers U values and V values with unsigned integers, 128 is added to each U value and V value before they are transferred from the camera. This way, values from 0 to 255 can be transferred. camera.PixelFormat.SetValue(\"YCbCr422_8\") ","date":"2020-09-03","objectID":"/baslerwithopencv/:4:2","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Black Level The Black Level camera feature allows you to change the overall brightness of an image. All gray values of the pixels are changed by a specified amount. For example, you can increase the gray value of each pixel in the image by 3. Adjusting the Black Level To adjust the black level, enter a value for the BlackLevel parameter. The minimum black level setting is 0. The maximum setting depends on the camera model. The change in the gray value resulting from the BlackLevel parameter value also depends on the camera model. INFO Basler recommends setting the black level to 0 before using any of the color enhancement features, e.g., Balance White, Color Transformation, or Gamma. After the color enhancements have been applied, you can change the black level as desired. However, increasing the black level will decrease the color accuracy. camera.BlackLevel.SetValue(0) print (\"camera.BlackLevel.GetValue()\", camera.BlackLevel.GetValue()) ","date":"2020-09-03","objectID":"/baslerwithopencv/:4:3","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Gamma The Gamma camera feature allows you to optimize the brightness of acquired images for display on a monitor. Using the Feature How It Works The camera applies a gamma correction value (γ) to the brightness value of each pixel according to the following formula (red pixel value (R) of a color camera shown as an example): $R_{corrected} = (\\frac{ R_{uncorrected} }{ R_{max} }) ^ γ * R_{max} $ The maximum pixel value (Rmax) equals, e.g., 255 for 8-bit pixel formats or 1 023 for 10-bit pixel formats. Enabling Gamma Correction To enable gamma correction: Set the GammaEnable parameter to true (if available). For best results, set the BlackLevel parameter to 0. Set the Gamma parameter to the desired value. The parameter’s value range is 0 to ≈4. Gamma = 1: The overall brightness remains unchanged. Gamma \u003c 1: The overall brightness increases. Gamma \u003e 1: The overall brightness decreases. In all cases, black pixels (brightness = 0) and white pixels (brightness = maximum) will not be adjusted. INFO If you enable gamma correction and the pixel format is set to a 12-bit pixel format, some image information will be lost. Pixel data output will still be 12-bit, but the pixel values will be interpolated during the gamma correction process. Additional Parameters Depending on your camera model, the following additional parameters are available: GammaEnable: Enables or disables gamma correction. GammaSelector: Allows you to select one of the following gamma correction modes: User: The gamma correction value can be set as desired. (Default.) sRGB: The camera automatically sets a gamma correction value of approximately 0.4. This value is optimized for image display on sRGB monitors. BslColorSpaceMode or BslColorSpace: Allows you to select one of the following gamma correction modes: RGB: No additional gamma correction value is applied. sRGB: The image brightness is optimized for display on an sRGB monitor. A gamma correction value of approximately 0.4 is applied. For more information, see the footnotes in the Specifics section. ## Set the Gamma value to 1.2 camera.Gamma.SetValue(1.2) print (\"camera.Gamma.GetValue()\", camera.Gamma.GetValue()) ## Set the color space to sRGB camera.BslColorSpace.SetValue(\"sRgb\"); print (\"camera.BslColorSpace.GetValue(): \", camera.BslColorSpace.GetValue()) ","date":"2020-09-03","objectID":"/baslerwithopencv/:4:4","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Auto Function ROI The Auto Function ROI camera feature allows you to specify the part of the sensor array with which you want to control the camera’s auto functions. ROI is short for region of interest (formerly AOI = area of interest). You can create several auto function ROIs, each occupying different parts of the sensor array. The settings for the Auto Function ROI feature are independent of the settings for the Image ROI feature. Changing Position and Size of an Auto Function ROI By default, all auto function ROIs are set to the same size as the camera’s image ROI. You can change their positions and sizes to suit your needs. To change the position and size of an auto function ROI: Set the AutoFunctionROISelector parameter to one of the available auto function ROIs, e.g., ROI1. Enter values for the following parameters to specify the position of the auto function ROI selected: AutoFunctionROIOffsetX AutoFunctionROIOffsetY Enter values for the following parameters to specify the size of the auto function ROI selected: AutoFunctionROIWidth AutoFunctionROIHeight The position of an auto function ROI is specified based on the lines and rows of the sensor array. Example: Assume that you have selected auto function ROI 1 and specified the following settings: AutoFunctionROIOffsetX = 14 AutoFunctionROIOffsetY = 7 AutoFunctionROIWidth = 5 AutoFunctionROIHeight = 6 This creates the following auto function ROI 1: Only the pixel data from the area of overlap between the auto function ROI and the image ROI will be used by the auto function assigned to it. Info On color cameras, Basler recommends setting the parameters for position and size to even values (multiples of 2). This matches the auto function ROI to the color filter pattern of the sensor. If the Binning feature is enabled, the auto function ROI settings refer to the binned lines and columns and not to the physical lines in the sensor. If the Reverse X or Reverse Y feature or both are enabled, the position of the auto function ROI relative to the sensor remains the same. As a consequence, different regions of the image will be controlled depending on whether or not Reverse X, Reverse Y or both are enabled. Auto Function ROI Highlighting If highlighting is supported by your camera model, you can highlight one or multiple Auto Function ROIs in the pylon Viewer. Areas that don’t belong to the Auto Function ROIs appear darker: To highlight an Auto Function ROI: Set the AutoFunctionROISelector parameter to the desired auto function ROI, e.g., ROI1. Set the AutoFunctionROIHighlight parameter to true. Assigning Auto Functions By default, each auto function ROI is assigned to a specific auto function. For example, the pixel data from auto function ROI 2 is used to control the Balance White Auto auto function. On some camera models, the default assignments can be changed. To do so: Set the AutoFunctionROISelector parameter to one of the available auto function ROIs, e.g., ROI1. Assign the desired auto function(s) to the auto function ROI selected: If you want to assign Balance White Auto, set the AutoFunctionROIUseWhiteBalance parameter to true. If you want to assign Exposure Auto and gain auto, set the AutoFunctionROIUseBrightness parameter to true. (Exposure Auto and Gain Auto always work together.) If you want to assign Tonal Range Auto, set the AutoFunctionROIUseTonalRange parameter to true. Info If you assign one auto function to multiple auto function ROIs, the pixel data from all selected auto function ROIs will be used for the auto function. If you assign multiple auto functions to one auto function ROI, the pixel data from the auto function ROI will be used for all auto functions selected. Exposure Auto and Gain Auto Assignments Work Together When making auto function ROI assignments, the Gain Auto auto function and the exposure auto auto function always work together. they are considered as a single auto function named “intensity” or “brightness”, depending on your camera mode","date":"2020-09-03","objectID":"/baslerwithopencv/:4:5","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Auto Function Profile The Auto Function Profile camera feature allows you to specify how gain and exposure time are balanced when the camera is making automatic adjustments. Setting the Auto Function Profile To set the auto function profile: Set the Gain Auto auto function and the Exposure Auto auto function to Continuous. Set the AutoFunctionProfile parameter to one of the following values (if available): MinimizeGain MinimizeExposureTime MinimizeGainQuick MinimizeExposureTimeQuick Smart AntiFlicker50Hz AntiFlicker60Hz Available Auto Function Profiles Minimize Gain (= Gain Minimum) The gain is kept as low as possible during the automatic adjustment process. If the exposure time is at its upper limit and the target brightness value has not been reached yet, the gain will be increased in order to reach the target. Minimize Exposure Time (= Exposure Minimum) The exposure time is kept as low as possible during the automatic adjustment process. If the gain is at its upper limit and the target brightness value has not been reached yet, the exposure time will be increased in order to reach the target. Minimize Gain Quick (= Gain Minimum Quick) This profile works the same as the Minimize Gain profile. The difference is that it reacts more quickly in situations with extreme changes in brightness or where the image brightness changes rapidly. This situation occurs, for example, when microscope objective lenses are changed using the objective turret. Minimize Exposure Time Quick (= Exposure Minimum Quick) This profile works the same as the Minimize Exposure Time profile. The difference is that it reacts more quickly in situations with extreme changes in brightness or where the image brightness changes rapidly. This situation occurs, for example, when microscope objective lenses are changed using the objective turret. Smart Gain is kept as low as possible and the frame rate will be kept as high as possible during automatic adjustments. This is a four-step process: The camera adjusts the exposure time to achieve the target brightness value. If the exposure time must be increased to achieve the target brightness value, the camera does so until the frame rate drops. If the frame rate drops, the camera stops increasing the exposure time and increases the gain until the AutoGainUpperLimit value is reached. When the AutoGainUpperLimit value has been reached, the camera stops increasing the gain and increases the exposure time until the target brightness value is reached. Increasing the exposure time results in a lower frame rate. Anti-Flicker 50 Hz / 60 Hz Gain and exposure time are optimized to reduce flickering. If the camera is operating in an environment where the lighting flickers at a 50-Hz or a 60-Hz rate, the flickering lights can cause significant changes in brightness from image to image. Enabling the anti-flicker profile may reduce the effect of the flickering in the captured images. Choose the frequency (50 Hz or 60 Hz) according your local power line frequency (e.g., North America: 60 Hz, Europe: 50 Hz). ## Set the auto function profile to Exposure Minimum camera.AutoFunctionProfile.SetValue(\"MinimizeExposureTime\"); print (\"camera.AutoFunctionProfile.GetValue(): \", camera.AutoFunctionProfile.GetValue()) ## Set the auto function profile to Gain Minimum camera.AutoFunctionProfile.SetValue(\"MinimizeGain\") print (\"camera.AutoFunctionProfile.GetValue(): \", camera.AutoFunctionProfile.GetValue()) ## Enable Gain and Exposure Auto auto functions and set the operating mode to Continuous camera.GainAuto.SetValue(\"Continuous\") print (\"camera.GainAuto.GetValue(): \", camera.GainAuto.GetValue()) camera.ExposureAuto.SetValue(\"Continuous\") print (\"camera.ExposureAuto.GetValue(): \", camera.ExposureAuto.GetValue()) ","date":"2020-09-03","objectID":"/baslerwithopencv/:4:6","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Balance White Auto The Balance White Auto camera feature automatically corrects color shifts in images acquired. The pixel data for the auto function can come from one or multiple auto function ROIs. To correct color shifts manually, use the Balance White feature. Enabling or Disabling Balance White Auto To enable or disable the Balance White Auto auto function: Assign at least one auto function ROI to the Balance White Auto auto function. Make sure the auto function ROI overlaps the image ROI, either partially or completely. Set the BalanceWhiteAuto parameter to one of the following operating modes: Once: The camera adjusts the white balance until the average gray values for red, green, and blue are identical. When this has been achieved, or after a maximum of 30 calculation cycles, the camera sets the auto function to Off and applies the balance ratios resulting from the last calculation to all following images. Continuous: The camera adjusts the white balance continuously while images are being acquired. The adjustment process continues until the operating mode is set to Once or Off. Off: Disables the Balance White Auto auto function. The BalanceRatio parameters remain at the values resulting from the last automatic or manual adjustment. How It Works Automatic white balancing is a two-step process: The camera compares the average gray values of the red, green, and blue pixels. It determines the color with the highest average gray value (i.e., the brightest color) and sets the BalanceRatio parameter value for this color to 1. The camera automatically adjusts the BalanceRatio parameter values of the other two colors until the average gray values for red, green, and blue are identical. As a result, the BalanceRatio parameter is set to 1 for one color and to a value between 1 and ≈15.98 for the other two colors. Example: Assume the green pixels in your image have the highest average gray value. If you enable the Balance White Auto auto function, the camera sets the BalanceRatio parameter value for green to 1. Then, the camera automatically adjusts the BalanceRatio parameter values for red and blue until the average gray values for red, green, and blue are identical. The new balance ratios could be, e.g., green = 1, red = 1.08789, and blue = 2.19678. Info To view the BalanceRatio parameter values for red, green, or blue, switch to the respective color channel using the BalanceRatioSelector. When the camera is capturing images continuously, the auto function takes effect with a short delay. The first few images may not be affected by the auto function. camera.AutoFunctionROISelector.SetValue(\"ROI1\") camera.AutoFunctionROIUseWhiteBalance.SetValue(True) print (\"camera.AutoFunctionROIUseWhiteBalance.GetValue(): \", camera.AutoFunctionROIUseWhiteBalance.GetValue()) camera.BalanceWhiteAuto.SetValue(\"Continuous\") print (\"camera.BalanceWhiteAuto.GetValue(): \", camera.BalanceWhiteAuto.GetValue()) ","date":"2020-09-03","objectID":"/baslerwithopencv/:4:7","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["Basler"],"content":"Light Source Preset The Light Source Preset camera feature allows you to correct color shifts caused by certain light sources. Depending on its specific color temperature, the light used for image acquisition can cause color shifts in the image. You can correct these color shifts by selecting the related light source preset. Selecting a Light Source Preset To select a light source preset, set the BslLightSourcePreset parameter to one of the following values (if available): Off: No light source preset is selected. Daylight5000K: The camera corrects color shifts caused by daylight lighting that has a color temperature of about 5 000 K. Daylight6500K: The camera corrects color shifts caused by daylight lighting that has a color temperature of about 6 500 K. Tungsten2800K: The camera corrects color shifts caused by tungsten lighting that has a color temperature of about 2 500 to 3 000 K. MicroscopeLED4500K: The camera corrects color shifts caused by microscope LED lighting that has a color temperature of about 4 500 K. MicroscopeLED5500K: The camera corrects color shifts caused by microscope LED lighting that has a color temperature of about 5 500 K. MicroscopeLED6000K: The camera corrects color shifts caused by microscope LED lighting that has a color temperature of about 6 000 K. Fluorescent4000K: The camera corrects color shifts caused by fluorescent lighting that has a color temperature of about 4 000 K. Custom: Selecting this preset enables the Color Transformation feature which allows you to customize the light source settings. You should only select this preset if you are thoroughly familiar with matrix color transformations. The camera also adjusts the Balance White and Color Adjustment settings so that they have neutral values that do not change the appearance of the colors. The default light source preset varies by camera model. Info On Basler dart cameras, the light source presets are calibrated for the IR cut filter in the CS-mount variant. If you are using an S-mount or bare board variant, make sure your IR cut filter has suitable spectral characteristics. For more information about the IR cut filter, see your camera topic. You can find your camera topic in the “Models” section. Impact on Other Features When you select a light source preset, the camera adjusts the settings of the following color enhancement features: Balance White Color Adjustment Color Transformation The settings will be optimized for the selected light source. On some camera models, you can choose which features you want the camera to adjust. Separate Processing On some camera models, when you select a light source preset, the camera processes the changes to the features listed above separately. This means that the values of the corresponding parameters visible in the pylon API and the pylon Viewer are not changed. Example: If you select the Daylight6500K light source preset, the camera adjusts the white balance, but the values of the BalanceRatio parameter don’t change. This has the advantage that you don’t lose your color enhancement feature settings when you change the light source preset. Your own settings are independent from the light source preset adjustments. No Separate Processing On some camera models, when you select a light source preset, the camera doesn’t process the feature changes separately. Instead, the camera directly adjusts the corresponding parameter values. Example: If you select the Daylight6500K light source preset, the values of the BalanceRatio parameter change. You can see the changes in the pylon Viewer or by accessing the parameter via the pylon API. This means if you set up the color enhancement features and then change the light source preset, your settings will be overwritten. Light Source Preset Feature Selector On some camera models, the BslLightSourcePresetFeatureSelector parameter is available. If the parameter is available, you can select which features you want the camera to adjust when you select a light sourc","date":"2020-09-03","objectID":"/baslerwithopencv/:4:8","tags":["Basler","Opencv","python"],"title":"Getting started with Basler Camera With Opencv on Python","uri":"/baslerwithopencv/"},{"categories":["UML"],"content":"Introduction This post document will brief introduct UML at the begining. After, will talk how using PlantUML for drawing UML diagrams. At the end, how does PlantUML integrate with Markdown and Emacs(Spacemacs). ","date":"2020-09-01","objectID":"/plantuml/:1:0","tags":["UML","plantuml","Markdown","Emacs","spacemacs"],"title":"Getting started with UML (Unified Modeling Language)","uri":"/plantuml/"},{"categories":["UML"],"content":"UML Basics ","date":"2020-09-01","objectID":"/plantuml/:2:0","tags":["UML","plantuml","Markdown","Emacs","spacemacs"],"title":"Getting started with UML (Unified Modeling Language)","uri":"/plantuml/"},{"categories":["UML"],"content":"PlantUML Basics ","date":"2020-09-01","objectID":"/plantuml/:3:0","tags":["UML","plantuml","Markdown","Emacs","spacemacs"],"title":"Getting started with UML (Unified Modeling Language)","uri":"/plantuml/"},{"categories":["UML"],"content":"Reasons for using PlantUML ","date":"2020-09-01","objectID":"/plantuml/:3:1","tags":["UML","plantuml","Markdown","Emacs","spacemacs"],"title":"Getting started with UML (Unified Modeling Language)","uri":"/plantuml/"},{"categories":["UML"],"content":"Markdown ","date":"2020-09-01","objectID":"/plantuml/:4:0","tags":["UML","plantuml","Markdown","Emacs","spacemacs"],"title":"Getting started with UML (Unified Modeling Language)","uri":"/plantuml/"},{"categories":["UML"],"content":"Emacs (Spacemacs) ","date":"2020-09-01","objectID":"/plantuml/:5:0","tags":["UML","plantuml","Markdown","Emacs","spacemacs"],"title":"Getting started with UML (Unified Modeling Language)","uri":"/plantuml/"},{"categories":["UML"],"content":"Summary ","date":"2020-09-01","objectID":"/plantuml/:6:0","tags":["UML","plantuml","Markdown","Emacs","spacemacs"],"title":"Getting started with UML (Unified Modeling Language)","uri":"/plantuml/"},{"categories":null,"content":"Introduction This post document is a guide to the popular file formats used in open source frameworks for machine learning in Python, including TensorFlow/Keras, PyTorch, Scikit-Learn, and PySpark. We will also describe how a Feature Store can make the Data Scientist’s life easier by generating training/test data in a file format of choice on a file system of choice. This guide is base on Jim Dowling’s blog. A file format defines the structure and encoding of the data stored in it and it is typically identified by its file extension — for example, a filename ending in .txt indicates the file is a text file. However, even though files are used to store much of the world’s data, most of that data is not in a format that can be used directly to train ML models. This post is mostly concerned with file formats for structured data and we will discuss the easy creation of training data in popular file formats for ML, such as .tfrecords, .csv, .npy, and .petastorm, as well as the file formats used to store models, such as .pb and .pkl . ","date":"2020-09-01","objectID":"/machinelearningfileformats/:1:0","tags":["Machine Learning","machine Learning File Formats"],"title":"Guide to File Formats for Machine Learning","uri":"/machinelearningfileformats/"},{"categories":null,"content":"Data sources Machine learning frameworks want to consume training data as a sequence of samples, so file formats for training ML models should have easily consumable layouts with no impedance mismatch with the storage platform or the language used to read/write the files. Additionally, distributed training (training ML models on many GPUs at the same time to make training go faster) requires files to be splittable and accessible over a distributed file system or object store, so that different GPUs can read different shards (partitions) of the data in parallel from different servers. ","date":"2020-09-01","objectID":"/machinelearningfileformats/:2:0","tags":["Machine Learning","machine Learning File Formats"],"title":"Guide to File Formats for Machine Learning","uri":"/machinelearningfileformats/"},{"categories":["Proxmox"],"content":"Introduction https://pve.proxmox.com/wiki/Linux_Container Containers are a lightweight alternative to fully virtualized machines (VMs). They use the kernel of the host system that they run on, instead of emulating a full operating system (OS). This means that containers can access resources on the host system directly. The runtime costs for containers is low, usually negligible. However, there are some drawbacks that need be considered: Only Linux distributions can be run in Proxmox Containers. It is not possible to run other operating systems like, for example, FreeBSD or Microsoft Windows inside a container. For security reasons, access to host resources needs to be restricted. Therefore, containers run in their own separate namespaces. Additionally some syscalls (user space requests to the Linux kernel) are not allowed within containers. Proxmox VE uses Linux Containers (LXC) as its underlying container technology. The “Proxmox Container Toolkit” (pct) simplifies the usage and management of LXC, by providing an interface that abstracts complex tasks. Containers are tightly integrated with Proxmox VE. This means that they are aware of the cluster setup, and they can use the same network and storage resources as virtual machines. You can also use the Proxmox VE firewall, or manage containers using the HA framework. Our primary goal is to offer an environment that provides the benefits of using a VM, but without the additional overhead. This means that Proxmox Containers can be categorized as “System Containers”, rather than “Application Containers”. NOTE: If you want to run application containers, for example, Docker images, it is recommended that you run them inside a Proxmox Qemu VM. This will give you all the advantages of application containerization, while also providing the benefits that VMs offer, such as strong isolation from the host and the ability to live-migrate, which otherwise isn’t possible with containers. ","date":"2020-02-06","objectID":"/lxccontainerproxmox/:1:0","tags":["LXC Container","Proxmox"],"title":"Get started with LXC Container in Proxmox","uri":"/lxccontainerproxmox/"},{"categories":["Proxmox"],"content":"Container Images Download Container images, sometimes also referred to as “templates” or “appliances”, are tar archives which contain everything to run a container. Proxmox VE itself provides a variety of basic templates for the most common Linux distributions. They can be downloaded using the GUI or the pveam (short for Proxmox VE Appliance Manager) command line utility. Additionally, TurnKey Linux container templates are also available to download. The list of available templates is updated daily through the pve-daily-update timer. You can also trigger an update manually by executing: ## pveam update To view the list of available images run: ## pveam available You can restrict this large list by specifying the section you are interested in, for example basic system images: List available system images root@richie:~# pveam available --section system system alpine-3.11-default_20200425_amd64.tar.xz system alpine-3.12-default_20200823_amd64.tar.xz system archlinux-base_20200508-1_amd64.tar.gz system archlinux-base_20201116-1_amd64.tar.gz system centos-7-default_20190926_amd64.tar.xz system centos-8-default_20191016_amd64.tar.xz system debian-10-standard_10.5-1_amd64.tar.gz system debian-9.0-standard_9.7-1_amd64.tar.gz system fedora-32-default_20200430_amd64.tar.xz system fedora-33-default_20201115_amd64.tar.xz system gentoo-current-default_20200310_amd64.tar.xz system opensuse-15.2-default_20200824_amd64.tar.xz system ubuntu-16.04-standard_16.04.5-1_amd64.tar.gz system ubuntu-18.04-standard_18.04.1-1_amd64.tar.gz system ubuntu-20.04-standard_20.04-1_amd64.tar.gz system ubuntu-20.10-standard_20.10-1_amd64.tar.gz root@richie:~# Before you can use such a template, you need to download them into one of your storages. If you’re unsure to which one, you can simply use the local named storage for that purpose. For clustered installations, it is preferred to use a shared storage so that all nodes can access those images. pveam download local ubuntu-20.04-standard_20.04-1_amd64.tar.gz root@richie:~# pveam list local NAME SIZE local:vztmpl/ubuntu-20.04-standard_20.04-1_amd64.tar.gz 204.28MB root@richie:~# ","date":"2020-02-06","objectID":"/lxccontainerproxmox/:2:0","tags":["LXC Container","Proxmox"],"title":"Get started with LXC Container in Proxmox","uri":"/lxccontainerproxmox/"},{"categories":["Proxmox"],"content":"Steps for create a new LXC Container Open Proxmox web Select Server View then select your Node then click on Create CT Enter hostname and Password Select Template Storage and then Select OS from Dropdown List and click on Next Enter Disk Size Enter Number of CPU Cores Enter RAM size in MB Enter Network Details Enter Name Servers Details Click on Finish Wait for the task to complete. You Proxmox Container is ready. ","date":"2020-02-06","objectID":"/lxccontainerproxmox/:3:0","tags":["LXC Container","Proxmox"],"title":"Get started with LXC Container in Proxmox","uri":"/lxccontainerproxmox/"},{"categories":["Linux"],"content":"Overview A Linux kernel is the core of a Linux distribution and consists of three things: The kernel itself The kernel’s headers The kernel’s extra modules The kernel headers are used to define device interfaces. For example, they can be used to compile the module that controls your computer’s video card and driver. The main reason you may find yourself needing to install kernel headers is if you are compiling kernel modules and need Linux to access and communicate with hardware as intended. Sometimes device drivers may require you to update the kernel headers on your system to function properly. Other times, there may be compatibility issues with new versions and you’ll need to roll back an update. The process for doing this can vary on each distribution, but in this guide we’ll be going over the steps specifically for Manjaro Linux. Continue reading to find out how to install kernel headers, check the version of installed kernel headers, and switch between kernel header versions on Manjaro. ","date":"2020-02-06","objectID":"/kernel/:1:0","tags":["kernel","manjaro"],"title":"Manjaro Linux kernel headers installation","uri":"/kernel/"},{"categories":["Linux"],"content":"In this tutorial you will learn: Linux API headers vs kernel headers Install or update kernel headers Switch between kernel header versions ","date":"2020-02-06","objectID":"/kernel/:2:0","tags":["kernel","manjaro"],"title":"Manjaro Linux kernel headers installation","uri":"/kernel/"},{"categories":["Linux"],"content":"Getting the right package To clear up any upcoming confusion, let us preface by saying that Arch Linux, the ancestor of Manjaro, uses two separate but similarly named header packages. One is linux-api-headers which is sanitized for use in userspace. This package is utilized when the user tries to compile their own libraries, like glibc for example. What you need to know is that this API package has nothing to do with the Linux kernel headers themselves. The two are independent of each other and may very well be at different versions. In this guide, we’re working with kernel headers, and not the API headers. ","date":"2020-02-06","objectID":"/kernel/:3:0","tags":["kernel","manjaro"],"title":"Manjaro Linux kernel headers installation","uri":"/kernel/"},{"categories":["Linux"],"content":"Check kernel header version You can see the version of your system’s current kernel headers by opening a terminal and executing the following command. This instructs pacman to retrieve a list of all the packages installed on your system and grep specifically for headers. [yanboyang713@boyang ~]$ pacman -Q | grep headers linux-api-headers 5.8-1 We have a fresh install of Manjaro on our test system and pacman shows us that there aren’t any kernel headers currently installed. Again, the linux-api-headers package is not the kernel headers. If your system does have kernel headers installed, you’ll see the version in that output. ","date":"2020-02-06","objectID":"/kernel/:4:0","tags":["kernel","manjaro"],"title":"Manjaro Linux kernel headers installation","uri":"/kernel/"},{"categories":["Linux"],"content":"Install or update kernel headers Generally, the version of our system’s kernel headers should correspond to the version of the kernel itself. This may not be the case if you’re having compatibility issues and want to install an older version or if you need to test out the newest kernel header packages by installing a more upgraded version. Open a terminal and type the following command to check the version of your Manjaro system’s kernel: [yanboyang713@boyang ~]$ uname -r 5.9.11-3-MANJARO As you’ll see in the output, the version of the kernel on our test system is 5.9.11-3. The first two numbers are what’s important here, so let’s remember 5.9. [yanboyang713@boyang ~]$ sudo pacman -S linux-headers [sudo] password for yanboyang713: :: There are 11 providers available for linux-headers: :: Repository core 1) linux414-headers 2) linux419-headers 3) linux44-headers 4) linux49-headers 5) linux510-headers 6) linux54-headers 7) linux57-headers 8) linux58-headers 9) linux59-headers :: Repository community 10) linux54-rt-headers 11) linux59-rt-headers Enter a number (default=1): 9 resolving dependencies... looking for conflicting packages... Packages (1) linux59-headers-5.9.11-3 Total Download Size: 9.68 MiB Total Installed Size: 48.25 MiB :: Proceed with installation? [Y/n] Y :: Retrieving packages... linux59-headers-5.9.11-3-x86_64 9.7 MiB 4.14 MiB/s 00:02 [##################################] 100% (1/1) checking keys in keyring [##################################] 100% (1/1) checking package integrity [##################################] 100% (1/1) loading package files [##################################] 100% (1/1) checking for file conflicts [##################################] 100% (1/1) checking available disk space [##################################] 100% :: Processing package changes... (1/1) installing linux59-headers [##################################] 100% :: Running post-transaction hooks... (1/3) Arming ConditionNeedsUpdate... (2/3) Updating module dependencies... (3/3) Install DKMS modules ==\u003e dkms install --no-depmod -m v4l2loopback -v 0.12.5 -k 5.9.11-3-MANJARO ==\u003e depmod 5.9.11-3-MANJARO Now, let’s update our kernel headers to be on par with the kernel itself. Or, if you don’t have kernel headers installed at all, this command will install them for you. Type the following command in your terminal: If you already have kernel headers installed, pacman will proceed to update them. Otherwise, pacman will present you with a few options. Unless you have a unique circumstance, we want to choose from among the repository core options. We have seven different choices here. As discussed, there are a few situations where you may want to install older kernel headers, but generally, you’ll want them on the same version as the kernel itself. Since our test machine is running kernel 5.9.x, we’ll choose option 6, which installs the linux59-headers package. Check again to see that the package was installed correctly. [yanboyang713@boyang ~]$ sudo pacman -Q | grep headers linux-api-headers 5.8-1 linux59-headers 5.9.11-3 You may need to reboot your system to start using the kernel headers. $ reboot ","date":"2020-02-06","objectID":"/kernel/:5:0","tags":["kernel","manjaro"],"title":"Manjaro Linux kernel headers installation","uri":"/kernel/"},{"categories":["Linux"],"content":"Switch between kernel header versions If you’re looking to switch kernel header versions, you have the option to either upgrade or downgrade. We covered upgrading above, but here’s the command again: $ sudo pacman -S linux-headers To downgrade, you need to remove the current version of your installed headers, and then you can use pacman to install an older version. For example, to remove our 5.6 headers: $ sudo pacman -R linux56-headers Next, issue the following command and choose which version you’d like to use: $ sudo pacman -S linux-headers You’ll have the option to install various header versions from either the repository core or community repository. ","date":"2020-02-06","objectID":"/kernel/:6:0","tags":["kernel","manjaro"],"title":"Manjaro Linux kernel headers installation","uri":"/kernel/"},{"categories":["Linux"],"content":"Conclusion In this guide, we saw how to install, upgrade, and switch between kernel header versions on Manjaro Linux. We also discussed the important Arch-unique caveat about the linux-api-headers package which causes a bit of confusion to the inexperienced. ","date":"2020-02-06","objectID":"/kernel/:7:0","tags":["kernel","manjaro"],"title":"Manjaro Linux kernel headers installation","uri":"/kernel/"},{"categories":["Linux"],"content":"I3 WM ","date":"2020-02-06","objectID":"/manjaro/:1:0","tags":["Manjaro"],"title":"Manjaro Linux Tips","uri":"/manjaro/"},{"categories":["Linux"],"content":"Screenshot Requirements maim xclip Set-up Set this on your i3 config file ~/.i3/config ## Screenshots bindsym Print exec --no-startup-id maim \"/home/$USER/Pictures/$(date)\" bindsym $mod+Print exec --no-startup-id maim --window $(xdotool getactivewindow) \"/home/$USER/Pictures/$(date)\" bindsym Shift+Print exec --no-startup-id maim --select \"/home/$USER/Pictures/$(date)\" ### Clipboard Screenshots bindsym Ctrl+Print exec --no-startup-id maim | xclip -selection clipboard -t image/png bindsym Ctrl+$mod+Print exec --no-startup-id maim --window $(xdotool getactivewindow) | xclip -selection clipboard -t image/png bindsym Ctrl+Shift+Print exec --no-startup-id maim --select | xclip -selection clipboard -t image/png NOTE: You may remove the default screenshot shortcuts to prevent error. What it do Feature Shortcut: Full Screen PrtScrn Selection Shift + PrtScrn Active Window Super + PrtScrn Clipboard Full Screen Ctrl + PrtScrn Clipboard Selection Ctrl + Shift + PrtScrn Clipboard Active Window Ctrl + Super + PrtScrn All the screen shots are saved on ~/Pictures/CURRENT_DATE super key is the windows key ","date":"2020-02-06","objectID":"/manjaro/:1:1","tags":["Manjaro"],"title":"Manjaro Linux Tips","uri":"/manjaro/"},{"categories":["Blog"],"content":"Overview This is a tutorial on creating a blog or website using hexo.io (a static site generator). We will use Docker as our development environment. Also, we will storge our Hexo development code in Github main branche and all of our generate static pages will hosting in Github Page on gh-pages branches with your own custom domain and https. ","date":"2020-02-05","objectID":"/hexowithdocker/:1:0","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Why we choose Hexo After comparing a multitude of offerings, I settled on the excellent hexo.io platform, ‘A fast, simple \u0026 powerful blog framework’. The great thing about hexo, is that your blog/site is generated as a collection of static files. You don’t need anything other than a decent web server to host your page and given that it’s a static site, it reduces security concerns significantly in comparison to other popular blogging platforms. With the use of Hexo, you’ve also got a means that you can work on your blog locally, without internet connectivity. So, before we public our blog, we can check on our local host. The platform is built with node.js and subsequently, there are for it’s configuration, a variety of modules that need to be installed through npm. Whilst this is not that difficult in itself, it is a process that can result in a lot of installation residual on ones system. For this reason, we’re going to be using Docker to encapsulate the main blogging component that will run our system, keeping our own system shipshape and shiny. Github, offers a great solution for hosting sites with it’s pages.github.io service, all you need to have is a github account to make use of this. You can even, configure the service to leverage your own domain name, with Github taking care of aspects like https for you for free. ","date":"2020-02-05","objectID":"/hexowithdocker/:2:0","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Prerequisites You will need to have the following: Docker installation. Docker is available for Mac/Windows/Linux and is easily installed. You can following the link at the below for install Docker. {% post_link dockerInstall %} Domain Name (optional). I will be using a domain name for my configuration. If you wish to follow along fully, you’ll need a domain name but, you can just as easily stick with the free subdomain provided by Github pages, i.e. name.github.io, and skip the domain name related aspects in this tutorial. If you don’t have a domain name but wish to purchase one, namecheap is an excellent provider and fits in well with this tutorial. Github account, sign up for free on github.com In the subsequent sections, we will be covering the complete setup of https://yanboyang.com ","date":"2020-02-05","objectID":"/hexowithdocker/:3:0","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Set-up ","date":"2020-02-05","objectID":"/hexowithdocker/:4:0","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"On Github, create a repository called something.github.io In my case, I created yanboyang.github.io ","date":"2020-02-05","objectID":"/hexowithdocker/:4:1","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Clone this repository to Local [yanboyang713@boyang Documents]$ git clone https://github.com/yanboyang713/yanboyang.github.io.git Cloning into 'yanboyang.github.io'... remote: Enumerating objects: 760, done. remote: Counting objects: 100% (760/760), done. remote: Compressing objects: 100% (148/148), done. remote: Total 760 (delta 289), reused 743 (delta 276), pack-reused 0 Receiving objects: 100% (760/760), 4.13 MiB | 1.88 MiB/s, done. Resolving deltas: 100% (289/289), done. ","date":"2020-02-05","objectID":"/hexowithdocker/:4:2","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Docker image build docker build -t hexo . NOTE: If you face “error checking context: ‘can’t stat ‘/home/yanboyang713/Documents/yanboyang.github.io/.ssh’’.”, this means: Docker client does not have sufficient permissions to read some of the files in the context You can run: sudo chmod -R 777 .ssh/ ","date":"2020-02-05","objectID":"/hexowithdocker/:4:3","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Docker Pull the spurin/hexo Image On Docker Hub there is a pre-made image made by spurin, containing the steps to install both Hexo and Hexo-Admin. More details about the image can be reviewed here. With docker installed, from a command prompt/terminal, issue the following command to pull the latest version of the image. docker pull spurin/hexo ","date":"2020-02-05","objectID":"/hexowithdocker/:4:4","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Create a container for your site Access to your Local repository Folder. [yanboyang713@boyang yanboyang.github.io]$ docker create --name=yanboyang.com \\ \u003e -e HEXO_SERVER_PORT=4001 \\ \u003e -e GIT_USER=\"Boyang Yan\" \\ \u003e -e GIT_EMAIL=\"yanboyang713@gmail.com\" \\ \u003e -v /home/yanboyang713/Documents/yanboyang.github.io:/app \\ \u003e -p 4001:4001 \\ \u003e spurin/hexo 782be92ed7722d81f496c06b5cd350f38b570679ccd444c0cf07fdfbdcba24d6 [yanboyang713@boyang yanboyang.github.io]$ docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 782be92ed772 spurin/hexo \"docker-entrypoint.s…\" 10 seconds ago Created yanboyang.com ","date":"2020-02-05","objectID":"/hexowithdocker/:4:5","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Start yanboyang.com docker start yanboyang.com \u0026\u0026 docker logs --follow yanboyang.com docker exec -it yanboyang.com bash We can press Ctrl+C at this point without concern as we’re following the logs, the container is running in the background and thus our Ctrl+C stops the following of the logs, not the actual container. I’m performing this on my laptop so Docker is localhost, adjust the name/ip address accordingly if you’re running Docker elsewhere. If you browse to http://localhost:4001, you’ll be presented with a hexo starting page as follows You can also, access the admin interface for hexo-admin at http://localhost:4001/admin - ","date":"2020-02-05","objectID":"/hexowithdocker/:4:6","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Customising the Theme This theme may suit your needs but personally, I am a big fan of the Next theme. The next steps cover the installation of this particular theme. Each theme will have their own requirements but this give an overall idea of the process. Execute a bash shell in the running container, giving you a prompt similar to the following - We will clone the theme from it’s source github repository, and whilst we’re in the shell, we’re going to rename the default configuration file to the expected name. Although we’re doing this in the shell, we’re in the app volume so all changes here are persistent. Technically, we could do this directly in the app volume on our local OS if that is more convenient. a [yanboyang713@boyang newBlog]$ docker exec -it yanboyang.com bash root@6e0c33f789be:/app# ls _config.landscape.yml db.json package-lock.json scaffolds themes _config.yml node_modules package.json source yarn.lock root@6e0c33f789be:/app# git clone https://github.com/theme-next/hexo-theme-next themes/next Cloning into 'themes/next'... remote: Enumerating objects: 12564, done. remote: Total 12564 (delta 0), reused 0 (delta 0), pack-reused 12564 Receiving objects: 100% (12564/12564), 8.00 MiB | 2.59 MiB/s, done. Resolving deltas: 100% (7996/7996), done. root@6e0c33f789be:/app# ls _config.landscape.yml db.json package-lock.json scaffolds themes _config.yml node_modules package.json source yarn.lock root@6e0c33f789be:/app# exit exit ","date":"2020-02-05","objectID":"/hexowithdocker/:4:7","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Blog Configuration The next change, is to our blog configuration file which again, is now accessible in two ways In the container under /app/_config.yml Outside of the container in the local directory, in my case this would be /home/yanboyang713/Documents/newBlog_config.yml NOTE: If you could NOT change source folder and _post folder. Please, sudo chmod 777 ./source/ sudo chmod 777 _posts/ sudo chmod 777 ./next/ ","date":"2020-02-05","objectID":"/hexowithdocker/:4:8","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Add Requirements in the app volume To reiterate, when we created the container, we specified the volume location so these files are accessible both inside the container (via an interactive shell) and outside of the container (via your filesystem). If you wish to make changes inside the container using an editor like vim, you will need to install an editor as the container is built from a lightweight image that has no default editors (i.e. apt update \u0026\u0026 apt -y install vim), n.b. this change is to the running container and won’t persist should the container be destroyed/recreated). For this, we’re going to need some additional npm requirements, fortunately, the image has been built to accomodate this. Execute a shell, and then add the requirement to requirements.txt in the app volume. [yanboyang713@boyang newBlog]$ docker exec -it yanboyang.com bash root@6e0c33f789be:/app# ls _config.landscape.yml _config.yml.old node_modules package.json scaffolds themes _config.yml db.json package-lock.json public source yarn.lock root@6e0c33f789be:/app# echo hexo-deployer-git \u003e\u003e requirements.txt root@6e0c33f789be:/app# exit exit After making these change, if you’re in the running container, exit and then restart the Docker container. [yanboyang713@boyang newBlog]$ docker restart yanboyang.com \u0026\u0026 docker logs --follow yanboyang.com ","date":"2020-02-05","objectID":"/hexowithdocker/:4:9","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Using the SSH Deploy Key We’re going to use SSH keys to deploy our site to Github, this is conveniently done as part of the application generation and we can use the SSH key generated, i.e. ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCkbxMRt/BuilZBGwBNMEdjF7Xny+D/DF+X/Ee+Nbq/UnfX3vTL1EovCxPeX8QWDmcYkkSchNK4V3Cvw5dlTnEZEvyUXCsqAEjTC+wfpAJ2cRMiApPgqg4Px0OyBA4dI5nxs03tMVmRk4B+LxLLS4QVhXL/R4eOnYl+he3lQ8ouzqb7nFKCg5psohvEvEtaSib3AKVE/eRSgevN34PDKrSFdxemQDne9JrSPH7nFgQLfmYt/MYkAxkWur1bjopfkS/KvNrh/jN493TZ+BB6onBYdXy9eNvNQrJGTD7HuiF2IBhmTAXQdeXQEHOmTjhygfn17AnU8TwkE7RpcV1BLtc3 root@782be92ed772 n.b. this is an example only and has not been used, outside of this tutorial, you will need to use your own SSH key. ","date":"2020-02-05","objectID":"/hexowithdocker/:4:10","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Adding the Deployment Keys to Github Whilst in the repository, we can add the deployment keys that we created. In Settings, choose Deploy Keys. Select Add deploy key And paste the key we created earlier NOTE: Please, check “Allow write sccess” If successful, you should see something similar to this ","date":"2020-02-05","objectID":"/hexowithdocker/:4:11","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Configuring a domain name using Github and DNS record If you wish to host your site with a custom domain, in the project settings in the github repository, you can enable as follows. We need add 4 A record and one CNAME. The example at the below. ","date":"2020-02-05","objectID":"/hexowithdocker/:5:0","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Configure CNAME in Hexo and Deploy Settings Earlier on, when we set a custom CNAME in Github, it created a file called CNAME in the repository, we need to ensure that this file is in our source on Hexo, otherwise, this will be removed during deployment, breaking the cname configuration for our site. Create an equivalent file in the source directory - $ docker exec -it yanboyang.com bash root@b3feb661344f:/app# echo www.yanboyang.com \u003e source/CNAME exit Edit _config.yml and update the deployment section with your own github settings. ## Deployment ### Docs: https://hexo.io/docs/deployment.html deploy: type: git repo: https://github.com/yanboyang713/yanboyang.github.io.git branch: gh-pages message: \"Site updated\" NOTE: In your requirements.txt file add hexo-deployer-git ","date":"2020-02-05","objectID":"/hexowithdocker/:5:1","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Create a new blog post $ docker exec yanboyang.com bash -c 'hexo new post \"chineseInputMethod\"' ","date":"2020-02-05","objectID":"/hexowithdocker/:5:2","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Generating content In preparation for the launch of the site, we can request that Hexo generates the content for us $ docker exec yanboyang.com hexo clean $ docker exec yanboyang.com hexo generate ","date":"2020-02-05","objectID":"/hexowithdocker/:5:3","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Finally, Deploy the site $ docker exec -it yanboyang.com hexo deploy ","date":"2020-02-05","objectID":"/hexowithdocker/:5:4","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Add gitignore for local public [yanboyang713@boyang yanboyang.github.io]$ echo /public/* \u003e\u003e .gitignore [yanboyang713@boyang yanboyang.github.io]$ cat .gitignore /public/* ","date":"2020-02-05","objectID":"/hexowithdocker/:5:5","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Commit and Push develop code to Github main branch git add * git commit -m \"init main branch\" git push ","date":"2020-02-05","objectID":"/hexowithdocker/:5:6","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Tips for Hexo https://github.com/hexojs/hexo/wiki/Breaking-Changes-in-Hexo-3.0 ","date":"2020-02-05","objectID":"/hexowithdocker/:6:0","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Future Changes (Generate and Deploy) From this point forward, you can quite simply use the local Hexo-Admin interface for updates, and when ready, push changes to your live site with a one liner as follows. $ docker exec yanboyang.com bash -c 'hexo clean \u0026\u0026 hexo generate \u0026\u0026 hexo deploy' ","date":"2020-02-05","objectID":"/hexowithdocker/:6:1","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Generate Atom 1.0 or RSS 2.0 feed https://github.com/hexojs/hexo-generator-feed In the requirements.txt add hexo-generator-feed ","date":"2020-02-05","objectID":"/hexowithdocker/:6:2","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"adds an anchor to every heading https://github.com/theme-next/hexo-theme-next-anchor In the requirements.txt add hexo-theme-next-anchor ","date":"2020-02-05","objectID":"/hexowithdocker/:6:3","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Embed a YouTube/Vimeo video YouTube Inserts a YouTube video. {% youtube video_id [type] [cookie] %} Examples Embed a video {% youtube lJIrF4YjHfQ %} Embed a playlist {% youtube PL9hW1uS6HUfscJ9DHkOSoOX45MjXduUxo 'playlist' %} Enable privacy-enhanced mode YouTube’s cookie is not used in this mode. {% youtube lJIrF4YjHfQ false %} {% youtube PL9hW1uS6HUfscJ9DHkOSoOX45MjXduUxo 'playlist' false %} Vimeo Inserts a responsive or specified size Vimeo video. {% vimeo video_id [width] [height] %} ","date":"2020-02-05","objectID":"/hexowithdocker/:6:4","tags":["Hexo","Blog"],"title":"Creating a Hexo blog with Docker","uri":"/hexowithdocker/"},{"categories":["Blog"],"content":"Installation Hugo yay -S hugo Step 1: Install Hugo yay -S hugo git clone https://github.com/yanboyang713/yanboyang713.github.io.git ## https://gohugo.io/commands/hugo_new_site/ hugo new site yanboyang713.github.io --force Step 2: Create a New Site [yanboyang713@boyang ~]$ hugo new site blog Congratulations! Your new Hugo site is created in /home/yanboyang713/blog. Just a few more steps and you're ready to go: 1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/ or create your own with the \"hugo new theme \u003cTHEMENAME\u003e\" command. 2. Perhaps you want to add some content. You can add single files with \"hugo new \u003cSECTIONNAME\u003e/\u003cFILENAME\u003e.\u003cFORMAT\u003e\". 3. Start the built-in live server via \"hugo server\". Visit https://gohugo.io/ for quickstart guide and full documentation. Step 3: Add a Theme cd blog git init git submodule add https://github.com/dillonzq/LoveIt.git themes/LoveIt Step 4: Add Some Content You can manually create content files (for example as content//.) and provide metadata in them, however you can use the new command to do a few things for you (like add title and date): hugo new posts/my-first-post.md Step 5: Start the Hugo server Now, start the Hugo server with drafts enabled: Build Hugo With GitHub Action Add ssh deploy key Generate your deploy key with the following command. ```bash ssh-keygen -t rsa -b 4096 -C \"$(git config user.email)\" -f gh-pages -N \"\" Next, Go to Repository Settings Go to Deploy Keys and add your public key with the Allow write access Go to Secrets and add your private key as ACTIONS_DEPLOY_KEY ","date":"2020-02-05","objectID":"/hugo/:1:0","tags":["Hugo","Blog"],"title":"Creating a Hugo blog","uri":"/hugo/"},{"categories":["Blog"],"content":"Set up your Google Analytics account Head over to https://analytics.google.com/ and make an account (or sign in with your Google account). Set up your “Property”, give it a name, and point it to the URL of the site you plan on tracking. Click through the basic options until you land on a page with a Tracking Code. This is the value we need to be keeping track of. If you’re having trouble with this step then I recommend checking out this tutorial, which has very detailed step by step instructions for configuring your GA account. ","date":"2020-02-05","objectID":"/hugo/:2:0","tags":["Hugo","Blog"],"title":"Creating a Hugo blog","uri":"/hugo/"},{"categories":["Blog"],"content":"Configuring Hugo Hugo has a built in template for Google Analytics. All we need to do is make sure the template gets included in all our pages, so we can have more detailed tracking. ","date":"2020-02-05","objectID":"/hugo/:2:1","tags":["Hugo","Blog"],"title":"Creating a Hugo blog","uri":"/hugo/"},{"categories":["Blog"],"content":"header.html The easiest way to do this is to insert the template into the header.html file used by your Hugo theme of choice. Since it is common to use a git submodule as a theme directory, it could be undesirable and messy to write changes directly into the submodule. The solution to this is to create another directory layouts in the root of your hugo directory. Hugo looks at the contents of this folder and uses it to overwrites the styles defined by your theme. For example I use the Terminal theme. The header file is located at /themes/terminal/layouts/partials/header.html We can copy this file to our new directory, so it sits at /layouts/partials/header.html. Next we can edit our header.html file to include the template for google analytics. Your header.html will look different depending on the theme you’re using, but there should be a section at the top. If not we can simply create it. \u003chead\u003e {{ template \"_internal/google_analytics.html\" . }} {{ template \"_internal/google_analytics_async.html\" . }} \u003c/head\u003e Adding these two lines inside will make the google tracking code embed in the HTML of all of your pages. config.toml Finally we need to use that tracking code from earlier. At the top level of your config.toml, add the line googleAnalytics = \"UA-302012394-1\" Replacing the string with your Google Analytics tracking code. After you rebuild the site, everything should work as expected! You should also be able to detect the traffic by running the server locally as well, before you push to production. Note! I had quite the time figuring out why I couldn’t see any activity on my site. Being the paranoid privacy nut that I am, I have a multitude of tracker/script blocker plugins on my browsers, to block nefarious ads/scripts. This also blocks the tracking I do want, wouldn’t you know. I switched over to a private window without plugins and the traffic was instantly visible! ","date":"2020-02-05","objectID":"/hugo/:2:2","tags":["Hugo","Blog"],"title":"Creating a Hugo blog","uri":"/hugo/"},{"categories":null,"content":"Steps Enable developer mode Go to Settings Click “About phone” 4 Click on “Build Number” - until developer mode enable When it is done, you will see “You are now a developer” Go back to Settings click System Click “Developer options” Go to MEDIA section Enable USB audio routing - DONE ","date":"2020-02-05","objectID":"/enableusbsoundcardonandroid/:1:0","tags":null,"title":"Enable USB Sound Card On Android Phone","uri":"/enableusbsoundcardonandroid/"},{"categories":null,"content":"Introduction A Kubernetes Cluster is a group of node or machines running together. At the highest level of Kubernetes, there exist two kinds of servers, a Master and a Worker node. These servers can be Virtual Machine (VM) or physical servers (Bare metal). Together, these servers form a Kubernetes cluster and are controlled by the services that make up the Control Plane. Kubernetes development started by focusing on the key features of an orchestrator, such as replication of an application with load balancing and service discovery, followed by basic health checks and repair features to ensure availability. Helm, a package manager for Kubernetes, was launched in early 2016, aimed at simplifying how one defines, installs, and upgrades complex Kubernetes applications. Minikube brought the Kubernetes environment to a developer’s local system. In this blog, I will cover How to install and configure a three-node cluster in Kubernetes which is the first topic in Kubernetes. ","date":"2020-02-05","objectID":"/k8s/:1:0","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"K8s Architecture/Components Kubernetes follows a client-server architecture where the master is installed on a machine and nodes are distributed across multiple machines accessible via the master. The K8s master and K8s workers are part of the Kubernetes control plane, whereas the container registry may lie outside of the control plane. ","date":"2020-02-05","objectID":"/k8s/:2:0","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Kubernetes Master The Kubernetes master is the main node responsible for managing the entire cluster. The orchestration of the K8s workers is handled by this node. This node is replicable to avoid any single point of failure. The control panel accesses the master only to make modifications to the cluster. The master comprises four major components. API server: This is the front end of a Kubernetes control plane. It maintains RESTful web services to define and configure a Kubernetes cluster. etcd: This is a highly available component maintaining a record of all the objects running in the system. Any changes in the configuration of Kubernetes are stored here, and the changes are allowed to be watched for immediate action. Scheduler: This schedules workloads on Kubernetes workers in the form of pods. We will cover pods in the next section. The scheduler reads through the resource requirements of each pod and distributes the pods throughout the cluster based on availability. By default, it also tries to distribute pod replicas to different nodes to maintain high availability. Controller manager: This runs controllers in the background that are responsible for different important tasks in the cluster. Controllers keep watch on etcd for configuration changes and take the cluster to the desired state; on the other end, the control loops watch for the changes in the cluster and work to maintain the desired state as per etcd. Let’s visit a few controller examples to understand what controllers do in the cluster. Node controller: This monitors the nodes in the cluster and responds when a node comes up or goes down. This is important so the scheduler can align pods per the availability of a node and maintain state per etcd. Endpoint controller: This joins services and pods by creating endpoint records in the API, and it alters the DNS configuration to return an address pointing to one of the pods running the service. Replication controller: Replication is a general practice to maintain the high availability of an application. The replication controller makes sure the desired number of pod replicas/copies is running in the cluster. ","date":"2020-02-05","objectID":"/k8s/:2:1","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Kubernetes Workers (also cald minions or node) The actual application runs on worker nodes. Each node has three major components. Kubelet: Kubelet is the primary node agent running on each node and monitoring that the containers on the node are running and healthy. Kubelet takes a set of PodSpecs, which is a YAML or JSON object describing a pod, and monitors the containers described in those specs only. Note that there can be other containers, other than the containers listed in PodSpecs, running on the node, but Kubelet does not monitor these containers. Kube-proxy: The Kubernetes master scheduler usually runs multiple services on a node. Kube-proxy creates a network proxy and load balancer for these services. It can do simple TCP, UDP, and SCTP stream forwarding or round-robin TCP, UDP, and SCTP forwarding across a set of back ends. It also allows, if configured, nodes to be exposed to the Internet. Pods: A pod is the smallest unit of the Kubernetes object model that can be created, deployed, or destroyed. A Kubernetes pod usually has a single container but is allowed to contain a group of tightly coupled containers as well. A pod represents a running process on a cluster. It can be used in two broad ways. Single-container pod: This was the most common Kubernetes use case, also called one container per pod. The pod wraps the container and provides an abstract layer to Kubernetes to access or modify the container. Multiple-container pod: There are scenarios when an application requires multiple tightly coupled containers that are sharing resources. In such scenarios, a pod builds a wrapper on these containers and treats them as a single service. An example would be one container serving REST APIs to end users, with a sidecar counting the number of requests implementing the API limitation. The containers inside a pod share the same IP that was given to the pod and share the same set of storage. Containers, as stated earlier, deployed inside each pod run the service. The container packaging and storage depend on the container runtime and registry. Container runtime: To understand this, let’s try to understand what a container is. A container is a unit of code packaged with its dependencies that creates an artifact that can run quickly on different computing environments. The container runtime lets someone run containers by providing a basic set of resources and libraries, which combined with the container’s package boots up an application. An application in a container has the liberty of its own environment including storage, network, etc., with the restriction of how much of each resource can be used. The container runtime also manages container images on a node. There are multiple container runtimes available, so let’s go through a couple of them. containerd CRI-O Docker NOTE: Which container runtime to choose is a matter of personal preference and also depends on how complex your codebase is and the kind of resources it depends on. Container registry: Each container generation requires code development, adding libraries from different package managers and creating the basic environment to run the code. A container can be built every time when deploying, but getting the latest code, getting new libraries, and preparing the environment every time is time-consuming. To simplify this, developers store their once-created container and use it whenever required. The container registry is the place that allows developers to save their container images and use them as and when required. Individual providers such as Azure, Docker, and Google have their own container registries that host images in a highly available environment with access-level restrictions. Kubernetes uses the Container Runtime Interface (CRI) to interact with the container runtime. Since Kubernetes 1.5, container runtimes are expected to implement CRI, which acts as a bridge between Kubernetes Kubelet and the container runtime. CRI provides an abstraction between Ku","date":"2020-02-05","objectID":"/k8s/:2:2","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Kubernetes Terminology There are a few terms that we may be using frequently. Let’s go through a few of them to avoid any confusion in future references. Deployment: A deployment is an abstract unit built on pods. To deploy an application or a microservice, one needs to run it inside a pod. To do so, a deployment configuration is created where one states what needs to be deployed along with the number of replicas of the application. On submitting this configuration to Kubernetes, a set of pods is spawned by the deployment controller deploying the application with the configured replicas. Image: An image is the software/container that will be deployed on the cluster. We will be using image interchangeably with Docker image. Kubectl: This is a CLI to interact with a Kubernetes cluster. We will be using this to deploy clusters, check the status of them, and update our clusters. Namespace: As the name suggests, this is used to group multiple virtual clusters on the same Kubernetes instance or organize the resources within the same cluster. It allows each resource to be identified uniquely. Replicaset: This is the same as a replication controller with an additional support of a set-based selector rather than an equality-based selector. Service: This is a description of how an application deployed on one or multiple pods can be accessed internally or externally. Since pods are not permanent and Kubernetes may relocate pods from time to time based on availability, relying on direct access to pods is not recommended. The service discovers the application running in pods and provides access to them via ports, load balancers, or other mechanisms. StatefulSet: This is similar to a deployment managing the ordering and uniqueness of the pods. In other words, if a pod dies, a new pod is spawned by the StatefulSet controller with the same identity and resources as the dead pod. ","date":"2020-02-05","objectID":"/k8s/:3:0","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Set Up a Kubernetes Cluster There are 3 ways to deploy a Kubernetes cluster: By deploying all the components separately. Using Kubeadm. Using Managed Kubernetes Services, such as Azure Kubernetes Service (AKS) Minikube In this blog, I will using Kubeadm deploy this Kubernetes cluster. ","date":"2020-02-05","objectID":"/k8s/:4:0","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Prerequisites For Cluster Setup I am using proxmox create three VM (one Master Node and 2 Worker Nodes) on my on-premises network. To create an Ubuntu Virtual Machine on proxmox, please, have a check my blog. The Ports specified below are the default Port range for NodePort Services for Master and Worker Nodes. Port numbers marked with * are overridable, so we have to make sure that any custom ports we provide are open. ","date":"2020-02-05","objectID":"/k8s/:5:0","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"The specifications required for a Node: One or more machines running a deb/rpm-compatible Linux OS; for example Ubuntu or CentOS. (Note: I am going to use Ubuntu in this setup.) 8 GiB or more of RAM per machine. At least 4 CPUs on the machine that you use as a control-plane node. ","date":"2020-02-05","objectID":"/k8s/:6:0","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Installing Docker, Kubectl, And Kubeadm Packages After doing the above-mentioned process, we have to install some packages on our machines. These packages are: Docker – is a software responsible for running the containers. kubeadm – a CLI tool that will install and configure the various components of a cluster in a standard way. kubelet – a system service/program that runs on all nodes and handles node-level operations. kubectl – a CLI tool used for issuing commands to the cluster through its API Server. In order to install these packages, follow the steps mentioned below on Master as well as Worker nodes: ","date":"2020-02-05","objectID":"/k8s/:7:0","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Step 1 We have to do SSH to our virtual machines with the username and password. If you are a Linux or Mac user then use ssh command and if you are a Windows user then you can use Putty. $ ssh root@publicipaddress ","date":"2020-02-05","objectID":"/k8s/:7:1","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Step 2 If you have logged in using another user, then we can switch to root user using the following command: $ sudo su ","date":"2020-02-05","objectID":"/k8s/:7:2","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Step 3 Now we will install the Docker package in our machine. To Install Docker on the local system, you can check out the following blog. https://www.yanboyang.com/2020/10/12/dockerInstall/#Install-Docker ","date":"2020-02-05","objectID":"/k8s/:7:3","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Step 4 We have to download packages like Kubeadm and kubectl using https, so we have to download the https package first. Use the following command for this: $ apt-get update \u0026\u0026 apt-get install -y apt-transport-https ","date":"2020-02-05","objectID":"/k8s/:7:4","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Step 5 We have to add the required GPG key to our apt-sources to authenticate the Kubernetes related packages we will install: $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - ","date":"2020-02-05","objectID":"/k8s/:7:5","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Step 6 To add Kubernetes to the package manager’s list of resources, use the following command: (Note: Type the following command carefully) root@k8s-master-ct:~# cat \u003c\u003cEOF | sudo tee /etc/apt/sources.list.d/kubernetes.list \u003e deb https://apt.kubernetes.io/ kubernetes-xenial main \u003e EOF deb https://apt.kubernetes.io/ kubernetes-xenial main ","date":"2020-02-05","objectID":"/k8s/:7:6","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Step 7 Before installing the packages, update the apt-get command: $ apt-get update ","date":"2020-02-05","objectID":"/k8s/:7:7","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Step 8 Now we will install Kubelet, Kubeadm, and kubectl packages into our machines. Run the following command: $ apt-get install -y kubelet kubeadm kubectl ","date":"2020-02-05","objectID":"/k8s/:7:8","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Step 9 To hold the installed packages at their installed versions, use the following command: $ apt-mark hold kubelet kubeadm kubectl ","date":"2020-02-05","objectID":"/k8s/:7:9","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Create A Kubernetes Cluster As we have successfully installed Kubeadm, next we will create a Kubernetes cluster using the following mentioned steps: ","date":"2020-02-05","objectID":"/k8s/:8:0","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Step 1 We have to initialize kubeadm on the master node. This command will check against the node that we have all the required dependencies. If it is passed, then it will install control plane components. (Note: Run this command in Master Node only.) $ kubeadm init NOTE: [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.1. Latest validated version: 19.03 apt-get remove docker-ce docker-ce-cli containerd.io then sudo apt-cache policy docker-ce docker-ce-cli containerd.io this will show the list of docker packages available apt-get install docker-ce=5:19.03.14~3-0~ubuntu-focal docker-ce-cli=5:19.03.14~3-0~ubuntu-focal containerd.io [ERROR Swap]: running with swap on is not supported. Please disable swap swapoff -a sudo swapoff -a is not persistent across reboot. You may disable swap after reboot by just commenting out (add # in front of the line) the swap entry in /etc/fstab file. It will prevent swap partition from automatically mounting after a reboot. Steps: Open the file /etc/fstab Look for a line below swap was on /dev/sda5 during installation UUID=e7bf7d6e-e90d-4a96-91db-7d5f282f9363 none swap sw 0 0 Comment out above line with # and save it. It should look like below swap was on /dev/sda5 during installation ##UUID=e7bf7d6e-e90d-4a96-91db-7d5f282f9363 none swap sw 0 0 Reboot the system or for current session execute “sudo swapoff -a” to avoid reboot. If this is a lxc container in proxmox, you can set Swap to 0. [ERROR SystemVerification]: failed to parse kernel config: unable to load kernel module: “configs”, output: “modprobe: FATAL: Module configs not found in directory /lib/modules/5.4.78-2-pve\\n”, err: exit status 1 apt-get install linux-image-$(uname -r) ","date":"2020-02-05","objectID":"/k8s/:8:1","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Minikube Minikube – a utility to run a Kubernetes cluster locally on your PC. It can use Virtualbox, VMware, Hyper-V etc hypervisors which will be used to create a virtual machine with a Kubernetes cluster. Minikube is a great tool for developers or DevOps engineers to test deployments/services etc without the need to create and conigure a real cluster. A quick HowTo install it and run a Kubernetes pod using the Minikube. ","date":"2020-02-05","objectID":"/k8s/:9:0","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Installing In manjaro can be installed from AUR: yay -S minikube If you haven’t Virtualbox yet – install it minikube can be deployed as a VM, a container, or bare-metal. To do so, we use the Docker Machine library to provide a consistent way to interact with different environments. VirtualBox is minikube’s original driver. It may not provide the fastest start-up time, but it is the most stable driver available for users of Microsoft Windows Home. [yanboyang713@boyang ~]$ yay -S virtualbox [sudo] password for yanboyang713: resolving dependencies... :: There are 10 providers available for VIRTUALBOX-HOST-MODULES: :: Repository extra 1) linux414-virtualbox-host-modules 2) linux419-virtualbox-host-modules 3) linux44-virtualbox-host-modules 4) linux49-virtualbox-host-modules 5) linux510-virtualbox-host-modules 6) linux54-virtualbox-host-modules 7) linux59-virtualbox-host-modules :: Repository community 8) linux54-rt-virtualbox-host-modules 9) linux59-rt-virtualbox-host-modules 10) virtualbox-host-dkms NOTE: You virtualbox should match your linux kernel. If you don’t know your kernel version. Please, open a terminal and type the following command to check the version of your Manjaro system’s kernel: [yanboyang713@boyang ~]$ uname -r 5.9.11-3-MANJARO minikube will work with Kubernetes via kubectl – install it as well: yay -S kubectl ","date":"2020-02-05","objectID":"/k8s/:9:1","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Start Minikube Start Minikube itself [yanboyang714@boyang ~]$ minikube start --driver=virtualbox 😄 minikube v1.16.0 on Arch 20.2.1 ✨ Using the virtualbox driver based on user configuration 💿 Downloading VM boot image ... \u003e minikube-v1.16.0.iso.sha256: 65 B / 65 B [-------------] 100.00% ? p/s 0s \u003e minikube-v1.16.0.iso: 212.62 MiB / 212.62 MiB [] 100.00% 25.68 MiB p/s 8s 👍 Starting control plane node minikube in cluster minikube 🔥 Creating virtualbox VM (CPUs=2, Memory=3900MB, Disk=20000MB) ... 🐳 Preparing Kubernetes v1.20.0 on Docker 20.10.0 ... ▪ Generating certificates and keys ... ▪ Booting up control plane ... ▪ Configuring RBAC rules ... 🔎 Verifying Kubernetes components... 🌟 Enabled addons: storage-provisioner, default-storageclass 🏄 Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default To make virtualbox the default driver: minikube config set driver virtualbox Special features minikube start supports some VirtualBox specific flags: –host-only-cidr: The CIDR to be used for the minikube VM (default “192.168.99.1/24”) –no-vtx-check: Disable checking for the availability of hardware virtualization Check Virtualbox VMs running VBoxManage list runningvms ","date":"2020-02-05","objectID":"/k8s/:9:2","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Pod Create a new pod [yanboyang713@boyang ~]$ kubectl run hello-minikube --image=gcr.io/google_containers/echoserver:1.4 --port=8080 deployment.apps/hello-minikube created List pods [yanboyang713@boyang ~]$ kubectl get pod NAME READY STATUS RESTARTS AGE hello-minikube 1/1 Running 0 2m32s Delete Pods [yanboyang713@boyang ~]$ kubectl delete pods hello-minikube pod \"hello-minikube\" deleted ","date":"2020-02-05","objectID":"/k8s/:9:3","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Deployment Creaye a new deployment kubectl create deployment hello-test --image=k8s.gcr.io/echoserver:1.10 List Deployment kubectl get deployment Delete Deployment kubectl delete deployments hello-test ","date":"2020-02-05","objectID":"/k8s/:9:4","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Service Run the service kubectl expose deployment hello-minikube --type=NodePort List service [yanboyang713@boyang ~]$ minikube service list |-------------|------------|--------------|-----| | NAMESPACE | NAME | TARGET PORT | URL | |-------------|------------|--------------|-----| | default | kubernetes | No node port | | kube-system | kube-dns | No node port | |-------------|------------|--------------|-----| Check the service itself curl $(minikube service hello-minikube --url) ","date":"2020-02-05","objectID":"/k8s/:9:5","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Dashboard [yanboyang713@boyang ~]$ minikube dashboard \u0026 [1] 9782 [yanboyang713@boyang ~]$ 🤔 Verifying dashboard health ... 🚀 Launching proxy ... 🤔 Verifying proxy health ... 🎉 Opening http://127.0.0.1:43353/api/v1/namespaces/kubernetes-dashboard:/proxy/ in your default browser... /usr/bin/xdg-mime: line 323: [: /home/yanboyang713/.local/share/appltor expected /usr/bin/xdg-mime: line 325: [: /home/yanboyang713/.local/share/applpected NOTE: If hangs on “Verifying proxy health” or show “Exiting due to SVC_URL_TIMEOUT: http://127.0.0.1:39481/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ is not accessible: Temporary Error: unexpected response code: 503” Please, check your Minikube driver set-up: https://minikube.sigs.k8s.io/docs/drivers/ ","date":"2020-02-05","objectID":"/k8s/:9:6","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Stop Minikube [yanboyang713@boyang ~]$ kubectl get pods -o wide No resources found in default namespace. [yanboyang713@boyang ~]$ minikube stop ✋ Stopping node \"minikube\" ... 🛑 Powering off \"minikube\" via SSH ... 🛑 1 nodes stopped. ","date":"2020-02-05","objectID":"/k8s/:9:7","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Run an Application on Kubernetes We have our Kubernetes cluster ready, so let’s try to deploy an application on it and understand how it happens. ","date":"2020-02-05","objectID":"/k8s/:10:0","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Application Details Let’s start by creating a simple web application in Python using Flask. The app structure should look like this: [yanboyang713@boyang WebApp]$ tree . . ├── app.py ├── Dockerfile └── requirement.txt 0 directories, 3 files Let’s create a project called WebApp with app.py inside to handle web requests: from flask import Flask app = Flask(__name__) @app.route(\"/\") def main(): return \"Welcome!\" if __name__ == \"__main__\": app.run(host='0.0.0.0') Let’s create a Dockerfile to containerize the application: FROMubuntu:18.04RUN apt-get update -y \u0026\u0026 apt-get install -y python-pip python-devCOPY ./requirement.txt /app/requirement.txtWORKDIR/appRUN pip install -r requirement.txtCOPY . /appENTRYPOINT [ \"python\" ]CMD [ \"app.py\" ] requirement.txt: flask ","date":"2020-02-05","objectID":"/k8s/:10:1","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Image Pushing and Building Pushing images to Minikube cluster The Docker environment and images stored are different for Minikube. Instead of storing images to our local environment, sending them to the registry, and bringing them back on Minikube, we will be storing the container image directly on the Minikube instance. There are five ways to push your image into a minikube cluster. Details: https://minikube.sigs.k8s.io/docs/handbook/pushing/#3-pushing-directly-to-in-cluster-crio-podman-env The default container-runtime on minikube is ‘docker’, so at the below, we will focue on Docker. The command minikube docker-env returns a set of Bash environment variable exports to configure your local environment to re-use the Docker daemon inside the Minikube instance. [yanboyang713@boyang WebApp]$ minikube docker-env export DOCKER_TLS_VERIFY=\"1\" export DOCKER_HOST=\"tcp://192.168.99.100:2376\" export DOCKER_CERT_PATH=\"/home/yanboyang713/.minikube/certs\" export MINIKUBE_ACTIVE_DOCKERD=\"minikube\" ## To point your shell to minikube's docker-daemon, run: ## eval $(minikube -p minikube docker-env) When using a container or VM driver (all drivers except none), you can reuse the Docker daemon inside minikube cluster. These variables will help your docker CLI (where you write docker commands) to connect with docker daemon in the VM created by minikube ! Now, to connect your Docker CLI to the docker daemon inside the VM you need to run the command at the below. This will temporarily(for that terminal) connect CLI to docker daemon inside the VM :) This means you don’t have to build on your host machine and push the image into a docker registry. You can just build inside the same docker daemon as minikube which speeds up local experiments. Run: eval $(minikube -p minikube docker-env) Passing this output through eval causes bash to evaluate these exports and put them into effect. You can review the specific commands which will be executed in your shell by omitting the evaluation step and running minikube docker-env directly. However, this will not perform the configuration – the output needs to be evaluated for that. so if you do the following commands, it will show you the containers inside the minikube, inside minikube’s VM or Container. Now, try to do docker ps , you can see all the containers created inside the VM (will be shown only if you have done some work in k8’s cluster) This is all possible due to those environment variables by docker-env docker ps Build the WebApp application container with the name web-app and assign version 1.0. docker build -t web-app:1.0 . This is a workflow optimization intended to improve your experience with building and running Docker images which you can run inside the minikube environment. It is not mandatory that you re-use minikube’s Docker daemon to use minikube effectively, but doing so will significantly improve the speed of your code-build-test cycle. In a normal workflow, you would have a separate Docker registry on your host machine to that in minikube, which necessitates the following process to build and run a Docker image inside minikube: Build the Docker image on the host machine. Re-tag the built image in your local machine’s image registry with a remote registry or that of the minikube instance. Push the image to the remote registry or minikube. (If using a remote registry) Configure minikube with the appropriate permissions to pull images from the registry. Set up your deployment in minikube to use the image. By re-using the Docker registry inside Minikube, this becomes: Build the Docker image using Minikube’s Docker instance. This pushes the image to Minikube’s Docker registry. Set up your deployment in minikube to use the image. Try to run minikube docker-env You will see some environment variables are mentioned there :) ","date":"2020-02-05","objectID":"/k8s/:10:2","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Deploy the Application Let’s create our first deployment configuration. This tells Kubernetes to create a container for our application. We create the webapp-deployment.yaml file for our webapp. apiVersion: apps/v1 kind: Deployment metadata: name: webapp-deployment labels: app: webapp spec: replicas: 1 selector: matchLabels: app: webapp template: metadata: labels: app: webapp spec: containers: - name: webapp image: web-app:1.0 imagePullPolicy: Never ports: - containerPort: 5000 Let’s try to understand the YAML file. apiVersion: This is the API version used to create this object. kind: This explains that we are creating a Kubernetes deployment. metadata: This specifies the name of deployment (a must-have key) and optional labels one may want to put on the deployment. replicas: This specifies the number of pods to be created for this deployment. selector: This is how the deployment manages to locate the pods. template.metadata: The pods created from this deployment will be named from these labels. containers: This specifies the containers that need to be deployed in this pod. In our case, we are deploying one container with the image we created in the previous section. Since we made sure in the previous section that the image is available to the Kubernetes cluster, we haven’t uploaded the image to any registry, and therefore imagePullPolicy is set to Never. ports: This is the port of the container exposed to the cluster. Deploy the application on the cluster using this: [yanboyang713@boyang WebApp]$ kubectl apply -f webapp-deployment.yaml deployment.apps/webapp-deployment created This command applies the configuration defined in YAML on the Kubernetes cluster. In other words, it creates a deployment named webapp-deployment. NOTE: Use older version of Kubernetes (1.15) when validator accept extensions as apiVersion for Deployment and StatefulSet. We are using 1.15, so apiVersion should be apps/v1. To convert an older Deployment to apps/v1, you can run: kubectl convert -f ./my-deployment.yaml --output-version apps/v1 Shows all the deployments on the cluster with their status and the number of running and active replicas. [yanboyang713@boyang WebApp]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE webapp-deployment 1/1 1 1 27m The figure shows there is a pod running the WebApp application. The deployment spawns a ReplicaSet, which tries to maintain the state of having one pod running at all times. You can use the command at the below, show the running ReplicaSets and pods on the cluster. [yanboyang713@boyang WebApp]$ kubectl get replicaset -o wide NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR webapp-deployment-6c4c85876d 1 1 1 33m webapp web-app:1.0 app=webapp,pod-template-hash=6c4c85876d [yanboyang713@boyang WebApp]$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES webapp-deployment-6c4c85876d-wj8t2 1/1 Running 0 34m 172.17.0.5 minikube \u003cnone\u003e \u003cnone\u003e The pod details include an IP address. The pod is accessible to the internal network using this IP address, but as stated earlier, accessing pods directly via IP addresses is discouraged since pods are expendables and a new pod might have a different IP address. It is clear from the IP address that though the pod is accessible through this IP address inside the Kubernetes network, one may not be able to access it from the host machine. Kubectl provides a way to use a proxy for the pod and access the application from the host machine. [yanboyang713@boyang WebApp]$ kubectl port-forward webapp-deployment-6c4c85876d-wj8t2 5000:5000 Forwarding from 127.0.0.1:5000 -\u003e 5000 Forwarding from [::1]:5000 -\u003e 5000 NOTE: webapp-deployment-6c4c85876d-wj8t2 is our pod name, and 5000 is the port exposed on the pod to access the application. Now the application is accessible from the host machine. The application running on the host machine browser. The application logs can be accessed from the pod using this: [yanboyang713@boyang Web","date":"2020-02-05","objectID":"/k8s/:10:3","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Kubernetes Service Kubernetes pods are expendable. ReplicaSet creates and destroys pods in the process of scaling up and down; therefore, accessing the pods via an IP address is not a reliable solution. Then how do microservices inside Kubernetes communicate with other microservices? The answer is Kubernetes services. Let’s try to understand the concept of services. Kubernetes services provide a virtual IP-based bridge to access the pods. One may access a single pod or may refer to a group of pods at the same time. There can be two types of interactions. Pods accessing services Services exposed publicly Before explaining this, let’s expose our web application via a service. There is a example shows a simple service with the selector pointing to our webapp. webapp-service.yaml File for Our Web App apiVersion:v1kind:Servicemetadata:name:webservicespec:selector:app:webappports:- protocol:TCPport:80targetPort:5000 The service is named webservice and points to the deployments with a selector as app:webapp. The service is exposed on port 80 and proxies the request to port 5000 of the result pods. Apply the service using this: [yanboyang713@boyang WebApp]$ kubectl apply -f webapp-service.yaml service/webservice created Verify that the service is created successfully using this: [yanboyang713@boyang WebApp]$ kubectl describe service webservice Name: webservice Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Selector: app=webapp Type: ClusterIP IP: 10.100.103.159 Port: \u003cunset\u003e 80/TCP TargetPort: 5000/TCP Endpoints: 172.17.0.5:5000 Session Affinity: None Events: \u003cnone\u003e The service is assigned a cluster IP address of 10.100.103.159. Any microservice inside the cluster will be able to access the service using this IP address via port 80. Now, let’s try to understand the two types of service interactions possible in a Kubernetes cluster. ","date":"2020-02-05","objectID":"/k8s/:11:0","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Pods Accessing Services Any microservices architecture requires a service to access multiple microservices within the private network. The access to other services is possible either through their IP address or through a DNS request. Kubernetes supports both of them. Environment variables: When a pod is launched in a node, Kubectl declares all the running services to be accessed as environment variables for the pod. But this forces a sequence to be followed; if new service is defined after the first service is booted, the first one doesn’t get access to the new service. Try to log in to the Docker container of the webapp pod and check the environment variables. The new service is not visible. If the developer deletes the existing deployment and re-creates the deployment, the service is visible in the environment variables. How to get this environment variables, You can see the subsection at the below. [yanboyang713@boyang WebApp]$ kubectl exec --stdin --tty webapp-deployment-6c4c85876d-wj8t2 -- /bin/bash root@webapp-deployment-6c4c85876d-wj8t2:/app# env LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36: HOSTNAME=webapp-deployment-6c4c85876d-wj8t2 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1 KUBERNETES_PORT=tcp://10.96.0.1:443 PWD=/app HOME=/root KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_PORT_443_TCP_PORT=443 KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 TERM=xterm SHLVL=1 KUBERNETES_SERVICE_PORT=443 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin KUBERNETES_SERVICE_HOST=10.96.0.1 _=/usr/bin/env root@webapp-deployment-6c4c85876d-wj8t2:/app# DNS: Though this is not a default setup, it is an optional but recommended add-on for Kubernetes. As the name says, each service registers a DNS record for itself as soon as it is created. The DNS record follows the pattern .. Any pod in the same namespace can access the service directly via , whereas pods outside the namespace must include . to access the service. Getting a shell to a container list pods kubectl get pods -o wide Verify that the container is running kubectl get pod webapp-deployment-6c4c85876d-wj8t2 Get a shell to the running container kubectl exec --stdin --tty webapp-deployment-6c4c85876d-wj8t2 -- /bin/bash Note: The double dash (–) separates the arguments you want to pass to the command from the kubectl arguments. ","date":"2020-02-05","objectID":"/k8s/:11:1","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Services Exposed Publicly There are multiple ways to expose a service to external world. Kubernetes provides multiple ways of achieving this. ClusterIP This allows a service to be exposed via a cluster’s internal IP. As shown earlier, a cluster’s internal IP address is exposed and can be accessed by the pods inside the cluster. NodePort This allows a service to be exposed at the node IP address on a specific port. This allows the service to be accessed via the : address. Internally Kubernetes creates a ClusterIP service that acts as a connection between the node IP and the actual service. The port number can be between 30000 and 32767. Each node proxies the selected port to the service pod. LoadBalancer This creates a public IP on top of NodePort. So, the service is accessible via a public IP, which is routed to NodePort and then is further routed to ClusterIP. Its implementation varies between cloud providers. A small addition to the configuration creates a LoadBalancer type. There is a example shows the addition of a LoadBalancer type in the service. [yanboyang713@boyang WebApp]$ cat webapp-service-loadbalancer.yaml apiVersion: v1 kind: Service metadata: name: webservice spec: type: LoadBalancer selector: app: webapp ports: - protocol: TCP port: 80 targetPort: 5000 [yanboyang713@boyang WebApp]$ kubectl get service webservice NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE webservice ClusterIP 10.100.103.159 \u003cnone\u003e 80/TCP 38h Once a service is created using this configuration, a developer can check the external IP address of the service using this: [yanboyang713@boyang WebApp]$ kubectl apply -f webapp-service-loadbalancer.yaml service/webservice configured [yanboyang713@boyang WebApp]$ kubectl get service webservice NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE webservice LoadBalancer 10.100.103.159 \u003cpending\u003e 80:32302/TCP 38h We do not get the external IP address since we are running our application on Minikube. On the cloud, the external IP is populated with a value. [yanboyang713@boyang WebApp]$ kubectl describe service webservice Name: webservice Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Selector: app=webapp Type: LoadBalancer IP: 10.100.103.159 Port: \u003cunset\u003e 80/TCP TargetPort: 5000/TCP NodePort: \u003cunset\u003e 32302/TCP Endpoints: 172.17.0.5:5000 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Type 91s service-controller ClusterIP -\u003e LoadBalancer ExternalName This simply maps the service to an address using a CNAME record. These are typically used when using an external service from within a cluster and abstracting out the actual link of the external service. There are a example shows a simple service with the type ExternalName. [yanboyang713@boyang WebApp]$ cat database-external-name.yaml apiVersion: v1 kind: Service metadata: name: db1 spec: type: ExternalName externalName: mysql01.database.test.com When internal pods look for the service db1, they receive a CNAME record of mysql01.database.text.com. There is no forwarding involved; only a single redirection happens at the DNS level. ExternalName also allows a developer to add a custom IP address to a service through which the service can be accessed by clients. The IP assignment is the sole responsibility of the cluster manager; it doesn’t come from Kubernetes. There is an example of an external IP assignment to a service. [yanboyang713@boyang WebApp]$ cat externalIpAssignedService.yaml apiVersion: v1 kind: Service metadata: name: externalIpAssignedService spec: selector: app: externalIpService ports: - name: http protocol: TCP port: 80 targetPort: 9000 externalIPs: - 70.34.1.23 Kubernetes, as defined earlier, is a self-healing platform. Let’s try to play around a bit with the cluster and see the role of Kubernetes services in it. ","date":"2020-02-05","objectID":"/k8s/:11:2","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Kubernetes Is Self-Healing In any application, it’s difficult to assure 100 percent uptime or availability of a single node. Kubernetes provides a means to create replicas of a service and also ensures the number of replicas are intact. Let’s modify our deployment and increase the number of replicas. [yanboyang713@boyang WebApp]$ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE webapp-deployment 1/1 1 1 40h [yanboyang713@boyang WebApp]$ kubectl scale --replicas=2 deployment webapp-deployment deployment.apps/webapp-deployment scaled [yanboyang713@boyang WebApp]$ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE webapp-deployment 2/2 2 2 40h [yanboyang713@boyang WebApp]$ kubectl get pods NAME READY STATUS RESTARTS AGE webapp-deployment-6c4c85876d-h4qgg 1/1 Running 0 27s webapp-deployment-6c4c85876d-wj8t2 1/1 Running 0 40h If one tries to kill any of the pods, the replication controller tries to reinstate the state and spawn a new pod. Let’s try killing one of the pods to see the state of the application. [yanboyang713@boyang WebApp]$ kubectl delete pod webapp-deployment-6c4c85876d-h4qgg pod \"webapp-deployment-6c4c85876d-h4qgg\" deleted [yanboyang713@boyang WebApp]$ kubectl get pods NAME READY STATUS RESTARTS AGE webapp-deployment-6c4c85876d-jnf7r 1/1 Running 0 42s webapp-deployment-6c4c85876d-wj8t2 1/1 Running 0 40h Through this, Kubernetes tries to keep the service available at all times. ","date":"2020-02-05","objectID":"/k8s/:11:3","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Add a Microservice Now you have seen how to deploy and run a microservice on Kubernetes, and you have seen the theory of how microservices interact with each other. Let’s create a new microservice that consumes a response from the webapp and renders it to the UI. Let’s call this app istio-frontend. We have already created a Docker file. ","date":"2020-02-05","objectID":"/k8s/:12:0","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Application Setup istio-frontend is a Java application that makes a request to the webapp service and populates its web page with the received data. In case the data is not received or the web-app service is not available, it populates ERROR RECEIVED as a response. We have created a Docker file with the tag frontend-app:1.0. Let’s follow the same approach as the previous application and create a deployment and service for the application. There are a example show the deployment and service file. [yanboyang713@boyang WebApp]$ cat frontend-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: frontend-deployment labels: app: frontend spec: replicas: 1 selector: matchLabels: app: frontend template: metadata: labels: app: frontend spec: containers: - name: frontend image: frontend:1.0 imagePullPolicy: Never ports: - containerPort: 8080 [yanboyang713@boyang WebApp]$ cat frontend-service.yaml apiVersion: v1 kind: Service metadata: name: frontendservice spec: selector: app: frontend ports: - protocol: TCP port: 80 targetPort: 8080 Let’s try to proxy to the new service to get the app running. Reducing the replica count to 0 for the webapp service. kubectl scale --replicas=0 deployment weapp-deployment ","date":"2020-02-05","objectID":"/k8s/:12:1","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Release and Deployment In a large organization, any application going to production requires regular development and maintenance. With new methodologies like agile firmly in place, release frequency has increased to multiple releases a day and so have release rollbacks. The traditional process of shutting down an application, redeploying, and restarting results in downtime. In the world of 99.99 percent availability, the scope of downtime means one minute or less in a seven-day period, so a single release a week violates the agile methodology. To minimize downtime, multiple deployment techniques are used, such as blue-green, canary, and rolling deployments. Kubernetes by default follows a rolling deployment. In other words, it creates two identical environments, and once the new environment is up, traffic is routed to the new environment, and later the old environment is terminated. Let’s upgrade our webapp to 2.0 and see the deployment on Kubernetes in action. There is a example shows the changes in the file. We will simply add time to the welcome message. [yanboyang713@boyang WebApp]$ cat appV2.py from flask import Flask import datetime app = Flask(__name__) @app.route(\"/\") def main(): currentDT = datetime.datetime.now() return \"Welcome user! current time is \" + str(currentDT) if __name__ == \"__main__\": app.run(host='0.0.0.0') Create a new container by following the same process as stated earlier. Below shows the modified deployment file with the upgraded container details. [yanboyang713@boyang WebApp]$ cat webapp-deployment-v2.yaml apiVersion: apps/v1 kind: Deployment metadata: name: webapp-deployment labels: app: webapp spec: replicas: 1 selector: matchLabels: app: webapp template: metadata: labels: app: webapp spec: containers: - name: webapp image: web-app:2.0 imagePullPolicy: Never ports: - containerPort: 5000 Let’s deploy the application on a cluster using this: kubectl apply -f webapp-deployment-v2.yaml A new pod is spawned, and the earlier one is terminated once the new pod is ready. What has happened in the background is a new environment with a single machine and version 2.0 is spawned while the webapp service was still pointing to the old environment. Once the new spawned pods returned the running status, the webapp service pointed the traffic to the new environment, and the earlier pods were terminated. Here’s the catch: what happens when a new pod is spawned but the application inside is still deploying and not yet up? The pod at this point returns a running status, but the application is still down, and at the same time the service starts directing traffic to the new environment. This adds downtime to the service until the application is up and running. To solve this issue, Kubernetes uses a readiness probe. ","date":"2020-02-05","objectID":"/k8s/:12:2","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":null,"content":"Readiness Probes Updating deployments with new ones can result in downtime as old pods are replaced by new ones. If for some reason the new deployment is misconfigured or has some error, the downtime continues until the error is detected. When a readiness probe is used, the service doesn’t forward traffic to new pods until the probe is successful. It also ensures that the old pods are not terminated until the new deployment pods are ready. This ensures that the deployment with the error doesn’t receive any traffic at all. To incorporate a readiness probe, we need to add a health link to our webapp. There is a example shows the change in the app.py code. A /health link is added, which will be available once the app is up and running. A delay of 60 seconds has been added in the code, which will help demonstrate this behavior of Kubernates. [yanboyang713@boyang WebApp]$ cat appV3.py from flask import Flask import datetime import time time.sleep(60) app = Flask(__name__) @app.route(\"/\") def main(): currentDT = datetime.datetime.now() return \"Welcome user! current time in v3 is \" + str(currentDT) @app.route(\"/health\") def health(): return \"OK\" if __name__ == \"__main__\": app.run(host='0.0.0.0') Create a new container with the tag web-app:3.0 and add it to the deployment file. [yanboyang713@boyang WebApp]$ cat webapp-deployment-v3.yaml apiVersion: apps/v1 kind: Deployment metadata: name: webapp-deployment labels: app: webapp spec: replicas: 1 selector: matchLabels: app: webapp template: metadata: labels: app: webapp spec: containers: - name: webapp image: web-app:3.0 imagePullPolicy: Never ports: - containerPort: 5000 readinessProbe: httpGet: path: /health port: 5000 initialDelaySeconds: 40 The readiness probe initializes with an initial delay of 40 seconds. If one already knows that an application deployment takes some time, this can be stated in initialDelaySeconds to avoid unnecessary checks on the application. After the initial delay, Kubelet does regular checks on the /health link, and when the link is up, the pod is moved to a ready state to accept traffic. There shows the status of deployment at different times. Let’s see what happened in the background. Checked the available deployments. A frontend-deployment and a webapp-deployment are working, each having one available pod in a ready state. Applied the new version 3 configuration. The ready pods number remains the same. On getting the pod’s details, we can see two webapp-deployment pods. The old one is ready, and the latest one is running but still not ready to accept traffic. At 40 seconds, no request to the readiness probe is triggered by Kubernetes; therefore, the pod remains in a ready-pending state. By default the health check is done every 10 seconds. After 60 seconds of deployment, the new pod upgrades to a ready state, and the old pod is moved to a terminating state. This ensures that until the new deployment becomes ready, the earlier deployment is not scrapped, and the traffic is routed to the older one. This is helpful when an application is being upgraded or when a new application is deployed. But this isn’t useful after the deployment is complete and the old deployment pods are terminated. If after that the deployment pods fail for known/unknown reasons, the readiness probe fails, and the traffic is not sent to the pod. This, on one hand, ensures that the application is not down, but the number of pods available to serve the traffic goes down. A corner case would be if the same issue happens to all the pods in the deployment; your complete application may go down. There is no ideal way to deal with such issues, but Kubernetes provides a common solution of restarting the application if the application becomes irresponsive. The liveness probe, similar to the readiness probe, keeps a check on the application, and in case the application stops responding, it restarts the pod. Let’s make a small change in our application to kill the application in 60 seconds and see","date":"2020-02-05","objectID":"/k8s/:13:0","tags":["kubernetes"],"title":"Getting started with Setup A Kubernetes Cluster","uri":"/k8s/"},{"categories":["Linux"],"content":"Install fcite packages in Manjaro yay -Syu adobe-source-han-sans-otc-fonts adobe-source-han-serif-otc-fonts yay -Syu fcitx fcitx-googlepinyin fcitx-im fcitx-configtool ","date":"2020-02-05","objectID":"/chineseinputmethod/:1:0","tags":["chinese input method"],"title":"Install Chinese Input Method in Linux","uri":"/chineseinputmethod/"},{"categories":["Linux"],"content":"Config Environmental variables vim ~/.profile add: export GTK_IM_MODULE=fcitx export QT_IM_MODULE=fcitx export XMODIFIERS=@im=fcitx fcitx \u0026 source .profile ","date":"2020-02-05","objectID":"/chineseinputmethod/:2:0","tags":["chinese input method"],"title":"Install Chinese Input Method in Linux","uri":"/chineseinputmethod/"},{"categories":null,"content":"Introduction ","date":"2020-02-04","objectID":"/pfsense/:1:0","tags":null,"title":"pfSense with Proxmox","uri":"/pfsense/"},{"categories":null,"content":"Virtualizing pfSense with Proxmox This following article is about building and running a pfSense® virtual machine under Proxmox 4.4. The guide applies to any newer Proxmox version. Article covers Proxmox networking setup and pfSense virtual machine setup process. The guide does not cover how to install Proxmox. A basic, working, pfSense virtual machine will exist by the end of this article. ","date":"2020-02-04","objectID":"/pfsense/:2:0","tags":null,"title":"pfSense with Proxmox","uri":"/pfsense/"},{"categories":null,"content":"Install Open vSwitch Update the package index and then install the Open vSwitch packages by executing: apt update apt install openvswitch-switch https://docs.netgate.com/pfsense/en/latest/recipes/virtualize-proxmox.html ","date":"2020-02-04","objectID":"/pfsense/:3:0","tags":null,"title":"pfSense with Proxmox","uri":"/pfsense/"},{"categories":["virtual environment"],"content":"Introduction Proxmox Virtual Environment is an open source server virtualization management solution based on QEMU/KVM and LXC. Proxmox VE is based on Debian. You can manage virtual machines, containers, highly available clusters, storage and networks with an integrated, easy-to-use web interface or via CLI. ","date":"2020-02-03","objectID":"/proxmox/:1:0","tags":["QEMU","KVM","LXC"],"title":"Get Started with Proxmox","uri":"/proxmox/"},{"categories":["virtual environment"],"content":"Install I recommend you use USB Flash Drive as Installation Medium because it is the faster option. ","date":"2020-02-03","objectID":"/proxmox/:2:0","tags":["QEMU","KVM","LXC"],"title":"Get Started with Proxmox","uri":"/proxmox/"},{"categories":["virtual environment"],"content":"Download the installer ISO image You can download ISO image from: https://www.proxmox.com/en/downloads/category/iso-images-pve Currently, the latest version is 6.3-1. ","date":"2020-02-03","objectID":"/proxmox/:2:1","tags":["QEMU","KVM","LXC"],"title":"Get Started with Proxmox","uri":"/proxmox/"},{"categories":["virtual environment"],"content":"Prepare a USB Flash Drive as Installation Medium Official link: https://pve.proxmox.com/wiki/Prepare_Installation_Media Linux Find the Correct USB Device Name There are two ways to find out the name of the USB flash drive. The first one is use lsblk (strong recommend). The second way is to compare the output of the fdisk -l command. Both example at the below. boyang:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 290.4M 1 loop /snap/vlc/1700 loop1 7:1 0 956K 1 loop /snap/gnome-logs/100 loop2 7:2 0 55.4M 1 loop /snap/core18/1932 loop3 7:3 0 156.4M 1 loop /snap/ramboxpro/22 loop4 7:4 0 204.4M 1 loop /snap/microk8s/1668 loop5 7:5 0 956K 1 loop /snap/gnome-logs/81 loop6 7:6 0 64.8M 1 loop /snap/gtk-common-themes/1514 loop7 7:7 0 161.4M 1 loop /snap/gnome-3-28-1804/128 loop8 7:8 0 2.2M 1 loop /snap/gnome-system-monitor/148 loop9 7:9 0 2.4M 1 loop /snap/gnome-calculator/748 loop10 7:10 0 217.9M 1 loop /snap/gnome-3-34-1804/60 loop11 7:11 0 276K 1 loop /snap/gnome-characters/570 loop12 7:12 0 2.5M 1 loop /snap/gnome-calculator/826 loop13 7:13 0 162.9M 1 loop /snap/gnome-3-28-1804/145 loop14 7:14 0 4.2M 1 loop /snap/tree/18 loop15 7:15 0 3.7M 1 loop /snap/gnome-system-monitor/127 loop16 7:16 0 276K 1 loop /snap/gnome-characters/550 loop17 7:17 0 97.8M 1 loop /snap/core/10185 loop18 7:18 0 255.6M 1 loop /snap/gnome-3-34-1804/36 loop19 7:19 0 64.4M 1 loop /snap/gtk-common-themes/1513 loop20 7:20 0 97.9M 1 loop /snap/core/10444 loop21 7:21 0 204.4M 1 loop /snap/microk8s/1786 loop22 7:22 0 55.3M 1 loop /snap/core18/1885 loop23 7:23 0 228.1M 1 loop /snap/netease-music/2 sda 8:0 1 3.8G 0 disk ├─sda1 8:1 1 242K 0 part ├─sda2 8:2 1 2.8M 0 part ├─sda3 8:3 1 859.2M 0 part /media/yanboyang713/PVE1 └─sda4 8:4 1 300K 0 part nvme0n1 259:0 0 465.8G 0 disk ├─nvme0n1p1 259:1 0 512M 0 part /boot/efi └─nvme0n1p2 259:2 0 465.3G 0 part / OR boyang:~$ sudo fdisk -l [sudo] password for yanboyang713: Disk /dev/loop0: 290.4 MiB, 304545792 bytes, 594816 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/loop1: 956 KiB, 978944 bytes, 1912 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/loop2: 55.4 MiB, 58052608 bytes, 113384 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/loop3: 156.4 MiB, 164028416 bytes, 320368 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/loop4: 204.4 MiB, 214278144 bytes, 418512 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/loop5: 956 KiB, 978944 bytes, 1912 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/loop6: 64.8 MiB, 67915776 bytes, 132648 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/loop7: 161.4 MiB, 169254912 bytes, 330576 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/nvme0n1: 465.8 GiB, 500107862016 bytes, 976773168 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: C60CB535-938E-4A9F-9BAC-92FDE1DD99B0 Device Start End Sectors Size Type /dev/nvme0n1p1 2048 1050623 1048576 512M EFI System /dev/nvme0n1p2 1050624 976771071 975720448 465.3G Linux filesystem Disk /dev/loop8: 2.2 MiB, 2273280 bytes, 4440 sec","date":"2020-02-03","objectID":"/proxmox/:2:2","tags":["QEMU","KVM","LXC"],"title":"Get Started with Proxmox","uri":"/proxmox/"},{"categories":["virtual environment"],"content":"Start Installion Office Website: https://pve.proxmox.com/wiki/Installation Step 1: When you boot from USB, You will see the below image. Install Proxmox VE Starts the normal installation. NOTE: It’s possible to use the installation wizard with a keyboard only. Buttons can be clicked by pressing the ALT key combined with the underlined character from the respective button. For example, ALT + N to press a Next button. Install Proxmox VE (Debug mode) Starts the installation in debug mode. A console will be opened at several installation steps. This helps to debug the situation if something goes wrong. To exit a debug console, press CTRL-D. This option can be used to boot a live system with all basic tools available. You can use it, for example, to repair a degraded ZFS rpool or fix the bootloader for an existing Proxmox VE setup. Rescue Boot With this option you can boot an existing installation. It searches all attached hard disks. If it finds an existing installation, it boots directly into that disk using the Linux kernel from the ISO. This can be useful if there are problems with the boot block (grub) or the BIOS is unable to read the boot block from the disk. Test Memory Runs memtest86+. This is useful to check if the memory is functional and free of errors. After selecting Install Proxmox VE and accepting the EULA, the prompt to select the target hard disk(s) will appear. The Options button opens the dialog to select the target file system. The default file system is ext4. The Logical Volume Manager (LVM) is used when ext4 or xfs is selected. Additional options to restrict LVM space can be set (see below). Proxmox VE can be installed on ZFS. As ZFS offers several software RAID levels, this is an option for systems that don’t have a hardware RAID controller. The target disks must be selected in the Options dialog. More ZFS specific settings can be changed under Advanced Options. I choose Install Proxmox VE for new install. Step 2: The next page asks for basic configuration options like the location, the time zone, and keyboard layout. The location is used to select a download server close by to speed up updates. The installer usually auto-detects these settings. They only need to be changed in the rare case that auto detection fails or a different keyboard layout should be used. Step 3: Next the password of the superuser (root) and an email address needs to be specified. The password must consist of at least 5 characters. It’s highly recommended to use a stronger password. Some guidelines are: Use a minimum password length of 12 to 14 characters. Include lowercase and uppercase alphabetic characters, numbers, and symbols. Avoid character repetition, keyboard patterns, common dictionary words, letter or number sequences, usernames, relative or pet names, romantic links (current or past), and biographical information (for example ID numbers, ancestors' names or dates). The email address is used to send notifications to the system administrator. For example: Information about available package updates. Error messages from periodic CRON jobs. Step 4: The last step is the network configuration. Please note that during installation you can either use an IPv4 or IPv6 address, but not both. To configure a dual stack node, add additional IP addresses after the installation. Step 5: The next step shows a summary of the previously selected options. Re-check every setting and use the Previous button if a setting needs to be changed. To accept, press Install. The installation starts to format disks and copies packages to the target. Please wait until this step has finished; then remove the installation medium and restart your system. If the installation failed check out specific errors on the second TTY (‘CTRL + ALT + F2’), ensure that the systems meets the minimum requirements. If the installation is still not working look at the how to get help chapter. Further configuration is done via the Proxmox web interface. Point your browser to ","date":"2020-02-03","objectID":"/proxmox/:2:3","tags":["QEMU","KVM","LXC"],"title":"Get Started with Proxmox","uri":"/proxmox/"},{"categories":["virtual environment"],"content":"How to update Proxmox without buying a subscription On the Proxmox website, they say: “Proxmox VE is a complete open-source platform for enterprise virtualization.” And typically you can’t charge for open source software; but the folks at Proxmox have done their best to scare most of us into buying a subscription — or, at least, to make us feel guilty for not having one. Now, Proxmox is great software; and developing great software takes great resources, great developers… and great money. As such, I encourage you to purchase a subscription if you’re using Proxmox in a business environment. However, some of you may be interested in using it for home usage, or just to tinker around, and for any of a variety of reasons you may not wish to, or may not be able to, purchase a subscription. This section is for you. To be clear, Proxmox works just fine without a license. The non-licensed version is just as functional as the paid version, with one exception: it doesn’t have access to the tested “enterprise” update repositories. As such (without the changes I’m about to show you), you can’t update the Debian software. Oh and of course, there’s that little nag screen each time you log in. SSH into the Proxmox host, or access its console through the web interface, and make a copy of the pve-enterprise.list sources file, like so: Step 1: root@pve ~# cd /etc/apt/sources.list.d/ Step 2: root@pve ~# cp pve-enterprise.list pve-no-subscription.list Step 3: OK, so now we have a copy of the original file. If we ever purchase a subscription later and want to use the enterprise repositories, we’ll be able to revert what we’ve done very easily. For now, edit the original file and comment out its one line; save and close the file. Step 4: Open the copied file, pve-no-subscription.list, and change the line ever so slightly. The original line looks something like this: deb https://enterprise.proxmox.com/debian/pve buster pve-enterprise The parts to note are https (change it to http,) enterprise.proxmox.com (change enterprise to download), and the end of the string — pve-enterprise (change to pve-no-subscription ). Do not edit the word stretch or buster, or any other word that appears in that position; that’s the Debian version code name. Your edited line should look like this: deb http://download.proxmox.com/debian/pve buster pve-no-subscription Save and close the file. Now, update the package lists: 5. Step 5: root@pve ~# apt-get update Step 6: And when that’s done, run software upgrades! root@pve ~# apt-get dist-upgrade Note: Always run dist-upgrade, not just “apt-get upgrade.” Dist-upgrade ensures that all packages and their dependencies are updated; if you just run “apt-get upgrade” things may break. ","date":"2020-02-03","objectID":"/proxmox/:3:0","tags":["QEMU","KVM","LXC"],"title":"Get Started with Proxmox","uri":"/proxmox/"},{"categories":["virtual environment"],"content":"Renaming a PVE node https://pve.proxmox.com/wiki/Renaming_a_PVE_node /etc/hosts 127.0.0.1 localhost.localdomain localhost 10.172.14.61 pve.richie.corp.microsoft.com richie ## The following lines are desirable for IPv6 capable hosts ::1 ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts /etc/hostname richie ","date":"2020-02-03","objectID":"/proxmox/:4:0","tags":["QEMU","KVM","LXC"],"title":"Get Started with Proxmox","uri":"/proxmox/"},{"categories":["virtual environment"],"content":"Backup ","date":"2020-02-03","objectID":"/proxmox/:5:0","tags":["QEMU","KVM","LXC"],"title":"Get Started with Proxmox","uri":"/proxmox/"},{"categories":null,"content":"Introduction In this tutorial, we will be reviewing what the fundamental similarities and distinctions are between Kubernetes and Docker Swarm. Kubernetes and Docker are two of the major players in container orchestration. Both Kubernetes and Docker Swarm continue to grow in popularity as they are increasingly used by those working with container deployment, orchestration, and management. Across all vertical markets, businesses continue to find new methods of utilization and practice with more uses constantly being discovered. ","date":"2020-02-02","objectID":"/kubervsdocker/:1:0","tags":["Kubernetes","Docker"],"title":"Kubernetes vs Docker Swarm","uri":"/kubervsdocker/"},{"categories":null,"content":"Table of Content Today, we will be reviewing the following concepts. What is Docker Swarm? What is Kubernetes? What are the main features of Kubernetes and Docker Swarm? Comparing Kubernetes and Docker Swarm When is it better to use Docker? When is it better to use Kubernetes? What are the alternatives to Kubernetes and Docker? ","date":"2020-02-02","objectID":"/kubervsdocker/:2:0","tags":["Kubernetes","Docker"],"title":"Kubernetes vs Docker Swarm","uri":"/kubervsdocker/"},{"categories":null,"content":"What is Docker Swarm? ","date":"2020-02-02","objectID":"/kubervsdocker/:3:0","tags":["Kubernetes","Docker"],"title":"Kubernetes vs Docker Swarm","uri":"/kubervsdocker/"},{"categories":["Azure"],"content":"Run Azure CLI in a Docker container You can use Docker to run a standalone Linux container with the Azure CLI pre-installed. Docker gets you started quickly with an isolated environment to run the CLI in. The image can also be used as a base for your own deployments. ","date":"2020-02-01","objectID":"/azurecli/:1:0","tags":["Azure CLI","Azure"],"title":"Getting Started with Azure CLI","uri":"/azurecli/"},{"categories":["Azure"],"content":"Run in a Docker container Install Install the CLI using docker run. docker run -it mcr.microsoft.com/azure-cli NOTE: MCR means Microsoft Container Registry, this is a public acaliable Container Registry. If you want to pick up the SSH keys from your user environment, use -v ${HOME}/.ssh:/root/.ssh to mount your SSH keys in the environment. For example: docker run -it -v ${HOME}/.ssh:/root/.ssh mcr.microsoft.com/azure-cli Login The CLI is installed on the image as the az command in /usr/local/bin. To sign in, run the az login command. Run the login command. az login If the CLI can open your default browser, it will do so and load an Azure sign-in page. Otherwise, open a browser page at https://aka.ms/devicelogin and enter the authorization code displayed in your terminal. If no web browser is available or the web browser fails to open, use device code flow with az login –use-device-code. Sign in with your account credentials in the browser. To learn more about different authentication methods, see Sign in with the Azure CLI. ","date":"2020-02-01","objectID":"/azurecli/:1:1","tags":["Azure CLI","Azure"],"title":"Getting Started with Azure CLI","uri":"/azurecli/"},{"categories":["Azure"],"content":"Update Docker image Updating with Docker requires both pulling the new image and re-creating any existing containers. For this reason, you should try to avoid using a container that hosts the CLI as a data store. Update your local image with docker pull. docker pull mcr.microsoft.com/azure-cli ","date":"2020-02-01","objectID":"/azurecli/:2:0","tags":["Azure CLI","Azure"],"title":"Getting Started with Azure CLI","uri":"/azurecli/"},{"categories":["Azure"],"content":"Uninstall Docker image If you decide to uninstall the Azure CLI, we’re sorry to see you go. Before you uninstall, use the az feedback command to let us know what could be improved or fixed. Our goal is to make the Azure CLI bug-free and user-friendly. If you found a bug, we’d appreciate it if you file a GitHub issue. After halting any containers running the CLI image, remove it. docker rmi mcr.microsoft.com/azure-cli ","date":"2020-02-01","objectID":"/azurecli/:3:0","tags":["Azure CLI","Azure"],"title":"Getting Started with Azure CLI","uri":"/azurecli/"},{"categories":["Azure"],"content":"Introduction Azure Container Registry is a managed, private Docker registry service based on the open-source Docker Registry 2.0. Create and maintain Azure container registries to store and manage your private Docker container images and related artifacts. Use Azure container registries with your existing container development and deployment pipelines, or use Azure Container Registry Tasks to build container images in Azure. Build on demand, or fully automate builds with triggers such as source code commits and base image updates. ","date":"2020-02-01","objectID":"/acr/:1:0","tags":["ACR"],"title":"Getting started with Azure Container Registry (ACR)","uri":"/acr/"},{"categories":["Azure"],"content":"Why we need using ACR in Machine Learning Workflow There are three machine learning environments (Development environment, Training Environment and Deployment Environment) depend on Docker. So, we have huge of Docker Image need to store and Management. Those three environments involve in all parts of {% post_link machineLearningWorkflow Machine Learning Workflow %} ","date":"2020-02-01","objectID":"/acr/:2:0","tags":["ACR"],"title":"Getting started with Azure Container Registry (ACR)","uri":"/acr/"},{"categories":["Azure"],"content":"Frequently Asked Questions (FAQ) If we enabled private endpoint for ACR, this ACR still can access by public Internet or not? Yes, there is an option called network access in the portal that will determine how the ACR can be accessed. If you want to disable the public access then you need to toggle with the options whether you want all networks or selected networks. Also you can completely disable the public access and use private end points. As a test I created an ACR with public access and tried to pull image from my laptop and it worked. I enabled a private endpoint and I can still access from my laptop because I did not disable the public access. Could we set-up a couple private endpoint in the same ACR? Technically yes we can do that. Please see below and I did that in my lab. Created an ACR and created multiple end points. You can also use this command to list all the private endpoints for an ACR az acr private-endpoint-connection list \\ \u003e --registry-name cert99.azurecr.io When we enabled private endpoint for ACR, are there any DNS FQDN rules need to set-up? This is done automatically during the private endpoint creation. But based on what you are doing in machine learning you might need to add DNS records. Please go through the document below that covers a lot about DNS records and private DNS zones for ACR. https://docs.microsoft.com/en-us/azure/container-registry/container-registry-private-link ","date":"2020-02-01","objectID":"/acr/:3:0","tags":["ACR"],"title":"Getting started with Azure Container Registry (ACR)","uri":"/acr/"},{"categories":["Networking"],"content":"Networking Overview One of the reasons Docker containers and services are so powerful is that you can connect them together, or connect them to non-Docker workloads. Docker containers and services do not even need to be aware that they are deployed on Docker, or whether their peers are also Docker workloads or not. Whether your Docker hosts run Linux, Windows, or a mix of the two, you can use Docker to manage them in a platform-agnostic way. Docker comes with three networking drivers by default. Network adapters are also initialized using these drivers, carrying the same exact name. For example, if you run docker network ls you will see a network named bridge, this driver uses bridge networking driver. This is the default network to which every container will try and connect, unless specified otherwise. boyang:~$ docker network ls NETWORK ID NAME DRIVER SCOPE 9180da752799 bridge bridge local 67c7f5587624 host host local d15c7dd51668 none null local This topic defines some basic Docker networking concepts and prepares you to design and deploy your applications to take full advantage of these capabilities. ","date":"2020-01-05","objectID":"/dockernetworking/:1:0","tags":["Docker","Networking"],"title":"Getting started with Docker Networking","uri":"/dockernetworking/"},{"categories":["Networking"],"content":"Network drivers Docker’s networking subsystem is pluggable, using drivers. Several drivers exist by default, and provide core networking functionality: ","date":"2020-01-05","objectID":"/dockernetworking/:2:0","tags":["Docker","Networking"],"title":"Getting started with Docker Networking","uri":"/dockernetworking/"},{"categories":["Networking"],"content":"Bridge The default network driver. If you don’t specify a driver, this is the type of network you are creating. Bridge networks are usually used when your applications run in standalone containers that need to communicate. See bridge networks. In terms of networking, a bridge network is a Link Layer device which forwards traffic between network segments. A bridge can be a hardware device or a software device running within a host machine’s kernel. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. Bridge networks apply to containers running on the same Docker daemon host. For communication among containers running on different Docker daemon hosts, you can either manage routing at the OS level, or you can use an overlay network. When you start Docker, a default bridge network (also called bridge) is created automatically, and newly-started containers connect to it unless otherwise specified. You can also create user-defined custom bridge networks. User-defined bridge networks are superior to the default bridge network. Differences between user-defined bridges and the default bridge User-defined bridges provide automatic DNS resolution between containers. Containers on the default bridge network can only access each other by IP addresses, unless you use the –link option, which is considered legacy. On a user-defined bridge network, containers can resolve each other by name or alias. Imagine an application with a web front-end and a database back-end. If you call your containers web and db, the web container can connect to the db container at db, no matter which Docker host the application stack is running on. If you run the same application stack on the default bridge network, you need to manually create links between the containers (using the legacy –link flag). These links need to be created in both directions, so you can see this gets complex with more than two containers which need to communicate. Alternatively, you can manipulate the /etc/hosts files within the containers, but this creates problems that are difficult to debug. User-defined bridges provide better isolation. All containers without a –network specified, are attached to the default bridge network. This can be a risk, as unrelated stacks/services/containers are then able to communicate. Using a user-defined network provides a scoped network in which only containers attached to that network are able to communicate. Containers can be attached and detached from user-defined networks on the fly. During a container’s lifetime, you can connect or disconnect it from user-defined networks on the fly. To remove a container from the default bridge network, you need to stop the container and recreate it with different network options. Each user-defined network creates a configurable bridge. If your containers use the default bridge network, you can configure it, but all the containers use the same settings, such as MTU and iptables rules. In addition, configuring the default bridge network happens outside of Docker itself, and requires a restart of Docker. User-defined bridge networks are created and configured using docker network create. If different groups of applications have different network requirements, you can configure each user-defined bridge separately, as you create it. Linked containers on the default bridge network share environment variables. Originally, the only way to share environment variables between two containers was to link them using the –link flag. This type of variable sharing is not possible with user-defined networks. However, there are superior ways to share environment variables. A few ideas: + Multiple co","date":"2020-01-05","objectID":"/dockernetworking/:2:1","tags":["Docker","Networking"],"title":"Getting started with Docker Networking","uri":"/dockernetworking/"},{"categories":["Networking"],"content":"Host For standalone containers, remove network isolation between the container and the Docker host, and use the host’s networking directly. See use the host network. If you use the host network mode for a container, that container’s network stack is not isolated from the Docker host (the container shares the host’s networking namespace), and the container does not get its own IP-address allocated. For instance, if you run a container which binds to port 80 and you use host networking, the container’s application is available on port 80 on the host’s IP address. Note: Given that the container does not have its own IP-address when using host mode networking, port-mapping does not take effect, and the -p, –publish, -P, and –publish-all option are ignored, producing a warning instead: WARNING: Published ports are discarded when using host network mode Host mode networking can be useful to optimize performance, and in situations where a container needs to handle a large range of ports, as it does not require network address translation (NAT), and no “userland-proxy” is created for each port. The host networking driver only works on Linux hosts, and is not supported on Docker Desktop for Mac, Docker Desktop for Windows, or Docker EE for Windows Server. You can also use a host network for a swarm service, by passing –network host to the docker service create command. In this case, control traffic (traffic related to managing the swarm and the service) is still sent across an overlay network, but the individual swarm service containers send data using the Docker daemon’s host network and ports. This creates some extra limitations. For instance, if a service container binds to port 80, only one service container can run on a given swarm node. ","date":"2020-01-05","objectID":"/dockernetworking/:2:2","tags":["Docker","Networking"],"title":"Getting started with Docker Networking","uri":"/dockernetworking/"},{"categories":["Networking"],"content":"Overlay Overlay networks connect multiple Docker daemons together and enable swarm services to communicate with each other. You can also use overlay networks to facilitate communication between a swarm service and a standalone container, or between two standalone containers on different Docker daemons. This strategy removes the need to do OS-level routing between these containers. See overlay networks. Docker Swarm What’s Docker Swarm? A Docker Swarm is a group of either physical or virtual machines that are running the Docker application and that have been configured to join together in a cluster. Once a group of machines have been clustered together, you can still run the Docker commands that you’re used to, but they will now be carried out by the machines in your cluster. The activities of the cluster are controlled by a swarm manager, and machines that have joined the cluster are referred to as nodes. What is Docker Swarm used for? Docker swarm is a container orchestration tool, meaning that it allows the user to manage multiple containers deployed across multiple host machines. One of the key benefits associated with the operation of a docker swarm is the high level of availability offered for applications. In a docker swarm, there are typically several worker nodes and at least one manager node that is responsible for handling the worker nodes' resources efficiently and ensuring that the cluster operates efficiently. What’s an overlay network? An “overlay network” is a virtual network that runs on top of a different network. Devices in that network are unaware that they are in an overlay. Traditional VPNs, for instance are overlay networks running over Internet. The term “overlay” has come to be used extensively (instead of VPN) only after technologies different than PPTP or L2TP have been developed to run virtual networks in Cloud environments. For those environments, protocols like VXLAN or GENEVE have been developed to address specific needs. Connects multiple Docker daemons together to create a flat virtual network across hosts where you can establish communication between a swarm service and a standalone container, or between two standalone containers on different Docker daemons. This strategy removes the need to do OS-level routing between these containers. This topic includes four different tutorials. You can run each of them on Linux, Windows, or a Mac, but for the last two, you need a second Docker host running elsewhere. Use the default overlay network demonstrates how to use the default overlay network that Docker sets up for you automatically when you initialize or join a swarm. This network is not the best choice for production systems. Use user-defined overlay networks shows how to create and use your own custom overlay networks, to connect services. This is recommended for services running in production. Use an overlay network for standalone containers shows how to communicate between standalone containers on different Docker daemons using an overlay network. Communicate between a container and a swarm service sets up communication between a standalone container and a swarm service, using an attachable overlay network. Prerequisites These require you to have at least a single-node swarm, which means that you have started Docker and run docker swarm init on the host. You can run the examples on a multi-node swarm as well. Use the default overlay network In this example, you start an alpine service and examine the characteristics of the network from the point of view of the individual service containers. This tutorial does not go into operation-system-specific details about how overlay networks are implemented, but focuses on how the overlay functions from the point of view of a service. Prerequisites This tutorial requires three physical or virtual Docker hosts which can all communicate with one another. This tutorial assumes that the three hosts are running on the same network with no firewall involved. Th","date":"2020-01-05","objectID":"/dockernetworking/:2:3","tags":["Docker","Networking"],"title":"Getting started with Docker Networking","uri":"/dockernetworking/"},{"categories":["Networking"],"content":"Macvlan Macvlan networks allow you to assign a MAC address to a container, making it appear as a physical device on your network. The Docker daemon routes traffic to containers by their MAC addresses. Using the macvlan driver is sometimes the best choice when dealing with legacy applications that expect to be directly connected to the physical network, rather than routed through the Docker host’s network stack. Some applications, especially legacy applications or applications which monitor network traffic, expect to be directly connected to the physical network. In this type of situation, you can use the macvlan network driver to assign a MAC address to each container’s virtual network interface, making it appear to be a physical network interface directly connected to the physical network. In this case, you need to designate a physical interface on your Docker host to use for the macvlan, as well as the subnet and gateway of the macvlan. You can even isolate your macvlan networks using different physical network interfaces. Keep the following things in mind: It is very easy to unintentionally damage your network due to IP address exhaustion or to “VLAN spread”, which is a situation in which you have an inappropriately large number of unique MAC addresses in your network. Your networking equipment needs to be able to handle “promiscuous mode”, where one physical interface can be assigned multiple MAC addresses. If your application can work using a bridge (on a single Docker host) or overlay (to communicate across multiple Docker hosts), these solutions may be better in the long term. Create a macvlan network When you create a macvlan network, it can either be in bridge mode or 802.1q trunk bridge mode. In bridge mode, macvlan traffic goes through a physical device on the host. In 802.1q trunk bridge mode, traffic goes through an 802.1q sub-interface which Docker creates on the fly. This allows you to control routing and filtering at a more granular level. Bridge mode To create a macvlan network which bridges with a given physical network interface, use –driver macvlan with the docker network create command. You also need to specify the parent, which is the interface the traffic will physically go through on the Docker host. $ docker network create -d macvlan \\ --subnet=172.16.86.0/24 \\ --gateway=172.16.86.1 \\ -o parent=eth0 pub_net If you need to exclude IP addresses from being used in the macvlan network, such as when a given IP address is already in use, use –aux-addresses: $ docker network create -d macvlan \\ --subnet=192.168.32.0/24 \\ --ip-range=192.168.32.128/25 \\ --gateway=192.168.32.254 \\ --aux-address=\"my-router=192.168.32.129\" \\ -o parent=eth0 macnet32 802.1q trunk bridge mode If you specify a parent interface name with a dot included, such as eth0.50, Docker interprets that as a sub-interface of eth0 and creates the sub-interface automatically. $ docker network create -d macvlan \\ --subnet=192.168.50.0/24 \\ --gateway=192.168.50.1 \\ -o parent=eth0.50 macvlan50 Use an ipvlan instead of macvlan In the above example, you are still using a L3 bridge. You can use ipvlan instead, and get an L2 bridge. Specify -o ipvlan_mode=l2. $ docker network create -d ipvlan \\ --subnet=192.168.210.0/24 \\ --subnet=192.168.212.0/24 \\ --gateway=192.168.210.254 \\ --gateway=192.168.212.254 \\ -o ipvlan_mode=l2 ipvlan210 Use IPv6 If you have configured the Docker daemon to allow IPv6, you can use dual-stack IPv4/IPv6 macvlan networks. $ docker network create -d macvlan \\ --subnet=192.168.216.0/24 --subnet=192.168.218.0/24 \\ --gateway=192.168.216.1 --gateway=192.168.218.1 \\ --subnet=2001:db8:abc8::/64 --gateway=2001:db8:abc8::10 \\ -o parent=eth0.218 \\ -o macvlan_mode=bridge macvlan216 ","date":"2020-01-05","objectID":"/dockernetworking/:2:4","tags":["Docker","Networking"],"title":"Getting started with Docker Networking","uri":"/dockernetworking/"},{"categories":["Networking"],"content":"None For this container, disable all networking. Usually used in conjunction with a custom network driver. none is not available for swarm services. See disable container networking. ","date":"2020-01-05","objectID":"/dockernetworking/:2:5","tags":["Docker","Networking"],"title":"Getting started with Docker Networking","uri":"/dockernetworking/"},{"categories":["Networking"],"content":"Network plugins You can install and use third-party network plugins with Docker. These plugins are available from Docker Hub or from third-party vendors. See the vendor’s documentation for installing and using a given network plugin. ","date":"2020-01-05","objectID":"/dockernetworking/:2:6","tags":["Docker","Networking"],"title":"Getting started with Docker Networking","uri":"/dockernetworking/"},{"categories":["Linux App"],"content":"Introduction WeChat is a popular micro-messaging cross-platform app supporting text, image/videos, group chats with over 900 million active monthly users, mostly in China. Along with the basic messaging facilities, it also provides payment services in China to pay bills, order goods and services, transfer money and much more. ","date":"2020-01-05","objectID":"/wechat/:1:0","tags":["Ubuntu","Wechat"],"title":"Installing Wechat Desktop Client in Ubuntu Linux","uri":"/wechat/"},{"categories":["Linux App"],"content":"Using WeChat on Linux desktop Just as you saw for WhatsApp on Linux, WeChat can also be used via a web browser by scanning the QR code. That of course doesn’t provide a native desktop experience. You have two ways to use WeChat on Linux: Use a dedicated unofficial WeChat client Use a generic messaging service aggregator like Franz or Rambox The unofficial WeChat client known as electronic-chat is discontinued unfortunately. It is no longer developed but you could still use it in snap format. However, I recommend not using it. ","date":"2020-01-05","objectID":"/wechat/:2:0","tags":["Ubuntu","Wechat"],"title":"Installing Wechat Desktop Client in Ubuntu Linux","uri":"/wechat/"},{"categories":["Linux App"],"content":"Method 1: Installing WeChat on Linux via Rambox Pro Rambox Pro is another method that you can use to run WeChat on Linux. It is a workspace browser that allows you to run and manage as many applications as you want. With this one, you cannot only run WeChat. But you can also run other applications like Discord, Airtable, Aol, Buffer, WhatsApp on Linux. You can download and install Rambox Pro free of cost. Also, it comes with premium plans for advanced users. However, as long as you are running WeChat, the free plan is enough. To get started with Rambox Pro, follow these steps: At first, you have to install Rambox Pro. You can install it from the Desktop Software center, or you can run the below command: boyang:~$ sudo snap install ramboxpro [sudo] password for yanboyang713: ramboxpro 1.4.1 from Rambox (ramboxapp✓) installed Once installed, run the application from the menu, and it will ask you to sign up. After signing up, hit the Home icon, and now you have to select WeChat. Then click on the Add button. Once done, you will get to see a QR code on your screen. Scan the QR code from your smartphone’s WeChat app to log in to WeChat on your Linux machine. ","date":"2020-01-05","objectID":"/wechat/:3:0","tags":["Ubuntu","Wechat"],"title":"Installing Wechat Desktop Client in Ubuntu Linux","uri":"/wechat/"},{"categories":["Linux App"],"content":"Method 2: Installing WeChat on Linux via Snap [Deprecated] electronic-chat is a third party open source client for WeChat hosted on github. As you can see, the repository is archived and the project is no longer active. Though the article has mentioned the steps, please avoid using this method. You can use snap in your Linux distribution to install it: sudo snap install electronic-wechat This will install WeChat client. Once done, launch it from the menu by searching for WeChat. When you launch it for the first time, it will ask you to scan QR code. Select the option to Scan QR code from the app and you can use it from your Ubuntu system. If at any moment, you are not satisfied and would rather prefer your phone to keep using WeChat, you can uninstall it using below command: sudo snap remove electronic-wechat That’s it. Enjoy WeChat on Linux desktop. ","date":"2020-01-05","objectID":"/wechat/:4:0","tags":["Ubuntu","Wechat"],"title":"Installing Wechat Desktop Client in Ubuntu Linux","uri":"/wechat/"},{"categories":["Machine Learning"],"content":"Machine Learning Development Liftcycle Overview There are two important parts for ML lifecycle (pipeline). Data-flow about how to management/store your data and how to transfer your data in your ML lifecycle. Transfer your data highly regarding with Networking, so in this post, I will talk a little bit about Network Topologgy from On-premises network to Azure Network Topology. Compute Resources/Environments about how to compute your data on each steps, different steps are running within different environments. In a typical model development lifecycle, you might: Start by developing and experimenting on a small amount of data. At this stage, use your local environment, such as a local computer or cloud-based virtual machine (VM), as your compute target. Details please click here Scale up to larger data, or do distributed training by using one of these training compute targets. After your model is ready, deploy it to a web hosting environment or IoT device with one of these deployment compute targets. Machine learning workflows define which phases are implemented during a machine learning project. The typical phases include data collection, data pre-processing, building datasets, model training and refinement, evaluation, and deployment to production. While these steps are generally accepted as a standard, there is also room for change. When creating a machine learning workflow, you first need to define the project, and then find an approach that works. Don’t try to fit the model into a rigid workflow. Rather, build a flexible workflow that allows you to start small and scale up to a production-grade solution. ","date":"2020-01-04","objectID":"/machinelearningworkflow/:1:0","tags":["Machine Learning","Azure"],"title":"Machine Learning Workflow","uri":"/machinelearningworkflow/"},{"categories":["Machine Learning"],"content":"Data store and access base on ML lifecycle The core of ML lifecycle is data-flow. The below image is show how does data-flow in ML lifecycle. ","date":"2020-01-04","objectID":"/machinelearningworkflow/:2:0","tags":["Machine Learning","Azure"],"title":"Machine Learning Workflow","uri":"/machinelearningworkflow/"},{"categories":["Machine Learning"],"content":"Understanding the Machine Learning data-flow Machine learning workflows define the steps initiated during a particular machine learning implementation. Machine learning workflows vary by project, but four basic phases are typically included. Gathering machine learning data as original datasets Gathering data is one of the most important stages of machine learning workflows. During data collection, you are defining the potential usefulness and accuracy of your project with the quality of the data you collect. To collect data, you need to identify your sources and aggregate data from those sources into a single dataset. This could mean streaming data from Internet of Things sensors, downloading open source data sets, or constructing a data lake from assorted files, logs, or media. The part of Data Flow is highly regrading about On-premises Network because most of original datasetsneed collect from on-premisses network. If you are no familiar with how to set-up On-premisses network. please have a look Azure IOT Edge and Azure IOT HUB for you can convience collect original. However, I strongly recommend you have a look my blog Network category. Data pre-processing Once your data is collected, you need to pre-process it. Pre-processing involves cleaning, verifying, and formatting data into a usable dataset. If you are collecting data from a single source, this may be a relatively straightforward process. However, if you are aggregating several sources you need to make sure that data formats match, that data is equally reliable, and remove any potential duplicates. Building datasets This phase involves breaking processed data into three datasets—training, validating, and testing: Training Set: used to initially train the algorithm and teach it how to process information. This set defines model classifications through parameters. Validation Set: used to estimate the accuracy of the model. This dataset is used to finetune model parameters. Test Set: used to assess the accuracy and performance of the models. This set is meant to expose any issues or mistrainings in the model. Training and refinement Once you have datasets, you are ready to train your model. This involves feeding your training set to your algorithm so that it can learn appropriate parameters and features used in classification. Once training is complete, you can then refine the model using your validation dataset. This may involve modifying or discarding variables and includes a process of tweaking model-specific settings (hyperparameters) until an acceptable accuracy level is reached. Machine learning evaluation Finally, after an acceptable set of hyperparameters is found and your model accuracy is optimized you can test your model. Testing uses your test dataset and is meant to verify that your models are using accurate features. Based on the feedback you receive you may return to training the model to improve accuracy, adjust output settings, or deploy the model as needed. ","date":"2020-01-04","objectID":"/machinelearningworkflow/:2:1","tags":["Machine Learning","Azure"],"title":"Machine Learning Workflow","uri":"/machinelearningworkflow/"},{"categories":["Machine Learning"],"content":"ML compute resources/environment ","date":"2020-01-04","objectID":"/machinelearningworkflow/:3:0","tags":["Machine Learning","Azure"],"title":"Machine Learning Workflow","uri":"/machinelearningworkflow/"},{"categories":["Machine Learning"],"content":"Development/Testing Environment Local (Docker/Docker Compose) DSVM (Azure Cloud) ","date":"2020-01-04","objectID":"/machinelearningworkflow/:3:1","tags":["Machine Learning","Azure"],"title":"Machine Learning Workflow","uri":"/machinelearningworkflow/"},{"categories":["Machine Learning"],"content":"Training Resources / Environment Azure Machine Learning Compute Cluster (Training Cluster) Azure Databricks (Spark) ","date":"2020-01-04","objectID":"/machinelearningworkflow/:3:2","tags":["Machine Learning","Azure"],"title":"Machine Learning Workflow","uri":"/machinelearningworkflow/"},{"categories":["Machine Learning"],"content":"Deployment Resources / Environment Web Service Azure Kubernetes Services Edge Device NVIDIA Jetson device (can work on Azure IOT edge) Xilinx FPGA (VITIS Platform) ","date":"2020-01-04","objectID":"/machinelearningworkflow/:3:3","tags":["Machine Learning","Azure"],"title":"Machine Learning Workflow","uri":"/machinelearningworkflow/"},{"categories":["Machine Learning"],"content":"Network Topology and Security ","date":"2020-01-04","objectID":"/machinelearningworkflow/:4:0","tags":["Machine Learning","Azure"],"title":"Machine Learning Workflow","uri":"/machinelearningworkflow/"},{"categories":["Machine Learning"],"content":"Basic Network Concepts NAT Gateway Subnet DNS Firewall (Iptables) Routing Table ","date":"2020-01-04","objectID":"/machinelearningworkflow/:4:1","tags":["Machine Learning","Azure"],"title":"Machine Learning Workflow","uri":"/machinelearningworkflow/"},{"categories":["Machine Learning"],"content":"One kind of On-premises network topology (ROS + LEDE + Ubiquiti UniFi) ","date":"2020-01-04","objectID":"/machinelearningworkflow/:4:2","tags":["Machine Learning","Azure"],"title":"Machine Learning Workflow","uri":"/machinelearningworkflow/"},{"categories":["Machine Learning"],"content":"Azure Networking Basic Concepts Virtual Network Network Security Groups (NSGs) User Defined Routing (UDRs) DNS Azure private DNS Zone Custom DNS Azure Firewall Azure Virtual Network Peering Private Endpoint ExpressRoute Azure Machine Learning Workspace Networking Topology Case Study Azure Networking Troubleshooting ","date":"2020-01-04","objectID":"/machinelearningworkflow/:4:3","tags":["Machine Learning","Azure"],"title":"Machine Learning Workflow","uri":"/machinelearningworkflow/"},{"categories":["Azure"],"content":"Stop and Start ACI for AML endpoint You can use Azure command-line interface (Azure CLI) to stop and start the Azure Container Instances. The steps at the below. Step 1: Go to your portal and click Cloud Shell Step 2: Check the Container Name using the below command. “sklearn-mnist-svc-4dea” is my Endpoint name az container list | grep “sklearn-mnist-svc-4dea” Step 3: Confirm your Container Name for your Endpoint using the below command. az container show –name sklearn-mnist-svc-4dea-Gk77a6id90eK5nXsPRkyEA –resource-group permanent Step 4: Stop the container, the command at the below. az container stop –name sklearn-mnist-svc-4dea-Gk77a6id90eK5nXsPRkyEA –resource-group permanent Step 5: Start the container, the command at the below. az container start –name sklearn-mnist-svc-4dea-Gk77a6id90eK5nXsPRkyEA –resource-group permanent ","date":"2020-01-04","objectID":"/aci/:1:0","tags":["Azure"],"title":"Using Azure Container Instances with Azure Machine Learning Service","uri":"/aci/"},{"categories":null,"content":"Introduction ","date":"2020-01-02","objectID":"/baslertriggerasynchronous/:1:0","tags":["Basler","OpenCV"],"title":"Getting started with Multi-Basler Trigger Asynchronous","uri":"/baslertriggerasynchronous/"},{"categories":null,"content":"Software Trigger Asynchronous ","date":"2020-01-02","objectID":"/baslertriggerasynchronous/:2:0","tags":["Basler","OpenCV"],"title":"Getting started with Multi-Basler Trigger Asynchronous","uri":"/baslertriggerasynchronous/"},{"categories":null,"content":"Synchronous Free Run https://docs.baslerweb.com/synchronous-free-run The Synchronous Free Run camera feature allows you to capture images on multiple cameras at the same time and the same frame rate. How It Works If you are using multiple cameras in free run mode, image acquisition is slightly asynchronous due to a variety of reasons, e.g., the camera’s individual timings and delays. The Synchronous Free Run feature allows you to synchronize cameras in free run mode. As a result, the cameras will acquire images at the same time and at the same frame rate. Also, you can use the Synchronous Free Run feature to capture images with multiple cameras in precisely time-aligned intervals, i.e., in a chronological sequence. For example, you can configure one camera to start image acquisition at a specific point in time. Then you configure another camera to start 100 milliseconds after the first camera and a third camera to start 200 milliseconds after the first camera: Also, you can configure the cameras to acquire images at the same time and the same frame rate, but with different exposure times: ","date":"2020-01-02","objectID":"/baslertriggerasynchronous/:2:1","tags":["Basler","OpenCV"],"title":"Getting started with Multi-Basler Trigger Asynchronous","uri":"/baslertriggerasynchronous/"},{"categories":null,"content":"Action Commands https://docs.baslerweb.com/action-commands ","date":"2020-01-02","objectID":"/baslertriggerasynchronous/:2:2","tags":["Basler","OpenCV"],"title":"Getting started with Multi-Basler Trigger Asynchronous","uri":"/baslertriggerasynchronous/"},{"categories":null,"content":"Scheduled Action Commands https://docs.baslerweb.com/scheduled-action-commands ","date":"2020-01-02","objectID":"/baslertriggerasynchronous/:2:3","tags":["Basler","OpenCV"],"title":"Getting started with Multi-Basler Trigger Asynchronous","uri":"/baslertriggerasynchronous/"},{"categories":null,"content":"Hardware Trigger Asynchronous ","date":"2020-01-02","objectID":"/baslertriggerasynchronous/:3:0","tags":["Basler","OpenCV"],"title":"Getting started with Multi-Basler Trigger Asynchronous","uri":"/baslertriggerasynchronous/"},{"categories":["Azure"],"content":"Introduction When you use the Azure platform, you can open a case/ticket at any time when you encounter problems or have questions. ","date":"0001-01-01","objectID":"/azureraisecase/:1:0","tags":["Azure"],"title":"Azure Raise Support Ticket/Case and Troubleshooting Guide","uri":"/azureraisecase/"},{"categories":["Azure"],"content":"The steps for raise a new case/ticket Go to portal. Click question mark. Click “Help+support” Click + New support request You can open a Technical support request. There is an example. ","date":"0001-01-01","objectID":"/azureraisecase/:2:0","tags":["Azure"],"title":"Azure Raise Support Ticket/Case and Troubleshooting Guide","uri":"/azureraisecase/"},{"categories":["Azure"],"content":"Troubleshooting ","date":"0001-01-01","objectID":"/azureraisecase/:3:0","tags":["Azure"],"title":"Azure Raise Support Ticket/Case and Troubleshooting Guide","uri":"/azureraisecase/"},{"categories":["Azure"],"content":"Permission Issue If you face the error MSG at the below. You don’t have permission to create a support request To get permission, ask your subscription administrator or owner to assign you ‘Support Request Contributor’ role for the selected subscription. Learn more about role assignments in the portal. You need contact your subscription administrator or owner to assign the “support request provider” role to the subscription you are currently in used. After, you can raise support ticket. ","date":"0001-01-01","objectID":"/azureraisecase/:3:1","tags":["Azure"],"title":"Azure Raise Support Ticket/Case and Troubleshooting Guide","uri":"/azureraisecase/"},{"categories":null,"content":"The world of data Data is generated every second, all over the world. The Web: homepages, Facebook Twitter, Mobiles and Sensors, Scientific and medical data: satellites, Enterprise information systems. ","date":"0001-01-01","objectID":"/databaseconcepts/:1:0","tags":null,"title":"Database Concepts","uri":"/databaseconcepts/"},{"categories":null,"content":"Data management Management of data, of different types, is behind almost everything you do on the Web. Google: Web pages, images, maps Amazon: books and other products Facebook: homepage, images, friendship, posts RMIT: library, student administration, course management, timetable. Digital libraries: Lynda.com ","date":"0001-01-01","objectID":"/databaseconcepts/:2:0","tags":null,"title":"Database Concepts","uri":"/databaseconcepts/"},{"categories":null,"content":"The database system A database system is for efficiently and effectively managing and using data. ","date":"0001-01-01","objectID":"/databaseconcepts/:3:0","tags":null,"title":"Database Concepts","uri":"/databaseconcepts/"},{"categories":null,"content":"Database system dimensions Overview Users, roles and security Databases A database programming language Transactions and concurrency control ","date":"0001-01-01","objectID":"/databaseconcepts/:4:0","tags":null,"title":"Database Concepts","uri":"/databaseconcepts/"},{"categories":null,"content":"Users There are multiple users for a database and they have different roles and privileges: Database Administrator (DBA): full control of the database — create and define database schemas, grant privileges to other users. Database developers: manage/control his/her own database. End users/data operators: only retrieve or update information in some parts of the database. ","date":"0001-01-01","objectID":"/databaseconcepts/:4:1","tags":null,"title":"Database Concepts","uri":"/databaseconcepts/"},{"categories":null,"content":"Databases A database is a collection of data (facts) that describe something of interest managed by a Database Management System (DBMS). More strictly speaking, a database is more than a collection — it is a collection of data management of a DBMS. Schema + Data files. Databases: data models A data model is a description of data so that the data can be more easily understood. A data model is a language for describing data or information. The description generally consists of three parts: Structure of the data, or data structure. Operations on the data. Constraints on the data Popular data models: The relational model for structured data, including object-relational extensions. The semi-structured data model, including XML and related standards. ** NOTE: ** I will talk about the relational model details after. Databases: Schema - The defaintion of data in the database A description of data in the database. The description can use data models at different levels. The Entity Relationship (ER) model. The Relational model. Constraints for data integrity. ** NOTE: ** Data constraints are restrictions on facts so that only sensible facts are kept in the database. Constraints should be derived from the real-world. The Schema – example Real world: Information: Student records should have student ID, name, contact phone number, and address. Constraints: Each student should be identified by Student No – Each student record must have the student No information. No two students can have the same student ID — Each student should have a unique student ID. ER model: Relation schema: Student(\\underline{studentNo}, name, phone, address) NOTE: underline mean primary key. Databases: data Data files on hard disks of computers. The files are managed by a DBMS (on top of some file system). How to access the data? Via the programming language (SQL) provided by the DBMS. Data (and the meta-data) can be read, retrieved and updated. ** NOTE: ** : data is information or facts about things (entities) in the real-world. Databases: Metadata Metadata (sometimes called data dictionary) is data describing the database, which may include: Schema definition Index – to speed up query processing Data types Constraints on data User information and privileges Database programming Language — SQL SQL – Structured Query Language — is not only a query language: Data Definition Language (DDL) Define database schema Data Query Language (DQL) Query the database to extract information Data Manipulation Language (DML) Update the database – insertion/deletion/update. ** NOTE: ** A query is a form of inquiry of the data in the database. A query is usually a program written in the query language (SQL) provided by a DBMS. SQL characteristic Database programming language SQL is declarative — you tell the system what problem to solve. Short programs. System optimization. Most other programming languages are procedural — you tell the system how to solve the problem. Long programs. User optimization. For Example: Given a list of records for students (student ID, name, address…) enrolled in ISYS1055/1057, calculate how many students are there in total? In SQL: select count(studentID) from Student; In C: read files, data structure, loop structure, output. ","date":"0001-01-01","objectID":"/databaseconcepts/:4:2","tags":null,"title":"Database Concepts","uri":"/databaseconcepts/"},{"categories":null,"content":"Transactions and Concurrency Control Many users operate the same database concurrently: Many students are booking tute classes on a timetable database. Many people book flight tickets online. Transactions Operations on databases (DDL/DML/DQL) are organized into transactions – an atomic unit that must finish in whole or nothing happens at all. No partial effect on the database. A transaction comprises several operations of READ and WRITE on databases. The execution of transactions should be durable — the effect of any completed transaction is permanent, even if system failure happens. By way of logging. The ACID properties of transactions Atomicity – the all-or-nothing execution of transactions Consistency – a transaction brings a database from one consistent state to another. In other words, meeting consistency constraints like balance can not be negative after a transaction. Isolation – each transaction must appear to execute as if no other transactions are executing at the same time. Durability – effect of a transaction is must never be lost, once transaction complete. Transaction processing Concurrency control Concurrency control allows multiple users transactions from each user are executed in “isolation”. Transactions: Commit and Rollback The “Commit” and “Rollback” command in SQL. Example: select*frommovies;//showcontentoftheMovietable.deletefrommovieswheremvID=2;//updatestotheMovietable.select*frommovies;//showcontentoftheMovietable.rollback;//rollbackthechanges. Starting the SQL Developer application starts a new session. When the application is killed (not when you close a connection), if there are updates to the database, you are asked whether to “commit the changes” or “rollback changes”. ","date":"0001-01-01","objectID":"/databaseconcepts/:4:3","tags":null,"title":"Database Concepts","uri":"/databaseconcepts/"},{"categories":null,"content":"The DBMS A database management system (DBMS) is a software for managing data. It provides mechanism for defining/describing data, querying the data and maintain the integrity and correctness of data (transactions and concurrency). Oracle is a DBMS. A DBMS typically has three parts: Storage management: how secondary storage (hard disks) is used effectively to store and access data. Query processing: how to execute (SQL) queries Transaction management: how to support transactions with the ACID properties. ","date":"0001-01-01","objectID":"/databaseconcepts/:5:0","tags":null,"title":"Database Concepts","uri":"/databaseconcepts/"},{"categories":null,"content":"The relation model The relational model is based on a single structure to represent data, a two-dimensional table called a relation. Example – the “movie” relation. ","date":"0001-01-01","objectID":"/databaseconcepts/:6:0","tags":null,"title":"Database Concepts","uri":"/databaseconcepts/"},{"categories":null,"content":"Basics of the relational model Attributes are the names for the columns of a relation. Each attribute has a domain — the set of all possible values allowed for the attribute, in another word it is type. Tuples are the rows of data in a relation comprising components that are atomic — a component cannot be broken into smaller components. Each component of a tuple is a value from the corresponding attribute domain. Schema of a relation is the name of a relation and its attributes. The schema for a relation is written as relation name followed by a list of is attributes in a pair of brackets. A relation instance is a set of tuples, or the data, for a given relation. – It is common to update the content of a relation and have a new relation instance. – It is less common to update the schema. Typically, a database system only keeps the ”current” relation instance – the data “now” kept in relations. The term “relation” can refer to a relation schema or a relation instance, and usually can be determined from the context. There is a example: The schema of the Movie relation is Movie(mvID, Title, Rating, Rel_date, Length, Studio). The domain of mvID is Integer (data type). The current instance of Movie relation has 5 tuples. The first tuple of Movie relation has 6 components: 1, ‘Angels \u0026 Demons’, M, 14-05-2009, 138, ‘Sony Pictures’. Note that the date ’14-05-2009’ is an atomic value. Data integrity constraints – Key and Primary Key An attribute or list of attributes are a Key of a relation if no two tuples/rows of the relation may agree in all the attribute(s) on the list. If there are several candidate keys, one is specified as the Primary Key. – No two movies can have the same value for mvID. mvID is the artificial key of Movie. – A movie (represented by mvID) can be linked to multiple genres, and a genre can link to multiple movies. So the mvID and genre together – {mvID, Genre} – are the only key for Classification. Primary key attributes are underlined in relation schemas. Note that the key constraint is a natural property of real-world data and part of a relation schema. Key definition can only be derived from the real-world. Although sample data in a relation can help disapprove a possible key definition, they cannot be used to prove a key definition. Example: For the Movie relation, even though the current content of the Movie relation may suggest that each movie has a distinct title, title is not a key for the relation. “No two movies can have the same title” is not a property that is always true for all movies to keep in a database. Data Integrity Constraints – Foreign Keys The foreign key constraint declares that an attribute or attributes of a relation references the primary key or key of a second relation. Values for the foreign key in the first relation must appear in the referenced attributes of the second relation. A child-parent relationship – the first and second relations can be seen as “child” and “parent” relations. Foreign keys in a relation schema are usually denoted by an asterisk (*). A foreign-key constraint on a relation prevents introducing nonsensical data into the relation. No insertion or update to the child relation introduces values not found in the parent relation. No deletion or update to the child relation causes some tuples of the parent relation to “dangle.” Relation schemas with data integrity constraints Primary key attributes are underlined. The primary key of a relation may comprise more than one attribute. Foreign key attributes are annotated with the asterisk. A primary key attribute can also be a foreign key. There is a Examples: Movie(mvID, Title, Rating, Rel_date, Length, Studio) - parent Classification(mvID*, genre) - child Relational databases A relational database schema is the set of all relation schemas in the database. A relational database instance comprises a set of relation instances. Or informally, a relational database comprises a set of relations, or tables. There is a example","date":"0001-01-01","objectID":"/databaseconcepts/:6:1","tags":null,"title":"Database Concepts","uri":"/databaseconcepts/"},{"categories":null,"content":"Introduction DNS is a short abbreviation for Domain Name Service which maps the IP and FQDN (Fully Qualified Domain Names) to one another. And by that, the DNS makes it easy to remember the IP. Name servers are the computers that run the DNS. So in this tutorial, we are going to install and configure DNS on Ubuntu. Through this tutorial, we will use one of the most common programs used for handling the name server on Ubuntu that is BIND (which is an abbreviation for Berkley Internet Naming Daemon). An important part of managing server configuration and infrastructure includes maintaining an easy way to look up network interfaces and IP addresses by name, by setting up a proper Domain Name System (DNS). Using fully qualified domain names (FQDNs), instead of IP addresses, to specify network addresses eases the configuration of services and applications, and increases the maintainability of configuration files. Setting up your own DNS for your private network is a great way to improve the management of your servers. In this tutorial, we will go over how to set up an internal DNS server, using the BIND name server software (BIND9) on Ubuntu 18.04, that can be used by your servers to resolve private hostnames and private IP addresses. This provides a central way to manage your internal hostnames and private IP addresses, which is indispensable when your environment expands to more than a few hosts. http://www.damagehead.com/blog/2015/04/28/deploying-a-dns-server-using-docker/ ","date":"0001-01-01","objectID":"/dns/:1:0","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"DNS Basics The Domain Name System (DNS) is an application-layer protocol that is part of the standard TCP/IP protocol suite. This protocol implements the DNS name service, which is the name service used on the Internet. ","date":"0001-01-01","objectID":"/dns/:2:0","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Name-to-Address Resolution Though it supports the complex, world-wide hierarchy of computers on the Internet, the basic function of DNS is actually very simple: providing name-to-address resolution for TCP/IP-based networks. Name-to-address resolution, also referred to as “mapping,” is the process of finding the IP address of a computer in a database by using its host name as an index. Name-to-address mapping occurs when a program running on your local machine needs to contact a remote computer. The program most likely will know the host name of the remote computer but may not know how to locate it, particularly if the remote machine is in another company, miles from your site. To get the remote machine’s address, the program requests assistance from the DNS software running on your local machine, which is considered a DNS client. Your machine sends a request to a DNS name server, which maintains the distributed DNS database. The files in the DNS database bear little resemblance to the NIS+ host or ipnodes Table or even the local /etc/hosts or /etc/inet/ipnodes file, though they maintain similar information: the host names, the ipnode names, IPv4 and IPv6 addresses, and other information about a particular group of computers. The name server uses the host name your machine sent as part of its request to find or “resolve” the IP address of the remote machine. It then returns this IP address to your local machine IF the host name is in its DNS database. Below image shows name-to-address mapping as it occurs between a DNS client and a name server, probably on the client’s local network. If the host name is not in that name server’s DNS database, this indicates that the machine is outside of its authority, or, to use DNS terminology, outside the local administrative domain. Thus, each name server is spoken of as being “authoritative” for its local administrative domain. Fortunately, the local name server maintains a list of host names and IP addresses of root domain name servers, to which it will forward the request from your machine. These root name servers are authoritative for huge organizational domains, as explained fully in “DNS Hierarchy and the Internet”. These hierarchies resemble UNIX file systems, in that they are organized into an upside-down tree structure. Each root name server maintains the host names and IP address of top level domain name servers for a company, a university, or other large organizations. The root name server sends your request to the top-level name servers that it knows about. If one of these servers has the IP address for the host you requested, it will return the information to your machine. If the top-level servers do not know about the host you requested, they pass the request to second-level name servers for which they maintain information. Your request is then passed on down through the vast organizational tree. Eventually, a name server that has information about your requested host in its database will return the IP address back to your machine. Image shows name-to-address resolution outside the local domain at the below. ","date":"0001-01-01","objectID":"/dns/:2:1","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"DNS Administrative Domains From a DNS perspective, an administrative domain is a group of machines that are administered as a unit. Information about this domain is maintained by at least two name servers; they are “authoritative” for the domain. The DNS domain is a purely logical grouping of machines. It could correspond to a physical grouping of machines, such as all machines attached to the Ethernet in a small business. But a local DNS domain just as likely could include all machines on a vast university network that belong to the computer science department or to university administration. For example, suppose the Ajax company has two sites, one in San Francisco and one in Seattle. The Retail.Sales.Ajax.com. domain might be in Seattle and the Wholesale.Sales.Ajax.com. domain might be in San Francisco. One part of the Sales.Ajax.com. domain would be in one city, the other part in the second city. Each administrative domain must have its own unique subdomain name. Moreover, if you want your network to participate in the Internet, the network must be part of a registered administrative domain. The section “Joining the Internet” has full details about domain names and domain registration. ","date":"0001-01-01","objectID":"/dns/:2:2","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"DNS Name Servers There are three types of DNS name servers: Primary server Secondary server Cache-only server Each domain must have one primary server and should have at least one secondary server to provide backup. “Zones” explains primary and secondary servers in detail. ","date":"0001-01-01","objectID":"/dns/:2:3","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"DNS Clients and the Resolver To be a DNS client, a machine must run the resolver. The resolver is neither a daemon nor a single program; rather, it is a set of dynamic library routines used by applications that need to know machine names. The resolver’s function is to resolve users' queries. To do that, it queries a name server, which then returns either the requested information or a referral to another server. Once the resolver is configured, a machine can request DNS service from a name server. When a machine’s /etc/nsswitch.conf file specifies hosts: dns (or any other variant that includes dns in the hosts line), the resolver libraries are automatically used. If the nsswitch.conf file specifies some other name service before dns, that name service is consulted first for host information and only if that name service does not find the host in question are the resolver libraries used. For example, if the hosts line in the nsswitch.conf file specifies hosts: nisplus dns, the NIS+ name service will first be searched for host information. If the information is not found in NIS+, then the DNS resolver is used. Since name services such as NIS+ and NIS only contain information about hosts in their own network, the effect of a hosts:nisplus dns line in a switch file is to specify the use of NIS+ for local host information and DNS for information on remote hosts out on the Internet. There are two kinds of DNS clients: Client-only. A client-only DNS client does not run in.named; instead, it consults the resolver. The resolver knows about a list of name servers for the domain, to which queries are then directed. Client-server. A client-server uses the services provided by in.named to resolve queries forwarded to it by client-machine resolvers. ","date":"0001-01-01","objectID":"/dns/:2:4","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Introducing the DNS Namespace The entire collection of DNS administrative domains throughout the world are organized in a hierarchy called the DNS namespace. This section shows how the namespace organization affects both local domains and the Internet. DNS Namespace Hierarchy Like the UNIX file system, DNS domains are organized as a set of descending branches similar to the roots of a tree. Each branch is a domain, each subbranch is a subdomain. The terms domain and subdomain are relative. A given domain is a subdomain relative to those domains above it in the hierarchy, and a parent domain to the subdomains below it. For example, in Figure 28-3, com is a parent domain to the Acme, Ajax, and AAA domains. Or you could just as easily say that those are subdomains relative to the com domain. In its turn, the Ajax domain is a parent to four subdomains (Sales, Manf, QA, and Corp). A domain contains one parent (or top) domain plus the associated subdomains if any. Domains are named up the tree starting with the lowest (deepest) subdomain and ending with the root domain. For example, Mktg.Corp.Ajax.Com. from Figure 28-3. ","date":"0001-01-01","objectID":"/dns/:2:5","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"DNS Hierarchy in a Local Domain If your company is large enough, it may support a number of domains,organized into a local namespace. Figure 28-4 shows a domain hierarchy that might be in place in a single company. The top-level, or “root” domain for the organization is ajax.com, which has three sub-domains, sales.ajax.com, test.ajax.com, and manf.ajax.com. DNS clients request service only from the servers that support their domain. If the domain’s server does not have the information the client needs, it forwards the request to its parent server, which is the server in the next-higher domain in the hierarchy. If the request reaches the top-level server, the top-level server determines whether the domain is valid. If it is not valid, the server returns a “not found” type message to the client. If the domain is valid, the server routes the request down to the server that supports that domain. ","date":"0001-01-01","objectID":"/dns/:2:6","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"DNS Hierarchy and the Internet The domain hierarchy shown in Figure 28-4 is, conceptually, a “leaf” of the huge DNS namespace supported on the global Internet. The DNS namespace for the Internet is organized hierarchically as shown in Figure 28-5. It consists of the root directory, represented as a dot (.) and two top level domain hierarchies, one organizational and one geographical. Note that the com domain introduced in Figure 28-3 is one of a number of top-level organizational domains in existence on the Internet. At the present time, the organizational hierarchy divides its namespace into the top-level domains listed shown in Table 28-1. It is probable that additional top-level organizational domains will be added in the future. Domain Purpose com Commercial organizations edu Educational institutions gov Government institutions mil Military groups net Major network support centers org Nonprofit organizations and others int International organizations The geographic hierarchy assigns each country in the world a two- or three-letter identifier and provides official names for the geographic regions within each country. For example, domains in Britain are subdomains of the uk top-level domain, Japanese domains are subdomains of jp, and so on. ","date":"0001-01-01","objectID":"/dns/:2:7","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Joining the Internet The Internet root domain, top-level domains (organizational and geographical) are maintained by the various Internet governing bodies. People with networks of any size can “join” the Internet by registering their domain name in either the organizational or the geographical hierarchy. Every DNS domain must have a domain name. If your site wants to use DNS for name service without connecting to the Internet, you can use any name your organization wants for its your domains and subdomains, if applicable. However, if your site plans wants to join the Internet, it must register its domain name with the Internet governing bodies. To join the Internet, you have to: Register your DNS domain name with the an appropriate Internet governing body. Obtain a network IP address from that governing body. There are two ways to accomplish this: You can communicate directly with the appropriate Internet governing body or their agent. In the United States, InterNIC is the company that currently handles network address and domain registration matters. You can contract with an Internet Service Provider (ISP) to assist you. ISPs provide a wide range of services from consulting to actually hosting your Internet presence. ","date":"0001-01-01","objectID":"/dns/:2:8","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Domain Names Domain names indicate a domain’s position in the overall DNS namespace, much as path names indicate a file’s position in the UNIX file system. After your local domain is registered, its name is prepended to the name of the Internet hierarchy to which it belongs. For example, the ajax domain shown in Figure 28-4 has been registered as part of the Internet com hierarchy. Therefore, its Internet domain name becomes ajax.com. Figure 28-6 shows the position of the ajax.com domain in the DNS namespace on the Internet. The ajax.com subdomains now have the following names. sales.ajax.com test.ajax.com manf.ajax.com DNS does not require domain names to be capitalized, though they may be. Here are some examples of machines and domain names: Boss.manf.ajax.com quota.Sales.ajax.com The Internet organization regulates administration of its domains by granting each domain authority over the names of its hosts and by expecting each domain to delegate authority to the levels below it. Thus, the com domain has authority over the names of the hosts in its domain. It also authorizes the formation of the Ajax.com domain and delegates authority over the names in that domain. The Ajax.com domain, in turn, assigns names to the hosts in its domain and approves the formation of the Sales.Ajax.com, Test.Ajax.com, and Manf.Ajax.com domains. ","date":"0001-01-01","objectID":"/dns/:2:9","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Fully-Qualified Domain Names A domain name is said to be fully-qualified when it includes the names of every DNS domain from the local domain on up to “.”, the DNS root domain. Conceptually, the fully-qualified domain name indicates the path to the root, as does the absolute path name of a UNIX file. However, fully-qualified domain names are read from lowest, on the left, to highest, on the right. Therefore, a fully-qualified domain name has the syntax: The fully qualified domain names for the ajax domain and its subdomains are: ajax.com. sales.ajax.com test.ajax.com. manf.ajax.com ","date":"0001-01-01","objectID":"/dns/:2:10","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Zones DNS service for a domain is managed on the set of name servers first introduced “in.named and DNS Name Servers”. Name servers can manage a single domain, or multiple domains, or domains and some or all of their corresponding subdomains. The part of the namespace that a given name server controls is called a zone; thus, the name server is said to be authoritative for the zone. If you are responsible for a particular name server, you may be given the title zone administrator. The data in a name server’s database are called zone files. One type of zone file stores IP addresses and host names. When someone attempts to connect to a remote host using a host name by a utility like ftp or telnet, DNS performs name-to-address mapping, by looking up the host name in the zone file and converting it into its IP address. For example, the Ajax domain shown in Figure 28-7 contains a top domain (Ajax), four subdomains, and five sub-subdomains. It is divided into four zones shown by the thick lines. Thus, the Ajax name server administers a zone composed of the Ajax, Sales, Retail, and Wholesale domains. The Manf and QA domains are zones unto themselves served by their own name servers, and the Corp name server manages a zone composed of the Corp, Actg, Finance, and Mktg domains. ","date":"0001-01-01","objectID":"/dns/:2:11","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Reverse Mapping The DNS database also include zone files that use the IP address as a key to find the host name of the machine, enabling IP address to host name resolution. This process is called reverse resolution or more commonly, reverse mapping. Reverse mapping is used primarily to verify the identity of the machine that sent a message or to authorize remote operations on a local host. ","date":"0001-01-01","objectID":"/dns/:2:12","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"The in-addr.arpa Domain The in-addr.arpa domain is a conceptual part of the DNS namespace that uses IP addresses for its leaves, rather than domain names. It is the part of your zone that enables address-to-name mapping. Just as DNS domain names are read with the lowest level subdomain occupying the furthest left position and the root at the far right, in-addr.arpa domain IP addresses are read from lowest level to the root. Thus, the IP addresses are read backward. For example, suppose a host has the IP address 192.200.21.165. In the in-addr.arpa zone files, its address is listed as 165.21.200.192.in-addr.arpa. with the dot at the end indicating the root of the in-addr.arpa domain. ","date":"0001-01-01","objectID":"/dns/:2:13","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"DNS Servers http://www.damagehead.com/blog/2015/04/28/deploying-a-dns-server-using-docker/ yanboyang713@boyang:~$ sudo netstat -pna | grep 53 [sudo] password for yanboyang713: tcp 0 0 172.17.0.1:53 0.0.0.0:* LISTEN 1504/named tcp 0 0 172.18.0.1:53 0.0.0.0:* LISTEN 1504/named tcp 0 0 192.168.1.1:53 0.0.0.0:* LISTEN 1504/named tcp 0 0 10.172.1.51:53 0.0.0.0:* LISTEN 1504/named tcp 0 0 127.0.0.1:53 0.0.0.0:* LISTEN 1504/named tcp 0 0 127.0.0.53:53 0.0.0.0:* LISTEN 1141/systemd-resolv tcp 0 0 127.0.0.1:953 0.0.0.0:* LISTEN 1504/named tcp6 0 0 2404:f801:18:402::49883 2a01:111:f102:8001::443 ESTABLISHED 17794/537.36 --node udp 0 0 0.0.0.0:5353 0.0.0.0:* 845/avahi-daemon: r udp 0 0 172.17.0.1:53 0.0.0.0:* 1504/named udp 0 0 172.18.0.1:53 0.0.0.0:* 1504/named udp 0 0 192.168.1.1:53 0.0.0.0:* 1504/named udp 0 0 10.172.1.51:53 0.0.0.0:* 1504/named udp 0 0 127.0.0.1:53 0.0.0.0:* 1504/named udp 0 0 127.0.0.53:53 0.0.0.0:* 1141/systemd-resolv udp6 0 0 :::60590 :::* 17794/537.36 --node udp6 0 0 :::5353 :::* 845/avahi-daemon: r udp6 0 0 :::42321 :::* 17794/537.36 --node https://moss.sh/name-resolution-issue-systemd-resolved/ systemd-resolved sudo service bind9 stop sudo ip addr add 192.168.1.53/24 dev br0 docker run -d –name=bind –dns=127.0.0.1 –publish=192.168.1.53:53:53/udp –publish=192.168.1.53:10001:10001 –volume=/srv/docker/bind:/data –env=‘ROOT_PASSWORD=SecretPassword’ sameersbn/bind:latest execute an interactive bash shell on the container. docker exec -it bind bash yanboyang713@boyang:~$ sudo systemctl status systemd-resolved ● systemd-resolved.service - Network Name Resolution Loaded: loaded (/lib/systemd/system/systemd-resolved.service; enabled; vendor preset: enabled) Active: active (running) since Sat 2020-11-14 12:56:49 CST; 8h ago Docs: man:systemd-resolved.service(8) https://www.freedesktop.org/wiki/Software/systemd/resolved https://www.freedesktop.org/wiki/Software/systemd/writing-network-configuration-managers https://www.freedesktop.org/wiki/Software/systemd/writing-resolver-clients Main PID: 1141 (systemd-resolve) Status: \"Processing requests...\" Tasks: 1 (limit: 4915) CGroup: /system.slice/systemd-resolved.service └─1141 /lib/systemd/systemd-resolved Nov 14 12:56:49 boyang systemd-resolved[1141]: . IN DS 19036 8 2 49aac11d7b6f6446702e54a1607371607a1a41855200fd2ce1cdde32f24e8fb5 Nov 14 12:56:49 boyang systemd-resolved[1141]: . IN DS 20326 8 2 e06d44b80b8f1d39a95c0b0d7c65d08458e880409bbc683457104237c7f8ec8d Nov 14 12:56:49 boyang systemd-resolved[1141]: Negative trust anchors: 10.in-addr.arpa 16.172.in-addr.arpa 17.172.in-addr.arpa 18.172.in-addr.arpa 19.172.in-addr.arpa 20.172.in-addr.arpa 21.172.in-addr.ar Nov 14 12:56:49 boyang systemd-resolved[1141]: Using system hostname 'boyang'. Nov 14 12:56:49 boyang systemd[1]: Started Network Name Resolution. Nov 14 15:39:22 boyang systemd-resolved[1141]: Grace period over, resuming full feature set (UDP+EDNS0) for DNS server 10.50.50.50. Nov 14 16:03:27 boyang systemd-resolved[1141]: Grace period over, resuming full feature set (UDP+EDNS0) for DNS server 10.50.10.50. Nov 14 17:32:02 boyang systemd-resolved[1141]: Server returned error NXDOMAIN, mitigating potential DNS violation DVE-2018-0001, retrying transaction with reduced feature level UDP. Nov 14 17:32:02 boyang systemd-resolved[1141]: Server returned error NXDOMAIN, mitigating potential DNS violation DVE-2018-0001, retrying transaction with reduced feature level UDP. Nov 14 17:44:01 boyang systemd-resolved[1141]: Grace period over, resuming full feature set (UDP+EDNS0) for DNS server 10.50.50.50. DNS servers perform one or more functions: Zone Master Servers. A master name server maintains all the data corresponding to the zone, making it the authority for that zone. Master servers are commonly called authoritative name servers. (See “Master Servers”.) The two types of master server are: Zone primary master server. Each zone has one server that is designated as the primary master server for that zone. (See “Primary Master ","date":"0001-01-01","objectID":"/dns/:2:14","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Prerequisites To complete this tutorial, you will need the following infrastructure. Create each server in the same datacenter with private networking enabled: A fresh Ubuntu 18.04 server to serve as the Primary DNS server, ns1 (Recommended) A second Ubuntu 18.04 server to serve as a Secondary DNS server, ns2 Additional servers in the same datacenter that will be using your DNS servers On each of these servers, configure administrative access via a sudo user and a firewall by following our Ubuntu 18.04 initial server setup guide. ","date":"0001-01-01","objectID":"/dns/:3:0","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Example Infrastructure and Goals For the purposes of this article, we will assume the following: We have two servers which will be designated as our DNS name servers. We will refer to these as ns1 and ns2 in this guide. We have two additional client servers that will be using the DNS infrastructure we create. We will call these host1 and host2 in this guide. You can add as many as you’d like for your infrastructure. All of these servers exist in the same datacenter. We will assume that this is the nyc3 datacenter. All of these servers have private networking enabled (and are on the 10.128.0.0/16 subnet. You will likely have to adjust this for your servers). All servers are connected to a project that runs on “example.com”. Since our DNS system will be entirely internal and private, you do not have to purchase a domain name. However, using a domain you own may help avoid conflicts with publicly routable domains. With these assumptions, we decide that it makes sense to use a naming scheme that uses “nyc3.example.com” to refer to our private subnet or zone. Therefore, host1’s private Fully-Qualified Domain Name (FQDN) will be host1.nyc3.example.com. Refer to the following table the relevant details: Host Role Private FQDN Private IP Address ns1 Primary DNS Server ns1.nyc3.example.com 10.128.10.11 ns2 Secondary DNS Server ns2.nyc3.example.com 10.128.20.12 host1 Generic Host 1 host1.nyc3.example.com 10.128.100.101 host2 Generic Host 2 host2.nyc3.example.com 10.128.200.102 Note Your existing setup will be different, but the example names and IP addresses will be used to demonstrate how to configure a DNS server to provide a functioning internal DNS. You should be able to easily adapt this setup to your own environment by replacing the host names and private IP addresses with your own. It is not necessary to use the region name of the datacenter in your naming scheme, but we use it here to denote that these hosts belong to a particular datacenter’s private network. If you utilize multiple datacenters, you can set up an internal DNS within each respective datacenter. By the end of this tutorial, we will have a primary DNS server, ns1, and optionally a secondary DNS server, ns2, which will serve as a backup. Let’s get started by installing our Primary DNS server, ns1. ","date":"0001-01-01","objectID":"/dns/:3:1","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Installing BIND on DNS Servers Note Text that is highlighted in red is important! It will often be used to denote something that needs to be replaced with your own settings or that it should be modified or added to a configuration file. For example, if you see something like host1.nyc3.example.com, replace it with the FQDN of your own server. Likewise, if you see host1_private_IP, replace it with the private IP address of your own server. Before starting the installation process, please ensure that your system is updated by executing the next three commands. On both DNS servers, ns1 and ns2, update the apt package cache by typing: ","date":"0001-01-01","objectID":"/dns/:4:0","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Step 1- Update System sudo apt-get update sudo apt-get upgrade sudo apt-get dist-upgrade ","date":"0001-01-01","objectID":"/dns/:4:1","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Step 2 – Install DNS package Use the following command: sudo apt-get install bind9 bind9utils bind9-doc Once you execute the previous command it will suggest some other packages to be installed, press y to confirm downloading and installing those packages. ","date":"0001-01-01","objectID":"/dns/:4:2","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Step 3 – Install DNS Utilities Another useful package that will help you a lot in troubleshooting and testing the DNS issues is the dnsutils package that can be installed using the next command. sudo apt-get install dnsutils ","date":"0001-01-01","objectID":"/dns/:4:3","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Setting Bind to IPv4 Mode Before continuing, let’s set BIND to IPv4 mode since our private networking uses IPv4 exclusively. On both servers, edit the bind9 default settings file by typing: sudo vim /etc/default/bind9 Add “-4” to the end of the OPTIONS parameter. It should look like the following: /etc/default/bind9 ## run resolvconf? RESOLVCONF=no ## startup options for the server OPTIONS=\"-u bind -4\" Save and close the file when you are finished. Restart BIND to implement the changes: sudo systemctl restart bind9 Now that BIND is installed, let’s configure the primary DNS server. ","date":"0001-01-01","objectID":"/dns/:4:4","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Configuring the Primary DNS Server BIND’s configuration consists of multiple files, which are included from the main configuration file, named.conf. These filenames begin with named because that is the name of the process that BIND runs (short for “domain name daemon”). We will start with configuring the options file. Configuring the Options File On ns1, open the named.conf.options file for editing: sudo nano /etc/bind/named.conf.options Above the existing options block, create a new ACL (access control list) block called “trusted”. This is where we will define a list of clients that we will allow recursive DNS queries from (i.e. your servers that are in the same datacenter as ns1). Using our example private IP addresses, we will add ns1, ns2, host1, and host2 to our list of trusted clients: ","date":"0001-01-01","objectID":"/dns/:4:5","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Step 4 – DNS Configuration Usually, you can find the DNS configuration files stored in /etc/bind directory. /etc/bind/named.conf is the master configuration file that contains the DNS options and it’s highly recommended that you should be careful while editing it. ","date":"0001-01-01","objectID":"/dns/:4:6","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Step 5 – Configuring NameServer The most used and default configuration is using your server as a caching server. This means that the DNS will get the answer to name queries, cache it and use the answer again when the domain is queried for another time. So, to use your server as a caching nameserver you can follow the next few steps. Open and edit the /etc/bind/named.conf.options with your favorite editor. sudo vi /etc/bind/named.conf.options Add the following block to it, here we have used Google’s DNS. forwarders { 8.8.8.8; }; To enable the new configurations you should restart the DNS service. sudo systemctl restart bind9 To test your query time we can use the dig command which is installed by the dnsutils package. ","date":"0001-01-01","objectID":"/dns/:4:7","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"TIPS ","date":"0001-01-01","objectID":"/dns/:5:0","tags":null,"title":"dns","uri":"/dns/"},{"categories":null,"content":"Show you currently DNS yanboyang713@boyang:~$ cat /etc/resolv.conf ## This file is managed by man:systemd-resolved(8). Do not edit. ## ## This is a dynamic resolv.conf file for connecting local clients to the ## internal DNS stub resolver of systemd-resolved. This file lists all ## configured search domains. ## ## Run \"systemd-resolve --status\" to see details about the uplink DNS servers ## currently in use. ## ## Third party programs must not access this file directly, but only through the ## symlink at /etc/resolv.conf. To manage man:resolv.conf(5) in a different way, ## replace this symlink by a static file or a different symlink. ## ## See man:systemd-resolved.service(8) for details about the supported modes of ## operation for /etc/resolv.conf. nameserver 127.0.0.53 options edns0 search corp.microsoft.com ","date":"0001-01-01","objectID":"/dns/:5:1","tags":null,"title":"dns","uri":"/dns/"},{"categories":["virtual environment"],"content":"Introduction https://blog.atulr.com/docker-local-environment/ ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:1:0","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Perrequires Install Docker Install Docker Compose ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:2:0","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Docker Docker is a tool designed to make it easier to create, deploy, and run applications by using containers. Containers allow a developer to package up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package. ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:3:0","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Docker container To form a mental picture, for the time being just consider a Docker Container as an extremely light weight isolated linux based virtual machine inside which we will run our application service (although containers are not exactly VMs). The container will contain our code and all of its dependencies (system libraries, tools, etc). For our setup we will use one docker container per service and separate docker containers for our database. containers_mental_modal ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:3:1","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Docker-Compose Docker compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration. TLDR; Docker compose lets you run all your services at once (also in the right order) and manage them via a unified interface. Docker compose in general will contain: Services: Services are the list of induvidual docker containers that will be run by the compose tool. We will specify the ports and other configurations needed to run the docker containers here. Networks: Network provides a way by which different services can interact with each other. Each container can attach itself to a network and all containers within same networks can communicate with each other. We will use a single network for our case. Volumes: Docker containers by default do not contain any kind of persistence storage. If a docker container is killed then all the data in its memory gets lost. So in order to save some persistant data you need volumes. Think of volumes as permanent hard drives for these containers. We will have one volume per service. ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:3:2","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Setting it up I have created a basic setup of services as described above. To follow along, take a clone of the repo with the following commands. git clone https://github.com/boyyan713/docker-as-dev-environment.git cd docker-as-dev-environment/ git checkout tags/basic-setup git checkout -b tryingout Now you should have the following project structure to start with: (base) yanboyang713@boyang:~/Documents/docker-as-dev-environment$ tree . . ├── go1 │ ├── main.go │ └── README.md ├── njs1 │ ├── index.js │ ├── package.json │ ├── package-lock.json │ └── README.md ├── njs2 │ ├── index.js │ ├── package.json │ ├── package-lock.json │ └── README.md └── py1 ├── README.md ├── requirements.txt └── server ├── __init__.py └── __main__.py 5 directories, 14 files You could browse through each project inside and read the README to understand how to run those projects induvidually. ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:4:0","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Enter docker Make sure you have docker running by following the instructions here https://www.docker.com/get-started. First step is to create a docker file for our service njs1. Create a Dockerfile: blog-docker-dev-environment-example/njs1/Dockerfile njs1/Dockerfile FROM node:6.17.0 WORKDIR /root ADD . /root Create the file at blog-docker-dev-environment-example/docker-compose.yml . Now let jump to docker-compose. Lets add our first service (njs1) to it. version:'3'services:njs1:build:./njs1command:sh -c \"npm install \u0026\u0026 npm start\"environment:- NODE_ENV=development- PORT=7000ports:- '7000:7000'working_dir:/root/njs1 The above docker compose file has only one service (njs1). We will add more services incrementally. Before that lets run it and see what we get. In the folder which contains our docker-compose.yml run: docker-compose up if all goes well you should see it building our app container and in the very end NJS1 app listening on port 7000! Open it up on the browser: http://localhost:7000 to test it out. docker-compose service in detail: build: : Path to the dockerfile. Note: you can either specify the folder which contains the Dockerfile or the complete path to Dockerfile itself. Both works. command: : Command to run when docker container is started. environment: : All the environment variables you need to set. ports:: This specifies the mapping of the port inside the container to that of the host machine. They need not be same. working_dir: : This is the path inside the container where you want to the run the command you specified above. ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:4:1","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"All this is good and fine but how can I use it for efficient development? To make an efficient development environment we need to be able to edit the source code (which with current setup is not possible without re building container again and again). To achieve this we will make use of volumes. Make the following changes. First lets tell our njs1/Dockerfile to not copy the project files to the container. njs1/Dockerfile FROM node:latest ## WORKDIR /root \u003c-- comment out ## ADD . /root \u003c-- these two lines then tell docker-compose to mount our project directory from our local machine as a directory inside the container. docker-compose.yml version:'3'services:njs1:build:./njs1command:sh -c \"npm install \u0026\u0026 npm start\"environment:- NODE_ENV=development- PORT=7000ports:- '7000:7000'working_dir:/root/njs1volumes:- ./njs1:/root/njs1:cached# \u003c--- This will map ./njs1 to /root/njs1 inside the container. In detail volumes: - volumes gives us a way to map our local directories to a directory inside the container. Here we are saying map njs1 folder from our local machine to /root/njs1 inside the docker container. Here we are not copying the files into the container, instead we are mounting it as a shared volume. And thats the trick that makes it useful. To test it out. Lets add nodemon to our njs1 service. cd njs1 npm install --save-dev nodemon Now make the following change in njs1/package.json ... ... ... \"description\": \"A sample nodejs server\", \"main\": \"index.js\", \"scripts\": { - \"start\": \"node index.js\" + \"start\": \"nodemon index.js\" }, ... ... ... Time to test it out! Go to the root folder and run (base) yanboyang713@boyang:~/Documents/docker-as-dev-environment$ docker-compose up --build Building njs1 Step 1/1 : FROM node:latest ---\u003e f47907840247 Successfully built f47907840247 Successfully tagged docker-as-dev-environment_njs1:latest Starting docker-as-dev-environment_njs1_1 ... done Attaching to docker-as-dev-environment_njs1_1 njs1_1 | npm WARN njs1@1.0.0 No repository field. njs1_1 | npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.3 (node_modules/fsevents): njs1_1 | npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.3: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) njs1_1 | njs1_1 | audited 171 packages in 1.633s njs1_1 | njs1_1 | 10 packages are looking for funding njs1_1 | run `npm fund` for details njs1_1 | njs1_1 | found 0 vulnerabilities njs1_1 | njs1_1 | njs1_1 | \u003e njs1@1.0.0 start /root/njs1 njs1_1 | \u003e nodemon index.js njs1_1 | njs1_1 | [nodemon] 2.0.4 njs1_1 | [nodemon] to restart at any time, enter `rs` njs1_1 | [nodemon] watching path(s): *.* njs1_1 | [nodemon] watching extensions: js,mjs,json njs1_1 | [nodemon] starting `node index.js` njs1_1 | NJS1 app listening on port 7000! The –build tells docker-compose to rebuild the images. ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:4:2","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Try editing njs1 source files! 🎉 Try making some changes in the njs1/index.js file and you should see nodemon auto reloading on file change. ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:4:3","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Finishing things up! 🏁 Once you add other services to the docker-compose file. It should look something like this: docker-compose.yml version:'3'services:njs1:build:./njs1command:sh -c \"npm install \u0026\u0026 npm start\"environment:- NODE_ENV=development- PORT=7000ports:- '7000:7000'working_dir:/root/njs1volumes:- ./njs1:/root/njs1:cached# \u003c--- This will map ./njs1 to /root/njs1 inside the container.njs2:image:node:12.3-alpinecommand:sh -c \"npm install \u0026\u0026 npm start\"environment:- NODE_ENV=development- PORT=8000ports:- '8000:8000'working_dir:/root/njs2volumes:- ./njs2:/root/njs2:cached# \u003c--- This will map ./njs2 to /root/njs2 inside the container.py1:image:python:3-stretchcommand:sh -c \"pip install -r requirements.txt \u0026\u0026 python -m server\"environment:- PORT=9000- FLASK_ENV=developmentports:- '9000:9000'working_dir:/root/py1volumes:- ./py1:/root/py1:cached# \u003c--- This will map ./py1 to /root/py1 inside the container.go1:image:golang:1.12-alpinecommand:sh -c \"go run .\"environment:- PORT=5000ports:- '5000:5000'working_dir:/root/go1volumes:- ./go1:/root/go1:cached# \u003c--- This will map ./py1 to /root/py1 inside the container. ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:4:4","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Few Changes image: instead of build : In docker-compose we can specify the docker image from docker-hub directly instead of a dockerfile using the image: property. Hence for simple setups we dont need to write our own Dockerfile. There are many more configuration options which you can use in your docker-compose.yml file. To see a complete reference of those, you can visit this link: https://docs.docker.com/compose/compose-file/ ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:4:5","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Commands Cheatsheet Now that you have setup your services to run via docker-compose for local development. There are few commands that can help. ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:5:0","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Start all services This will start all services in the docker-compose file and detach from the terminal. So your services can run in background. docker-compose start ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:5:1","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Stop all services Corresponding stop command docker-compose stop ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:5:2","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Launch a specific service This will only launch njs1 from the list of services in the docker-compose.yml docker-compose up njs1 You can use similar commands to stop, start induvidual services as well. ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:5:3","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Restart a single service docker-compose restart njs1 ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:5:4","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"logs from specific service This will show logs of only njs1 and also watch for more logs docker-compose logs -f njs1 ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:5:5","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"ssh into a particular service container docker-compose exec njs1 bash ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:5:6","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Some Tips for smoother workflow ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:6:0","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Containers / services running too slow? You might notice that your services are running/launching at extremely slow as compared to when you launch them without docker-compose. This might be because you have allocated less CPU/RAM to docker service. The default values are very low and that causes issues when launching multiple services. Go to : dockerIcon -\u003e preferences -\u003e Advanced Change the slider to give CPU \u003e 3 cores and RAM \u003e 6GB ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:6:1","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Low on space / messed up and want to restart everything from scratch ? ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:6:2","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["virtual environment"],"content":"Removing all images and then refreshing the entire thing. To remove all docker containers: docker rm $(docker ps -a -q) -f To remove all docker images: docker rmi $(docker images) -f ","date":"0001-01-01","objectID":"/dockeraslocaldevelopmentenvironment/:6:3","tags":["Docker"],"title":"Docker as Local Development Environment","uri":"/dockeraslocaldevelopmentenvironment/"},{"categories":["networking"],"content":"Mininet Overview Mininet is a network emulator which creates a network of virtual hosts, switches, controllers, and links. Mininet hosts run standard Linux network software, and its switches support OpenFlow for highly flexible custom routing and Software-Defined Networking. Mininet supports research, development, learning, prototyping, testing, debugging, and any other tasks that could benefit from having a complete experimental network on a laptop or other PC. Mininet: Provides a simple and inexpensive network testbed for developing OpenFlow applications Enables multiple concurrent developers to work independently on the same topology Supports system-level regression tests, which are repeatable and easily packaged Enables complex topology testing, without the need to wire up a physical network Includes a CLI that is topology-aware and OpenFlow-aware, for debugging or running network-wide tests Supports arbitrary custom topologies, and includes a basic set of parametrized topologies is usable out of the box without programming, but also Provides a straightforward and extensible Python API for network creation and experimentation. Mininet provides an easy way to get correct system behavior (and, to the extent supported by your hardware, performance) and to experiment with topologies. Mininet networks run real code including standard Unix/Linux network applications as well as the real Linux kernel and network stack (including any kernel extensions which you may have available, as long as they are compatible with network namespaces.) Because of this, the code you develop and test on Mininet, for an OpenFlow controller, modified switch, or host, can move to a real system with minimal changes, for real-world testing, performance evaluation, and deployment. Importantly this means that a design that works in Mininet can usually move directly to hardware switches for line-rate packet forwarding. ","date":"0001-01-01","objectID":"/mininet/:1:0","tags":["mininet","SDN"],"title":"Getting Started Mininet","uri":"/mininet/"},{"categories":["networking"],"content":"How it Works Nearly every operating system virtualizes computing resources using a process abstraction. Mininet uses process-based virtualization to run many (we’ve successfully booted up to 4096) hosts and switches on a single OS kernel. Since version 2.2.26, Linux has supported network namespaces, a lightweight virtualization feature that provides individual processes with separate network interfaces, routing tables, and ARP tables. The full Linux container architecture adds chroot() jails, process and user namespaces, and CPU and memory limits to provide full OS-level virtualization, but Mininet does not require these additional features. Mininet can create kernel or user-space OpenFlow switches, controllers to control the switches, and hosts to communicate over the simulated network. Mininet connects switches and hosts using virtual ethernet (veth) pairs. While Mininet currently depends on the Linux kernel, in the future it may support other operating systems with process-based virtualization, such Solaris containers or !FreeBSD jails. Mininet’s code is almost entirely Python, except for a short C utility. ","date":"0001-01-01","objectID":"/mininet/:1:1","tags":["mininet","SDN"],"title":"Getting Started Mininet","uri":"/mininet/"},{"categories":["networking"],"content":"Why it’s better Mininet combines many of the best features of emulators, hardware testbeds, and simulators. Compared to full system virtualization based approaches, Mininet: Boots faster: seconds instead of minutes Scales larger: hundreds of hosts and switches vs. single digits Provides more bandwidth: typically 2Gbps total bandwidth on modest hardware Installs easily: a prepackaged VM is available that runs on VMware or VirtualBox for Mac/Win/Linux with OpenFlow v1.0 tools already installed. Compared to hardware testbeds, Mininet is inexpensive and always available (even before conference deadlines) is quickly reconfigurable and restartable Compared to simulators, Mininet runs real, unmodified code including application code, OS kernel code, and control plane code (both OpenFlow controller code and Open vSwitch code) easily connects to real networks offers interactive performance - you can type at it ","date":"0001-01-01","objectID":"/mininet/:1:2","tags":["mininet","SDN"],"title":"Getting Started Mininet","uri":"/mininet/"},{"categories":["networking"],"content":"Limitations Mininet-based networks cannot (currently) exceed the CPU or bandwidth available on a single server. Mininet cannot (currently) run non-Linux-compatible OpenFlow switches or applications; this has not been a major issue in practice. ","date":"0001-01-01","objectID":"/mininet/:1:3","tags":["mininet","SDN"],"title":"Getting Started Mininet","uri":"/mininet/"},{"categories":["networking"],"content":"Installing yay -S mininet ","date":"0001-01-01","objectID":"/mininet/:2:0","tags":["mininet","SDN"],"title":"Getting Started Mininet","uri":"/mininet/"},{"categories":["networking"],"content":"Practice One [yanboyang713@boyang ~]$ sudo mn *** Creating network *** Adding controller *** Adding hosts: h1 h2 *** Adding switches: s1 *** Adding links: (h1, s1) (h2, s1) *** Configuring hosts h1 h2 *** Starting controller c0 *** Starting 1 switches s1 ... *** Starting CLI: mininet\u003e NOTE: If you faced “ovs-vsctl: unix:/run/openvswitch/db.sock: database connection failed (No such file or directory) ovs-vsctl exited with code 1 Error connecting to ovs-db with ovs-vsctl Make sure that Open vSwitch is installed, that ovsdb-server is running, and that “ovs-vsctl show” works correctly. Please, go ahead. sudo systemctl start ovs-vswitchd.service sudo systemctl enable ovs-vswitchd.service mininet\u003e net h1 h1-eth0:s1-eth1 h2 h2-eth0:s1-eth2 s1 lo: s1-eth1:h1-eth0 s1-eth2:h2-eth0 c0 mininet\u003e mininet\u003e links h1-eth0\u003c-\u003es1-eth1 (OK OK) h2-eth0\u003c-\u003es1-eth2 (OK OK) mininet\u003e mininet\u003e exit *** Stopping 1 controllers c0 *** Stopping 2 links .. *** Stopping 1 switches s1 *** Stopping 2 hosts h1 h2 *** Done completed in 896.731 seconds ","date":"0001-01-01","objectID":"/mininet/:3:0","tags":["mininet","SDN"],"title":"Getting Started Mininet","uri":"/mininet/"},{"categories":["networking"],"content":"Practice Two [yanboyang713@boyang ~]$ sudo mn --topo=single,3 *** Creating network *** Adding controller *** Adding hosts: h1 h2 h3 *** Adding switches: s1 *** Adding links: (h1, s1) (h2, s1) (h3, s1) *** Configuring hosts h1 h2 h3 *** Starting controller c0 *** Starting 1 switches s1 ... *** Starting CLI: mininet\u003e ","date":"0001-01-01","objectID":"/mininet/:4:0","tags":["mininet","SDN"],"title":"Getting Started Mininet","uri":"/mininet/"},{"categories":["networking"],"content":"Practice Three [yanboyang713@boyang ~]$ sudo mn -c [sudo] password for yanboyang713: *** Removing excess controllers/ofprotocols/ofdatapaths/pings/noxes killall controller ofprotocol ofdatapath ping nox_corelt-nox_core ovs-openflowd ovs-controllerovs-testcontroller udpbwtest mnexec ivs ryu-manager 2\u003e /dev/null killall -9 controller ofprotocol ofdatapath ping nox_corelt-nox_core ovs-openflowd ovs-controllerovs-testcontroller udpbwtest mnexec ivs ryu-manager 2\u003e /dev/null pkill -9 -f \"sudo mnexec\" *** Removing junk from /tmp rm -f /tmp/vconn* /tmp/vlogs* /tmp/*.out /tmp/*.log *** Removing old X11 tunnels *** Removing excess kernel datapaths ps ax | egrep -o 'dp[0-9]+' | sed 's/dp/nl:/' *** Removing OVS datapaths ovs-vsctl --timeout=1 list-br ovs-vsctl --timeout=1 list-br *** Removing all links of the pattern foo-ethX ip link show | egrep -o '([-_.[:alnum:]]+-eth[[:digit:]]+)' ip link show *** Killing stale mininet node processes pkill -9 -f mininet: *** Shutting down stale tunnels pkill -9 -f Tunnel=Ethernet pkill -9 -f .ssh/mn rm -f ~/.ssh/mn/* *** Cleanup complete. [yanboyang713@boyang ~]$ ","date":"0001-01-01","objectID":"/mininet/:5:0","tags":["mininet","SDN"],"title":"Getting Started Mininet","uri":"/mininet/"},{"categories":["networking"],"content":"Practice Four 线性拓扑(Linear)拓扑： sudo mn –topo=linear,4 # 主机数=交换机数=4 树形拓扑(Tree)： sudo mn –topo=tree,depth=2,fanout=2 # 深度为2，设备下挂设备数为2 自定义拓扑(Custom)： sudo mn –custom file.py –topo mytopo ","date":"0001-01-01","objectID":"/mininet/:6:0","tags":["mininet","SDN"],"title":"Getting Started Mininet","uri":"/mininet/"},{"categories":null,"content":"Overview FFmpeg is a free software project and is the leading software for everything related to multimedia like video encoding, streaming and muxing. FFmpeg - “FF” mean “Fast Forward”, “mpeg” mean “Moving Picture Expers Group” ","date":"0001-01-01","objectID":"/ffmpeg/:1:0","tags":["ffmpeg"],"title":"Getting started with FFmpeg","uri":"/ffmpeg/"},{"categories":null,"content":"Installation https://github.com/jrottenberg/ffmpeg https://www.whoishostingthis.com/compare/ffmpeg/resources/ alias ffmpeg=‘docker run -v=pwd:/tmp/ffmpeg opencoconut/ffmpeg’ yay -S ffmpeg ","date":"0001-01-01","objectID":"/ffmpeg/:2:0","tags":["ffmpeg"],"title":"Getting started with FFmpeg","uri":"/ffmpeg/"},{"categories":null,"content":"Using Linux Terminal to Install VLC in Ubuntu sudo snap install vlc ","date":"0001-01-01","objectID":"/ffmpeg/:3:0","tags":["ffmpeg"],"title":"Getting started with FFmpeg","uri":"/ffmpeg/"},{"categories":null,"content":"Invert the video stream to a virtual video camera If your video stream is inverted, you can make a new virtual video camera which inverts the inverted video. You need to install v4l-utils and also v4l2loopback-dkms. yay -S v4l-utils v4l2loopback-dkms ","date":"0001-01-01","objectID":"/ffmpeg/:4:0","tags":["ffmpeg"],"title":"Getting started with FFmpeg","uri":"/ffmpeg/"},{"categories":null,"content":"Create the virtual video camera: modprobe v4l2loopback Check the name of the newly created camera: [yanboyang713@boyang ~]$ v4l2-ctl --list-devices Dummy video device (0x0000) (platform:v4l2loopback-000): /dev/video0 Then you can run ffmpeg to read from your actual webcam (here /dev/video0) and invert it and feed it to the virtual camera: $ ffmpeg -f v4l2 -i /dev/video0 -vf “vflip” -f v4l2 /dev/video1 You can use the “Dummy” camera in your applications instead of the “Integrated” camera. Bad image quality If you experience images being too bright, too dark, too exposed or any other, you can install v4l2ucpAUR to tweak your image output. ","date":"0001-01-01","objectID":"/ffmpeg/:5:0","tags":["ffmpeg"],"title":"Getting started with FFmpeg","uri":"/ffmpeg/"},{"categories":null,"content":"Introduction ","date":"0001-01-01","objectID":"/zotero/:1:0","tags":null,"title":"Getting Started with Zotero","uri":"/zotero/"},{"categories":null,"content":"Why Use Zotero? Be Organized: Keep all of your research and citations in one place Save time: Format fewer citations by hand Collaborate: Work with anyone in the world, anytime It’s Free: No cost even after you ","date":"0001-01-01","objectID":"/zotero/:2:0","tags":null,"title":"Getting Started with Zotero","uri":"/zotero/"},{"categories":null,"content":"Zotero Installation yay -S zotero ","date":"0001-01-01","objectID":"/zotero/:3:0","tags":null,"title":"Getting Started with Zotero","uri":"/zotero/"},{"categories":null,"content":"Launch Zotero ","date":"0001-01-01","objectID":"/zotero/:4:0","tags":null,"title":"Getting Started with Zotero","uri":"/zotero/"},{"categories":null,"content":"Create a Zotero Account If you haven’t already created a Zotero account, please take a few moments to register now Here. It’s a free way to sync and access your library from anywhere, and it lets you join groups and back up all your attached files. ","date":"0001-01-01","objectID":"/zotero/:4:1","tags":null,"title":"Getting Started with Zotero","uri":"/zotero/"},{"categories":null,"content":"Set up Zotero syncing You can now set up Zotero syncing to sync your data across multiple computers, access your library online, or collaborate in group libraries. Follow these steps to get started. Open the Sync pane of the Zotero preferences Goto “Edit” and click “Preferences” Enter your username and password Enter your username and password into the Sync preferences and click “Set Up Syncing”. Zotero will now automatically sync your data as you make changes. Zotero Connector https://chrome.google.com/webstore/detail/zotero-connector/ekhagklcjbdpajgpjgmbionohlpdbjgc ","date":"0001-01-01","objectID":"/zotero/:4:2","tags":null,"title":"Getting Started with Zotero","uri":"/zotero/"},{"categories":["virtual environment"],"content":"Install Docker https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04 https://docs.docker.com/engine/install/ubuntu/ ","date":"0001-01-01","objectID":"/dockerinstall/:1:0","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Step 1 — Installing Docker The Docker installation package available in the official Ubuntu repository may not be the latest version. To ensure we get the latest version, we’ll install Docker from the official Docker repository. To do that, we’ll add a new package source, add the GPG key from Docker to ensure the downloads are valid, and then install the package. First, update your existing list of packages: sudo apt update Next, install a few prerequisite packages which let apt use packages over HTTPS: sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common Then add the GPG key for the official Docker repository to your system: boyyan-ms:~$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - OK Verify that you now have the key with the fingerprint 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88, by searching for the last 8 characters of the fingerprint. boyyan-ms:~$ sudo apt-key fingerprint 0EBFCD88 pub rsa4096 2017-02-22 [SCEA] 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 uid [ unknown] Docker Release (CE deb) \u003cdocker@docker.com\u003e sub rsa4096 2017-02-22 [S] Add the Docker repository to APT sources: For x86_64 / amd64: boyyan-ms:~$ sudo add-apt-repository \\ \u003e \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ \u003e $(lsb_release -cs) \\ \u003e stable\" Get:1 https://download.docker.com/linux/ubuntu focal InRelease [36.2 kB] Get:2 https://download.docker.com/linux/ubuntu focal/stable amd64 Packages [4226 B] Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease Get:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB] Get:5 http://archive.ubuntu.com/ubuntu focal-security InRelease [109 kB] Fetched 263 kB in 2s (128 kB/s) Reading package lists... Done Next, update the package database with the Docker packages from the newly added repo: sudo apt update Finally, install Docker: sudo apt-get install docker-ce docker-ce-cli containerd.io Docker should now be installed, the daemon started, and the process enabled to start on boot. Check that it’s running: (base) yanboyang713@boyang:~/Documents/docker-as-dev-environment$ sudo systemctl status docker ● docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2020-10-09 15:28:29 CST; 1 day 2h ago Docs: https://docs.docker.com Main PID: 1701 (dockerd) Tasks: 21 CGroup: /system.slice/docker.service └─1701 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock Oct 09 15:28:28 boyang dockerd[1701]: time=\"2020-10-09T15:28:28.951155226+08:00\" level=warning msg=\"Y Oct 09 15:28:28 boyang dockerd[1701]: time=\"2020-10-09T15:28:28.951158322+08:00\" level=warning msg=\"Y Oct 09 15:28:28 boyang dockerd[1701]: time=\"2020-10-09T15:28:28.951164075+08:00\" level=warning msg=\"Y Oct 09 15:28:28 boyang dockerd[1701]: time=\"2020-10-09T15:28:28.951254105+08:00\" level=info msg=\"Load Oct 09 15:28:29 boyang dockerd[1701]: time=\"2020-10-09T15:28:29.081341203+08:00\" level=info msg=\"Defa Oct 09 15:28:29 boyang dockerd[1701]: time=\"2020-10-09T15:28:29.106837782+08:00\" level=info msg=\"Load Oct 09 15:28:29 boyang dockerd[1701]: time=\"2020-10-09T15:28:29.138925271+08:00\" level=info msg=\"Dock Oct 09 15:28:29 boyang dockerd[1701]: time=\"2020-10-09T15:28:29.139550992+08:00\" level=info msg=\"Daem Oct 09 15:28:29 boyang systemd[1]: Started Docker Application Container Engine. Oct 09 15:28:29 boyang dockerd[1701]: time=\"2020-10-09T15:28:29.151309815+08:00\" level=info msg=\"API Verify that Docker Engine is installed correctly by running the hello-world image. $ sudo docker run hello-world This command downloads a test image and runs it in a container. When the container runs, it prints an informational message and exits. Docker Engine is installed and running. NOTE: If you install Docker in Proxmox VE LXC, facing the ERROR message at the below. boyyan@boyyan-ms:~$ sudo docker run hello-world [sudo] pas","date":"0001-01-01","objectID":"/dockerinstall/:1:1","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Step 2 — Executing the Docker Command Without Sudo (Optional) By default, the docker command can only be run the root user or by a user in the docker group, which is automatically created during Docker’s installation process. If you attempt to run the docker command without prefixing it with sudo or without being in the docker group, you’ll get an output like this: docker: Cannot connect to the Docker daemon. Is the docker daemon running on this host?. See 'docker run --help'. If you want to avoid typing sudo whenever you run the docker command, add your username to the docker group: Create the docker group. $ sudo groupadd docker Add your user to the docker group. sudo usermod -aG docker ${USER} To apply the new group membership, log out of the server and back in, or type the following: su - ${USER} You will be prompted to enter your user’s password to continue. Confirm that your user is now added to the docker group by typing: (base) yanboyang713@boyang:~/Documents/docker-as-dev-environment$ id -nG yanboyang713 adm cdrom sudo dip plugdev lpadmin sambashare microk8s docker If you need to add a user to the docker group that you’re not logged in as, declare that username explicitly using: sudo usermod -aG docker username The rest of this article assumes you are running the docker command as a user in the docker group. If you choose not to, please prepend the commands with sudo. Let’s explore the docker command next. ","date":"0001-01-01","objectID":"/dockerinstall/:1:2","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Install Docker Compose Docker Compose relies on Docker Engine for any meaningful work, so make sure you have Docker Engine installed either locally or remote, depending on your setup. On desktop systems like Docker Desktop for Mac and Windows, Docker Compose is included as part of those desktop installs. On Linux systems, first install the Docker Engine for your OS as described on the Get Docker page, then come back here for instructions on installing Compose on Linux systems. To run Compose as a non-root user, see Manage Docker as a non-root user. Install Steps On Linux, you can download the Docker Compose binary from the Compose repository release page on GitHub. Follow the instructions from the link, which involve running the curl command in your terminal to download the binaries. These step-by-step instructions are also included below. Run this command to download the current stable release of Docker Compose: sudo curl -L \"https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose Apply executable permissions to the binary: sudo chmod +x /usr/local/bin/docker-compose Note: If the command docker-compose fails after installation, check your path. You can also create a symbolic link to /usr/bin or any other directory in your path. For example: sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose Optionally, install command completion for the bash and zsh shell. Test the installation. $ docker-compose --version docker-compose version 1.27.4, build 1110ad01 ","date":"0001-01-01","objectID":"/dockerinstall/:1:3","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Nvidia Docker (NVIDIA Container Toolkit) ","date":"0001-01-01","objectID":"/dockerinstall/:2:0","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Introduction The NVIDIA Container Toolkit allows users to build and run GPU accelerated Docker containers. The toolkit includes a container runtime library and utilities to automatically configure containers to leverage NVIDIA GPUs. Product documentation including an architecture overview, platform support, installation and usage guides can be found in the documentation repository. ","date":"0001-01-01","objectID":"/dockerinstall/:2:1","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Install Nvidia Docker Pre-Requisites Make sure you have installed the NVIDIA driver and Docker engine for your Linux distribution Note that you do not need to install the CUDA Toolkit on the host system, but the NVIDIA driver needs to be installed. NVIDIA Drivers Before you get started, make sure you have installed the NVIDIA driver for your Linux distribution. The recommended way to install drivers is to use the package manager for your distribution but other installer mechanisms are also available (e.g. by downloading .run installers from NVIDIA Driver Downloads). On Ubuntu LTS The NVIDIA driver requires that the kernel headers and development packages for the running version of the kernel be installed at the time of the driver installation, as well whenever the driver is rebuilt. For example, if your system is running kernel version 4.4.0, the 4.4.0 kernel headers and development packages must also be installed. The kernel headers and development packages for the currently running kernel can be installed with: sudo apt-get install linux-headers-$(uname -r) Ensure packages on the CUDA network repository have priority over the Canonical repository. $ distribution=$(. /etc/os-release;echo $ID$VERSION_ID | sed -e 's/\\.//g') $ wget https://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64/cuda-$distribution.pin $ sudo mv cuda-$distribution.pin /etc/apt/preferences.d/cuda-repository-pin-600 Install the CUDA repository public GPG key. Note that on Ubuntu 16.04, replace https with http in the command below. sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64/7fa2af80.pub Setup the CUDA network repository. echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64 /\" | sudo tee /etc/apt/sources.list.d/cuda.list Update the APT repository cache and install the driver using the cuda-drivers meta-package. Use the –no-install-recommends option for a lean driver install without any dependencies on X packages. This is particularly useful for headless installations on cloud instances. $ sudo apt-get update $ sudo apt-get -y install cuda-drivers Secure UEFI Boot An issue that often arises when attempting to install Nvidia drivers on Linux involves a motherboard setting known as Secure UEFI Boot. In order for the driver installation to be successful it is necessary to disable this setting on most motherboards. The procedure for carrying this out is highly specific to each particular motherboard. Hence it is difficult to provide detailed instructions that apply to a range of system configurations. In broad terms it will be necessary to enter the BIOS as the machine boots up. This can usually be achieved by pressing the Del or F10 key. Once in the BIOS it is necessary to navigate to the section that determines boot settings. To disable Secure UEFI Boot it sometimes involves backing up and removing certain keys while in other instances it is simply a boolean setting that is easily modified. If this feature is not disabled then in all likelihood trouble will arise subsequent to the Nvidia driver installation. Issues may occur when attempting to login in to Ubuntu after bootup. The problem is particularly difficult to resolve as it often involves an inability to load the Ubuntu GUI! Installing on Ubuntu and Debian Setup the stable repository and the GPG key: distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list Note To get access to experimental features such as CUDA on WSL or the new MIG capability on A100, you may want to add the experimental branch to the repository listing: curl -s -L https://nvidia.github.io/nvidia-container-runtime/experimental/$distribution/nvidia-container-runtime.list | sudo tee /etc/apt/sources.list.d/nvidia","date":"0001-01-01","objectID":"/dockerinstall/:2:2","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Install Docker in Arch Linux ","date":"0001-01-01","objectID":"/dockerinstall/:3:0","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Step 1: yay -S docker ","date":"0001-01-01","objectID":"/dockerinstall/:3:1","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Step 2: sudo systemctl start docker.service ","date":"0001-01-01","objectID":"/dockerinstall/:3:2","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Step 3: sudo docker info ","date":"0001-01-01","objectID":"/dockerinstall/:3:3","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Step 4: Verify that you can run containers. The following command downloads the latest Arch Linux image and uses it to run a Hello World program within a container: [yanboyang713@boyang ~]$ sudo docker run -it --rm archlinux bash -c \"echo hello world\" [sudo] password for yanboyang713: Unable to find image 'archlinux:latest' locally latest: Pulling from library/archlinux cc7ff1c0e722: Pull complete 503b62125c98: Pull complete Digest: sha256:3af015abc04cb71af3d9eb97d6c8c8c1d46a89e729a122251451e4b94d2710cd Status: Downloaded newer image for archlinux:latest hello world ","date":"0001-01-01","objectID":"/dockerinstall/:3:4","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Step 5: sudo systemctl enable docker.service sudo usermod -aG docker $USER reboot ","date":"0001-01-01","objectID":"/dockerinstall/:3:5","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Step 6: Install the docker-rootless-extras-binAUR package to run docker in rootless mode (that is, as a regular user instead of as root). yay -S docker-rootless-extras-bin To Run the Docker daemon as a non-root user (Rootless mode) for ArchLinux, you need to do the following things: Configure subuid and subgid Create ‘/etc/subuid’ and ‘/etc/subgid’ with: ‘testuser:231072:65536’ (for example, ‘testuser’ is username) Example: yanboyang713:231072:65536 Enable socket-activation for the user service: ‘systemctl –user enable –now docker.socket’ sudo systemctl enable --now docker.socket Finally set docker socket environment variable: export DOCKER_HOST=unix://$XDG_RUNTIME_DIR/docker.sock You can also add it to ‘~/.bashrc’ or somewhere alike. ","date":"0001-01-01","objectID":"/dockerinstall/:3:6","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Run GPU accelerated Docker containers with NVIDIA GPUs Install the nvidia-container-toolkitAUR package. Next, restart docker. sudo systemctl restart docker.service docker run --gpus all --device /dev/nvidia0 --device /dev/nvidia-uvm --device /dev/nvidia-uvm-tools --device /dev/nvidiactl nvidia/cuda:11.0-base nvidia-smi ","date":"0001-01-01","objectID":"/dockerinstall/:3:7","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["virtual environment"],"content":"Installing Nvidia Driver [yanboyang713@Boyang-PC ~]$ nvidia-smi Fri May 21 01:30:15 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.73.01 Driver Version: 460.73.01 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce RTX 2070 Off | 00000000:01:00.0 On | N/A | | 31% 34C P8 17W / 175W | 267MiB / 7979MiB | 9% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 484 G /usr/lib/Xorg 227MiB | | 0 N/A N/A 1051 G picom 12MiB | | 0 N/A N/A 1159 G palemoon 24MiB | +-----------------------------------------------------------------------------+ Manual Install using the Official Nvidia.com driver Update your system to load the latest kernel image. Failing this step may result in kernel headers mismatch: sudo pacman -Syu Identify your NVIDIA VGA card.The below commands will allow you to identify your Nvidia card model: lspci -vnn | grep VGA [yanboyang713@Boyang-PC ~]$ lspci -vnn | grep VGA lspci: Unable to load libkmod resources: error -2 01:00.0 VGA compatible controller [0300]: NVIDIA Corporation TU106 [GeForce RTX 2070] [10de:1f02] (rev a1) (prog-if 00 [VGA controller]) Download the Official Nvidia Driver. Using your web browser navigate to the official Nvidia website and download an appropriate driver for your Nvidia graphic card. Save the file into your home directory. Example: [yanboyang713@Boyang-PC ~]$ cd ~/Downloads/ [yanboyang713@Boyang-PC Downloads]$ ls NVIDIA-Linux-x86_64-465.31.run Install Prerequisites. Development tools and kernel headers are required to compile and install Nvidia driver. Let' s start by installation of kernel headers. First, we need to detect currently loaded kernel. For example: [yanboyang713@Boyang-PC Downloads]$ uname -r 5.10.34-1-MANJARO The kernel headers we need to install are [yanboyang713@Boyang-PC Downloads]$ sudo pacman -S linux-headers [sudo] password for yanboyang713: :: There are 11 providers available for linux-headers: :: Repository core 1) linux414-headers 2) linux419-headers 3) linux44-headers 4) linux49-headers 5) linux510-headers 6) linux511-headers 7) linux512-headers 8) linux513-headers 9) linux54-headers :: Repository community 10) linux54-rt-headers 11) linux59-rt-headers Enter a number (default=1): 5 resolving dependencies... looking for conflicting packages... Packages (1) linux510-headers-5.10.36-2 Total Download Size: 9.46 MiB Total Installed Size: 47.74 MiB :: Proceed with installation? [Y/n] Y :: Retrieving packages... linux510-headers-5.10.36-2-x86_64 9.5 MiB 157 KiB/s 01:02 [#############################################################################] 100% (1/1) checking keys in keyring [#############################################################################] 100% (1/1) checking package integrity [#############################################################################] 100% (1/1) loading package files [#############################################################################] 100% (1/1) checking for file conflicts [#############################################################################] 100% (1/1) checking available disk space [#############################################################################] 100% :: Processing package changes... (1/1) installing linux510-headers [#############################################################################] 100% :: Running post-transaction hooks... (1/2) Arming ConditionNeedsUpda","date":"0001-01-01","objectID":"/dockerinstall/:4:0","tags":["Docker"],"title":"How to install Docker, Nvidia Docker and Docker Compose","uri":"/dockerinstall/"},{"categories":["GPU"],"content":"Introduction This post document is about how to build GPU support environment for Tensorflow 2.1. This document will include install Nvidia driver, CUDA Toolkit, cuDNN on Ubuntu 18.04. ","date":"0001-01-01","objectID":"/tensorflowgpusupport/:1:0","tags":["Tensorflow","Machine Learning","Nvidia GPU"],"title":"Install Tensorflow 2.1 on Ubuntu 18.04 LTS with GPU support","uri":"/tensorflowgpusupport/"},{"categories":["GPU"],"content":"Hardware Environment ","date":"0001-01-01","objectID":"/tensorflowgpusupport/:2:0","tags":["Tensorflow","Machine Learning","Nvidia GPU"],"title":"Install Tensorflow 2.1 on Ubuntu 18.04 LTS with GPU support","uri":"/tensorflowgpusupport/"},{"categories":["GPU"],"content":"Secure UEFI Boot An issue that often arises when attempting to install Nvidia drivers on Linux involves a motherboard setting known as Secure UEFI Boot. In order for the driver installation to be successful it is necessary to disable this setting on most motherboards. The procedure for carrying this out is highly specific to each particular motherboard. Hence it is difficult to provide detailed instructions that apply to a range of system configurations. In broad terms it will be necessary to enter the BIOS as the machine boots up. This can usually be achieved by pressing the Del or F10 key. Once in the BIOS it is necessary to navigate to the section that determines boot settings. To disable Secure UEFI Boot it sometimes involves backing up and removing certain keys while in other instances it is simply a boolean setting that is easily modified. If this feature is not disabled then in all likelihood trouble will arise subsequent to the Nvidia driver installation. Issues may occur when attempting to login in to Ubuntu after bootup. The problem is particularly difficult to resolve as it often involves an inability to load the Ubuntu GUI! TensorFlow GPU support requires an assortment of drivers and libraries. To simplify installation and avoid library conflicts, we recommend using a TensorFlow Docker image with GPU support (Linux only). This setup only requires the NVIDIA® GPU drivers. ","date":"0001-01-01","objectID":"/tensorflowgpusupport/:3:0","tags":["Tensorflow","Machine Learning","Nvidia GPU"],"title":"Install Tensorflow 2.1 on Ubuntu 18.04 LTS with GPU support","uri":"/tensorflowgpusupport/"},{"categories":["GPU"],"content":"TensorFlow Docker requirements Install Docker on your local host machine. For GPU support on Linux, install NVIDIA Docker support. Take note of your Docker version with docker -v. Versions earlier than 19.03 require nvidia-docker2 and the –runtime=nvidia flag. On versions including and after 19.03, you will use the nvidia-container-toolkit package and the –gpus all flag. Both options are documented on the page linked above. ","date":"0001-01-01","objectID":"/tensorflowgpusupport/:4:0","tags":["Tensorflow","Machine Learning","Nvidia GPU"],"title":"Install Tensorflow 2.1 on Ubuntu 18.04 LTS with GPU support","uri":"/tensorflowgpusupport/"},{"categories":["GPU"],"content":"Download a TensorFlow Docker image The official TensorFlow Docker images are located in the tensorflow/tensorflow Docker Hub repository. Image releases are tagged using the following format: Tag Description latest The latest release of TensorFlow CPU binary image. Default. nightly Nightly builds of the TensorFlow image. (Unstable.) version Specify the version of the TensorFlow binary image, for example: 2.1.0 devel Nightly builds of a TensorFlow master development environment. Includes TensorFlow source code. custom-op Special experimental image for developing TF custom ops. More info here. Each base tag has variants that add or change functionality: Tag Variants Description tag-gpu The specified tag release with GPU support. (See below) tag-jupyter The specified tag release with Jupyter (includes TensorFlow tutorial notebooks) You can use multiple variants at once. For example, the following downloads TensorFlow release images to your machine: docker pull tensorflow/tensorflow # latest stable release docker pull tensorflow/tensorflow:devel-gpu # nightly dev release w/ GPU support docker pull tensorflow/tensorflow:latest-gpu-jupyter # latest release w/ GPU support and Jupyter ","date":"0001-01-01","objectID":"/tensorflowgpusupport/:5:0","tags":["Tensorflow","Machine Learning","Nvidia GPU"],"title":"Install Tensorflow 2.1 on Ubuntu 18.04 LTS with GPU support","uri":"/tensorflowgpusupport/"},{"categories":["GPU"],"content":"Example docker run -it --gpus all --rm -v $(realpath ~/notebooks):/tf/notebooks -p 8888:8888 tensorflow/tensorflow:latest-gpu-jupyter ","date":"0001-01-01","objectID":"/tensorflowgpusupport/:6:0","tags":["Tensorflow","Machine Learning","Nvidia GPU"],"title":"Install Tensorflow 2.1 on Ubuntu 18.04 LTS with GPU support","uri":"/tensorflowgpusupport/"}]